{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51cd34f9",
   "metadata": {},
   "source": [
    "#  VIT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514b4534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] La procédure spécifiée est introuvable'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filtered train data entries: 11856\n",
      "Number of filtered test data entries: 2975\n",
      "Number of preprocessed files: 14640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\huggingface_hub-0.23.2-py3.8.egg\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee532d51710441f9de07ebbb8f34516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\huggingface_hub-0.23.2-py3.8.egg\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--google--vit-base-patch16-224. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b3e82cb6804e62855ef663318ec69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([13, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [10/741], Loss: 2.5350\n",
      "Epoch [1/5], Batch [20/741], Loss: 2.3148\n",
      "Epoch [1/5], Batch [30/741], Loss: 2.2133\n",
      "Epoch [1/5], Batch [40/741], Loss: 2.2502\n",
      "Epoch [1/5], Batch [50/741], Loss: 2.0798\n",
      "Epoch [1/5], Batch [60/741], Loss: 2.1455\n",
      "Epoch [1/5], Batch [70/741], Loss: 2.0068\n",
      "Epoch [1/5], Batch [80/741], Loss: 1.9502\n",
      "Epoch [1/5], Batch [90/741], Loss: 1.8795\n",
      "Epoch [1/5], Batch [100/741], Loss: 1.9982\n",
      "Epoch [1/5], Batch [110/741], Loss: 1.9482\n",
      "Epoch [1/5], Batch [120/741], Loss: 1.9646\n",
      "Epoch [1/5], Batch [130/741], Loss: 1.7429\n",
      "Epoch [1/5], Batch [140/741], Loss: 2.0422\n",
      "Epoch [1/5], Batch [150/741], Loss: 1.8537\n",
      "Epoch [1/5], Batch [160/741], Loss: 1.9221\n",
      "Epoch [1/5], Batch [170/741], Loss: 1.6791\n",
      "Epoch [1/5], Batch [180/741], Loss: 1.8701\n",
      "Epoch [1/5], Batch [190/741], Loss: 1.7824\n",
      "Epoch [1/5], Batch [200/741], Loss: 1.7401\n",
      "Epoch [1/5], Batch [210/741], Loss: 1.7004\n",
      "Epoch [1/5], Batch [220/741], Loss: 1.6815\n",
      "Epoch [1/5], Batch [230/741], Loss: 1.6987\n",
      "Epoch [1/5], Batch [240/741], Loss: 1.7559\n",
      "Epoch [1/5], Batch [250/741], Loss: 1.7648\n",
      "Epoch [1/5], Batch [260/741], Loss: 1.5122\n",
      "Epoch [1/5], Batch [270/741], Loss: 1.6273\n",
      "Epoch [1/5], Batch [280/741], Loss: 1.5716\n",
      "Epoch [1/5], Batch [290/741], Loss: 1.6094\n",
      "Epoch [1/5], Batch [300/741], Loss: 1.5138\n",
      "Epoch [1/5], Batch [310/741], Loss: 1.4018\n",
      "Epoch [1/5], Batch [320/741], Loss: 1.6614\n",
      "Epoch [1/5], Batch [330/741], Loss: 1.6950\n",
      "Epoch [1/5], Batch [340/741], Loss: 1.6629\n",
      "Epoch [1/5], Batch [350/741], Loss: 1.5590\n",
      "Epoch [1/5], Batch [360/741], Loss: 1.6262\n",
      "Epoch [1/5], Batch [370/741], Loss: 1.5481\n",
      "Epoch [1/5], Batch [380/741], Loss: 1.4905\n",
      "Epoch [1/5], Batch [390/741], Loss: 1.4452\n",
      "Epoch [1/5], Batch [400/741], Loss: 1.4875\n",
      "Epoch [1/5], Batch [410/741], Loss: 1.4153\n",
      "Epoch [1/5], Batch [420/741], Loss: 1.6030\n",
      "Epoch [1/5], Batch [430/741], Loss: 1.5951\n",
      "Epoch [1/5], Batch [440/741], Loss: 1.5485\n",
      "Epoch [1/5], Batch [450/741], Loss: 1.4924\n",
      "Epoch [1/5], Batch [460/741], Loss: 1.6841\n",
      "Epoch [1/5], Batch [470/741], Loss: 1.4982\n",
      "Epoch [1/5], Batch [480/741], Loss: 1.5807\n",
      "Epoch [1/5], Batch [490/741], Loss: 1.5906\n",
      "Epoch [1/5], Batch [500/741], Loss: 1.5665\n",
      "Epoch [1/5], Batch [510/741], Loss: 1.5363\n",
      "Epoch [1/5], Batch [520/741], Loss: 1.4883\n",
      "Epoch [1/5], Batch [530/741], Loss: 1.3994\n",
      "Epoch [1/5], Batch [540/741], Loss: 1.2929\n",
      "Epoch [1/5], Batch [550/741], Loss: 1.4512\n",
      "Epoch [1/5], Batch [560/741], Loss: 1.2888\n",
      "Epoch [1/5], Batch [570/741], Loss: 1.3944\n",
      "Epoch [1/5], Batch [580/741], Loss: 1.5396\n",
      "Epoch [1/5], Batch [590/741], Loss: 1.4764\n",
      "Epoch [1/5], Batch [600/741], Loss: 1.3162\n",
      "Epoch [1/5], Batch [610/741], Loss: 1.3455\n",
      "Epoch [1/5], Batch [620/741], Loss: 1.4888\n",
      "Epoch [1/5], Batch [630/741], Loss: 1.4991\n",
      "Epoch [1/5], Batch [640/741], Loss: 1.4168\n",
      "Epoch [1/5], Batch [650/741], Loss: 1.4062\n",
      "Epoch [1/5], Batch [660/741], Loss: 1.3687\n",
      "Epoch [1/5], Batch [670/741], Loss: 1.3083\n",
      "Epoch [1/5], Batch [680/741], Loss: 1.4217\n",
      "Epoch [1/5], Batch [690/741], Loss: 1.5409\n",
      "Epoch [1/5], Batch [700/741], Loss: 1.5904\n",
      "Epoch [1/5], Batch [710/741], Loss: 1.4864\n",
      "Epoch [1/5], Batch [720/741], Loss: 1.3094\n",
      "Epoch [1/5], Batch [730/741], Loss: 1.6788\n",
      "Epoch [1/5], Batch [740/741], Loss: 1.2938\n",
      "Epoch 1 finished.\n",
      "Epoch [2/5], Batch [10/741], Loss: 1.2337\n",
      "Epoch [2/5], Batch [20/741], Loss: 1.3223\n",
      "Epoch [2/5], Batch [30/741], Loss: 1.1534\n",
      "Epoch [2/5], Batch [40/741], Loss: 1.1620\n",
      "Epoch [2/5], Batch [50/741], Loss: 1.2840\n",
      "Epoch [2/5], Batch [60/741], Loss: 0.8959\n",
      "Epoch [2/5], Batch [70/741], Loss: 1.0278\n",
      "Epoch [2/5], Batch [80/741], Loss: 1.1852\n",
      "Epoch [2/5], Batch [90/741], Loss: 1.1363\n",
      "Epoch [2/5], Batch [100/741], Loss: 1.1264\n",
      "Epoch [2/5], Batch [110/741], Loss: 1.1142\n",
      "Epoch [2/5], Batch [120/741], Loss: 1.3284\n",
      "Epoch [2/5], Batch [130/741], Loss: 1.1309\n",
      "Epoch [2/5], Batch [140/741], Loss: 1.2219\n",
      "Epoch [2/5], Batch [150/741], Loss: 0.9304\n",
      "Epoch [2/5], Batch [160/741], Loss: 0.9692\n",
      "Epoch [2/5], Batch [170/741], Loss: 1.0737\n",
      "Epoch [2/5], Batch [180/741], Loss: 1.0726\n",
      "Epoch [2/5], Batch [190/741], Loss: 0.9356\n",
      "Epoch [2/5], Batch [200/741], Loss: 1.0627\n",
      "Epoch [2/5], Batch [210/741], Loss: 1.2376\n",
      "Epoch [2/5], Batch [220/741], Loss: 1.0201\n",
      "Epoch [2/5], Batch [230/741], Loss: 1.0806\n",
      "Epoch [2/5], Batch [240/741], Loss: 1.0112\n",
      "Epoch [2/5], Batch [250/741], Loss: 0.8870\n",
      "Epoch [2/5], Batch [260/741], Loss: 1.2062\n",
      "Epoch [2/5], Batch [270/741], Loss: 0.8717\n",
      "Epoch [2/5], Batch [280/741], Loss: 1.1110\n",
      "Epoch [2/5], Batch [290/741], Loss: 1.1684\n",
      "Epoch [2/5], Batch [300/741], Loss: 1.0438\n",
      "Epoch [2/5], Batch [310/741], Loss: 1.2510\n",
      "Epoch [2/5], Batch [320/741], Loss: 1.1312\n",
      "Epoch [2/5], Batch [330/741], Loss: 1.1069\n",
      "Epoch [2/5], Batch [340/741], Loss: 1.1420\n",
      "Epoch [2/5], Batch [350/741], Loss: 1.0934\n",
      "Epoch [2/5], Batch [360/741], Loss: 0.9137\n",
      "Epoch [2/5], Batch [370/741], Loss: 1.2426\n",
      "Epoch [2/5], Batch [380/741], Loss: 1.0411\n",
      "Epoch [2/5], Batch [390/741], Loss: 1.2457\n",
      "Epoch [2/5], Batch [400/741], Loss: 1.0614\n",
      "Epoch [2/5], Batch [410/741], Loss: 0.9943\n",
      "Epoch [2/5], Batch [420/741], Loss: 1.1444\n",
      "Epoch [2/5], Batch [430/741], Loss: 1.0774\n",
      "Epoch [2/5], Batch [440/741], Loss: 1.0254\n",
      "Epoch [2/5], Batch [450/741], Loss: 1.1504\n",
      "Epoch [2/5], Batch [460/741], Loss: 0.9876\n",
      "Epoch [2/5], Batch [470/741], Loss: 1.0949\n",
      "Epoch [2/5], Batch [480/741], Loss: 1.0540\n",
      "Epoch [2/5], Batch [490/741], Loss: 1.1867\n",
      "Epoch [2/5], Batch [500/741], Loss: 1.1115\n",
      "Epoch [2/5], Batch [510/741], Loss: 1.2161\n",
      "Epoch [2/5], Batch [520/741], Loss: 1.0517\n",
      "Epoch [2/5], Batch [530/741], Loss: 1.1351\n",
      "Epoch [2/5], Batch [540/741], Loss: 1.1004\n",
      "Epoch [2/5], Batch [550/741], Loss: 0.9740\n",
      "Epoch [2/5], Batch [560/741], Loss: 1.0572\n",
      "Epoch [2/5], Batch [570/741], Loss: 1.0722\n",
      "Epoch [2/5], Batch [580/741], Loss: 0.9591\n",
      "Epoch [2/5], Batch [590/741], Loss: 1.0395\n",
      "Epoch [2/5], Batch [600/741], Loss: 1.0566\n",
      "Epoch [2/5], Batch [610/741], Loss: 0.9019\n",
      "Epoch [2/5], Batch [620/741], Loss: 1.1755\n",
      "Epoch [2/5], Batch [630/741], Loss: 0.8702\n",
      "Epoch [2/5], Batch [640/741], Loss: 1.1814\n",
      "Epoch [2/5], Batch [650/741], Loss: 1.1048\n",
      "Epoch [2/5], Batch [660/741], Loss: 1.0018\n",
      "Epoch [2/5], Batch [670/741], Loss: 0.9643\n",
      "Epoch [2/5], Batch [680/741], Loss: 1.1578\n",
      "Epoch [2/5], Batch [690/741], Loss: 0.9180\n",
      "Epoch [2/5], Batch [700/741], Loss: 0.9467\n",
      "Epoch [2/5], Batch [710/741], Loss: 0.9874\n",
      "Epoch [2/5], Batch [720/741], Loss: 0.8538\n",
      "Epoch [2/5], Batch [730/741], Loss: 1.1673\n",
      "Epoch [2/5], Batch [740/741], Loss: 1.1026\n",
      "Epoch 2 finished.\n",
      "Epoch [3/5], Batch [10/741], Loss: 0.7263\n",
      "Epoch [3/5], Batch [20/741], Loss: 0.7687\n",
      "Epoch [3/5], Batch [30/741], Loss: 0.6528\n",
      "Epoch [3/5], Batch [40/741], Loss: 0.5655\n",
      "Epoch [3/5], Batch [50/741], Loss: 0.7032\n",
      "Epoch [3/5], Batch [60/741], Loss: 0.6306\n",
      "Epoch [3/5], Batch [70/741], Loss: 0.5943\n",
      "Epoch [3/5], Batch [80/741], Loss: 0.5953\n",
      "Epoch [3/5], Batch [90/741], Loss: 0.6226\n",
      "Epoch [3/5], Batch [100/741], Loss: 0.6471\n",
      "Epoch [3/5], Batch [110/741], Loss: 0.6774\n",
      "Epoch [3/5], Batch [120/741], Loss: 0.6066\n",
      "Epoch [3/5], Batch [130/741], Loss: 0.7257\n",
      "Epoch [3/5], Batch [140/741], Loss: 0.6345\n",
      "Epoch [3/5], Batch [150/741], Loss: 0.4689\n",
      "Epoch [3/5], Batch [160/741], Loss: 0.5186\n",
      "Epoch [3/5], Batch [170/741], Loss: 0.6382\n",
      "Epoch [3/5], Batch [180/741], Loss: 0.6494\n",
      "Epoch [3/5], Batch [190/741], Loss: 0.5957\n",
      "Epoch [3/5], Batch [200/741], Loss: 0.7013\n",
      "Epoch [3/5], Batch [210/741], Loss: 0.5960\n",
      "Epoch [3/5], Batch [220/741], Loss: 0.6093\n",
      "Epoch [3/5], Batch [230/741], Loss: 0.6812\n",
      "Epoch [3/5], Batch [240/741], Loss: 0.6674\n",
      "Epoch [3/5], Batch [250/741], Loss: 0.6767\n",
      "Epoch [3/5], Batch [260/741], Loss: 0.5096\n",
      "Epoch [3/5], Batch [270/741], Loss: 0.6153\n",
      "Epoch [3/5], Batch [280/741], Loss: 0.7588\n",
      "Epoch [3/5], Batch [290/741], Loss: 0.6953\n",
      "Epoch [3/5], Batch [300/741], Loss: 0.5844\n",
      "Epoch [3/5], Batch [310/741], Loss: 0.6917\n",
      "Epoch [3/5], Batch [320/741], Loss: 0.6739\n",
      "Epoch [3/5], Batch [330/741], Loss: 0.5956\n",
      "Epoch [3/5], Batch [340/741], Loss: 0.5956\n",
      "Epoch [3/5], Batch [350/741], Loss: 0.5367\n",
      "Epoch [3/5], Batch [360/741], Loss: 0.5567\n",
      "Epoch [3/5], Batch [370/741], Loss: 0.5906\n",
      "Epoch [3/5], Batch [380/741], Loss: 0.5748\n",
      "Epoch [3/5], Batch [390/741], Loss: 0.6324\n",
      "Epoch [3/5], Batch [400/741], Loss: 0.7303\n",
      "Epoch [3/5], Batch [410/741], Loss: 0.7697\n",
      "Epoch [3/5], Batch [420/741], Loss: 0.6563\n",
      "Epoch [3/5], Batch [430/741], Loss: 0.5934\n",
      "Epoch [3/5], Batch [440/741], Loss: 0.8481\n",
      "Epoch [3/5], Batch [450/741], Loss: 0.6826\n",
      "Epoch [3/5], Batch [460/741], Loss: 0.6600\n",
      "Epoch [3/5], Batch [470/741], Loss: 0.5758\n",
      "Epoch [3/5], Batch [480/741], Loss: 0.6939\n",
      "Epoch [3/5], Batch [490/741], Loss: 0.4781\n",
      "Epoch [3/5], Batch [500/741], Loss: 0.5818\n",
      "Epoch [3/5], Batch [510/741], Loss: 0.6620\n",
      "Epoch [3/5], Batch [520/741], Loss: 0.6079\n",
      "Epoch [3/5], Batch [530/741], Loss: 0.5453\n",
      "Epoch [3/5], Batch [540/741], Loss: 0.6043\n",
      "Epoch [3/5], Batch [550/741], Loss: 0.7306\n",
      "Epoch [3/5], Batch [560/741], Loss: 0.4883\n",
      "Epoch [3/5], Batch [570/741], Loss: 0.6022\n",
      "Epoch [3/5], Batch [580/741], Loss: 0.5755\n",
      "Epoch [3/5], Batch [590/741], Loss: 0.5559\n",
      "Epoch [3/5], Batch [600/741], Loss: 0.6433\n",
      "Epoch [3/5], Batch [610/741], Loss: 0.5685\n",
      "Epoch [3/5], Batch [620/741], Loss: 0.4835\n",
      "Epoch [3/5], Batch [630/741], Loss: 0.5464\n",
      "Epoch [3/5], Batch [640/741], Loss: 0.5957\n",
      "Epoch [3/5], Batch [650/741], Loss: 0.6024\n",
      "Epoch [3/5], Batch [660/741], Loss: 0.5998\n",
      "Epoch [3/5], Batch [670/741], Loss: 0.6485\n",
      "Epoch [3/5], Batch [680/741], Loss: 0.6028\n",
      "Epoch [3/5], Batch [690/741], Loss: 0.5838\n",
      "Epoch [3/5], Batch [700/741], Loss: 0.6719\n",
      "Epoch [3/5], Batch [710/741], Loss: 0.6483\n",
      "Epoch [3/5], Batch [720/741], Loss: 0.5126\n",
      "Epoch [3/5], Batch [730/741], Loss: 0.6870\n",
      "Epoch [3/5], Batch [740/741], Loss: 0.5707\n",
      "Epoch 3 finished.\n",
      "Epoch [4/5], Batch [10/741], Loss: 0.3541\n",
      "Epoch [4/5], Batch [20/741], Loss: 0.2320\n",
      "Epoch [4/5], Batch [30/741], Loss: 0.3309\n",
      "Epoch [4/5], Batch [40/741], Loss: 0.2833\n",
      "Epoch [4/5], Batch [50/741], Loss: 0.2951\n",
      "Epoch [4/5], Batch [60/741], Loss: 0.2244\n",
      "Epoch [4/5], Batch [70/741], Loss: 0.2391\n",
      "Epoch [4/5], Batch [80/741], Loss: 0.2821\n",
      "Epoch [4/5], Batch [90/741], Loss: 0.2541\n",
      "Epoch [4/5], Batch [100/741], Loss: 0.2264\n",
      "Epoch [4/5], Batch [110/741], Loss: 0.2684\n",
      "Epoch [4/5], Batch [120/741], Loss: 0.2338\n",
      "Epoch [4/5], Batch [130/741], Loss: 0.2700\n",
      "Epoch [4/5], Batch [140/741], Loss: 0.2211\n",
      "Epoch [4/5], Batch [150/741], Loss: 0.2794\n",
      "Epoch [4/5], Batch [160/741], Loss: 0.2547\n",
      "Epoch [4/5], Batch [170/741], Loss: 0.2619\n",
      "Epoch [4/5], Batch [180/741], Loss: 0.3181\n",
      "Epoch [4/5], Batch [190/741], Loss: 0.2954\n",
      "Epoch [4/5], Batch [200/741], Loss: 0.2433\n",
      "Epoch [4/5], Batch [210/741], Loss: 0.2189\n",
      "Epoch [4/5], Batch [220/741], Loss: 0.2439\n",
      "Epoch [4/5], Batch [230/741], Loss: 0.1851\n",
      "Epoch [4/5], Batch [240/741], Loss: 0.2006\n",
      "Epoch [4/5], Batch [250/741], Loss: 0.2288\n",
      "Epoch [4/5], Batch [260/741], Loss: 0.2528\n",
      "Epoch [4/5], Batch [270/741], Loss: 0.2708\n",
      "Epoch [4/5], Batch [280/741], Loss: 0.2555\n",
      "Epoch [4/5], Batch [290/741], Loss: 0.3236\n",
      "Epoch [4/5], Batch [300/741], Loss: 0.2763\n",
      "Epoch [4/5], Batch [310/741], Loss: 0.2767\n",
      "Epoch [4/5], Batch [320/741], Loss: 0.2086\n",
      "Epoch [4/5], Batch [330/741], Loss: 0.1967\n",
      "Epoch [4/5], Batch [340/741], Loss: 0.2995\n",
      "Epoch [4/5], Batch [350/741], Loss: 0.2704\n",
      "Epoch [4/5], Batch [360/741], Loss: 0.1799\n",
      "Epoch [4/5], Batch [370/741], Loss: 0.2909\n",
      "Epoch [4/5], Batch [380/741], Loss: 0.2149\n",
      "Epoch [4/5], Batch [390/741], Loss: 0.2871\n",
      "Epoch [4/5], Batch [400/741], Loss: 0.2518\n",
      "Epoch [4/5], Batch [410/741], Loss: 0.2479\n",
      "Epoch [4/5], Batch [420/741], Loss: 0.1920\n",
      "Epoch [4/5], Batch [430/741], Loss: 0.2477\n",
      "Epoch [4/5], Batch [440/741], Loss: 0.2395\n",
      "Epoch [4/5], Batch [450/741], Loss: 0.2183\n",
      "Epoch [4/5], Batch [460/741], Loss: 0.2419\n",
      "Epoch [4/5], Batch [470/741], Loss: 0.2324\n",
      "Epoch [4/5], Batch [480/741], Loss: 0.2179\n",
      "Epoch [4/5], Batch [490/741], Loss: 0.2491\n",
      "Epoch [4/5], Batch [500/741], Loss: 0.2701\n",
      "Epoch [4/5], Batch [510/741], Loss: 0.2822\n",
      "Epoch [4/5], Batch [520/741], Loss: 0.2224\n",
      "Epoch [4/5], Batch [530/741], Loss: 0.2769\n",
      "Epoch [4/5], Batch [540/741], Loss: 0.3012\n",
      "Epoch [4/5], Batch [550/741], Loss: 0.2528\n",
      "Epoch [4/5], Batch [560/741], Loss: 0.2603\n",
      "Epoch [4/5], Batch [570/741], Loss: 0.2733\n",
      "Epoch [4/5], Batch [580/741], Loss: 0.1891\n",
      "Epoch [4/5], Batch [590/741], Loss: 0.2432\n",
      "Epoch [4/5], Batch [600/741], Loss: 0.2059\n",
      "Epoch [4/5], Batch [610/741], Loss: 0.1789\n",
      "Epoch [4/5], Batch [620/741], Loss: 0.2144\n",
      "Epoch [4/5], Batch [630/741], Loss: 0.2726\n",
      "Epoch [4/5], Batch [640/741], Loss: 0.1557\n",
      "Epoch [4/5], Batch [650/741], Loss: 0.2261\n",
      "Epoch [4/5], Batch [660/741], Loss: 0.1854\n",
      "Epoch [4/5], Batch [670/741], Loss: 0.2346\n",
      "Epoch [4/5], Batch [680/741], Loss: 0.2280\n",
      "Epoch [4/5], Batch [690/741], Loss: 0.2128\n",
      "Epoch [4/5], Batch [700/741], Loss: 0.2706\n",
      "Epoch [4/5], Batch [710/741], Loss: 0.2140\n",
      "Epoch [4/5], Batch [720/741], Loss: 0.2063\n",
      "Epoch [4/5], Batch [730/741], Loss: 0.2011\n",
      "Epoch [4/5], Batch [740/741], Loss: 0.2388\n",
      "Epoch 4 finished.\n",
      "Epoch [5/5], Batch [10/741], Loss: 0.0799\n",
      "Epoch [5/5], Batch [20/741], Loss: 0.0735\n",
      "Epoch [5/5], Batch [30/741], Loss: 0.0964\n",
      "Epoch [5/5], Batch [40/741], Loss: 0.0754\n",
      "Epoch [5/5], Batch [50/741], Loss: 0.0871\n",
      "Epoch [5/5], Batch [60/741], Loss: 0.0590\n",
      "Epoch [5/5], Batch [70/741], Loss: 0.0625\n",
      "Epoch [5/5], Batch [80/741], Loss: 0.0649\n",
      "Epoch [5/5], Batch [90/741], Loss: 0.0532\n",
      "Epoch [5/5], Batch [100/741], Loss: 0.0773\n",
      "Epoch [5/5], Batch [110/741], Loss: 0.0596\n",
      "Epoch [5/5], Batch [120/741], Loss: 0.0752\n",
      "Epoch [5/5], Batch [130/741], Loss: 0.0729\n",
      "Epoch [5/5], Batch [140/741], Loss: 0.0796\n",
      "Epoch [5/5], Batch [150/741], Loss: 0.0588\n",
      "Epoch [5/5], Batch [160/741], Loss: 0.0786\n",
      "Epoch [5/5], Batch [170/741], Loss: 0.0580\n",
      "Epoch [5/5], Batch [180/741], Loss: 0.0637\n",
      "Epoch [5/5], Batch [190/741], Loss: 0.0610\n",
      "Epoch [5/5], Batch [200/741], Loss: 0.0531\n",
      "Epoch [5/5], Batch [210/741], Loss: 0.0492\n",
      "Epoch [5/5], Batch [220/741], Loss: 0.0489\n",
      "Epoch [5/5], Batch [230/741], Loss: 0.0623\n",
      "Epoch [5/5], Batch [240/741], Loss: 0.0545\n",
      "Epoch [5/5], Batch [250/741], Loss: 0.0357\n",
      "Epoch [5/5], Batch [260/741], Loss: 0.0479\n",
      "Epoch [5/5], Batch [270/741], Loss: 0.0536\n",
      "Epoch [5/5], Batch [280/741], Loss: 0.0436\n",
      "Epoch [5/5], Batch [290/741], Loss: 0.0690\n",
      "Epoch [5/5], Batch [300/741], Loss: 0.0546\n",
      "Epoch [5/5], Batch [310/741], Loss: 0.0489\n",
      "Epoch [5/5], Batch [320/741], Loss: 0.0530\n",
      "Epoch [5/5], Batch [330/741], Loss: 0.0469\n",
      "Epoch [5/5], Batch [340/741], Loss: 0.0466\n",
      "Epoch [5/5], Batch [350/741], Loss: 0.0571\n",
      "Epoch [5/5], Batch [360/741], Loss: 0.0481\n",
      "Epoch [5/5], Batch [370/741], Loss: 0.0495\n",
      "Epoch [5/5], Batch [380/741], Loss: 0.0611\n",
      "Epoch [5/5], Batch [390/741], Loss: 0.0417\n",
      "Epoch [5/5], Batch [400/741], Loss: 0.0532\n",
      "Epoch [5/5], Batch [410/741], Loss: 0.0493\n",
      "Epoch [5/5], Batch [420/741], Loss: 0.0570\n",
      "Epoch [5/5], Batch [430/741], Loss: 0.0691\n",
      "Epoch [5/5], Batch [440/741], Loss: 0.0474\n",
      "Epoch [5/5], Batch [450/741], Loss: 0.0500\n",
      "Epoch [5/5], Batch [460/741], Loss: 0.0536\n",
      "Epoch [5/5], Batch [470/741], Loss: 0.0389\n",
      "Epoch [5/5], Batch [480/741], Loss: 0.0613\n",
      "Epoch [5/5], Batch [490/741], Loss: 0.0618\n",
      "Epoch [5/5], Batch [500/741], Loss: 0.0454\n",
      "Epoch [5/5], Batch [510/741], Loss: 0.0385\n",
      "Epoch [5/5], Batch [520/741], Loss: 0.0413\n",
      "Epoch [5/5], Batch [530/741], Loss: 0.0379\n",
      "Epoch [5/5], Batch [540/741], Loss: 0.0616\n",
      "Epoch [5/5], Batch [550/741], Loss: 0.0298\n",
      "Epoch [5/5], Batch [560/741], Loss: 0.0350\n",
      "Epoch [5/5], Batch [570/741], Loss: 0.0455\n",
      "Epoch [5/5], Batch [580/741], Loss: 0.0579\n",
      "Epoch [5/5], Batch [590/741], Loss: 0.0766\n",
      "Epoch [5/5], Batch [600/741], Loss: 0.0669\n",
      "Epoch [5/5], Batch [610/741], Loss: 0.0456\n",
      "Epoch [5/5], Batch [620/741], Loss: 0.0626\n",
      "Epoch [5/5], Batch [630/741], Loss: 0.0487\n",
      "Epoch [5/5], Batch [640/741], Loss: 0.0462\n",
      "Epoch [5/5], Batch [650/741], Loss: 0.0714\n",
      "Epoch [5/5], Batch [660/741], Loss: 0.0835\n",
      "Epoch [5/5], Batch [670/741], Loss: 0.0702\n",
      "Epoch [5/5], Batch [680/741], Loss: 0.0522\n",
      "Epoch [5/5], Batch [690/741], Loss: 0.0640\n",
      "Epoch [5/5], Batch [700/741], Loss: 0.0306\n",
      "Epoch [5/5], Batch [710/741], Loss: 0.0314\n",
      "Epoch [5/5], Batch [720/741], Loss: 0.0343\n",
      "Epoch [5/5], Batch [730/741], Loss: 0.0475\n",
      "Epoch [5/5], Batch [740/741], Loss: 0.0676\n",
      "Epoch 5 finished.\n",
      "Progress: 400/2975 images evaluated.\n",
      "Progress: 800/2975 images evaluated.\n",
      "Progress: 1200/2975 images evaluated.\n",
      "Progress: 1600/2975 images evaluated.\n",
      "Progress: 2000/2975 images evaluated.\n",
      "Progress: 2400/2975 images evaluated.\n",
      "Progress: 2800/2975 images evaluated.\n",
      "Accuracy: 67.73%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load Data from JSON\n",
    "with open('C:\\\\Users\\\\hp\\\\Downloads\\\\DataVIT.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Flatten the image data\n",
    "image_data = []\n",
    "for item in data:\n",
    "    for img in item['images']:\n",
    "        image_data.append({'url': img['url'], 'label': item['interests'][0]})\n",
    "\n",
    "# Collect paths to all .pt files\n",
    "preprocessed_files = sorted([f'C:\\\\Users\\\\hp\\\\Downloads\\\\preprocessed_images_combined\\\\{filename}' \n",
    "                             for filename in os.listdir('C:\\\\Users\\\\hp\\\\Downloads\\\\preprocessed_images_combined/') \n",
    "                             if filename.endswith('.pt')])\n",
    "\n",
    "# Create a mapping from filename to preprocessed file path\n",
    "preprocessed_file_map = {os.path.basename(file): file for file in preprocessed_files}\n",
    "\n",
    "# Step 2: Split Data into Training and Test Sets\n",
    "train_data, test_data = train_test_split(image_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Filter the train data to include only entries that have a corresponding preprocessed file\n",
    "filtered_train_data = [entry for entry in train_data if os.path.basename(entry['url']).replace('.jpg', '.pt') in preprocessed_file_map]\n",
    "\n",
    "# Filter the test data to include only entries that have a corresponding preprocessed file\n",
    "filtered_test_data = [entry for entry in test_data if os.path.basename(entry['url']).replace('.jpg', '.pt') in preprocessed_file_map]\n",
    "\n",
    "# Verify the length of filtered_train_data, filtered_test_data and preprocessed_files\n",
    "print(f\"Number of filtered train data entries: {len(filtered_train_data)}\")\n",
    "print(f\"Number of filtered test data entries: {len(filtered_test_data)}\")\n",
    "print(f\"Number of preprocessed files: {len(preprocessed_files)}\")\n",
    "\n",
    "# Map the preprocessed files to train and test data\n",
    "preprocessed_train_files = [preprocessed_file_map[os.path.basename(entry['url']).replace('.jpg', '.pt')] for entry in filtered_train_data]\n",
    "preprocessed_test_files = [preprocessed_file_map[os.path.basename(entry['url']).replace('.jpg', '.pt')] for entry in filtered_test_data]\n",
    "\n",
    "# Step 3: Create the Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, preprocessed_files, transform=None):\n",
    "        self.data = data\n",
    "        self.preprocessed_files = preprocessed_files\n",
    "        self.transform = transform\n",
    "\n",
    "        # Ensure the lengths match\n",
    "        assert len(self.data) == len(self.preprocessed_files), (\n",
    "            f\"Data length {len(self.data)} does not match number of preprocessed files {len(self.preprocessed_files)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        preprocessed_image = torch.load(self.preprocessed_files[idx])  # Load the preprocessed tensor from file\n",
    "        label = self.data[idx]['label']\n",
    "        if self.transform:\n",
    "            preprocessed_image = self.transform(preprocessed_image)\n",
    "        return preprocessed_image, label\n",
    "\n",
    "# Define transformations (if any additional transformations are needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(filtered_train_data, preprocessed_train_files, transform=transform)\n",
    "test_dataset = CustomDataset(filtered_test_data, preprocessed_test_files, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Step 4: Create the ViT Model\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=len(set([item['label'] for item in filtered_train_data])),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Step 5: Train the Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # Print every 10 batches\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {running_loss / 10:.4f}')\n",
    "            running_loss = 0.0\n",
    "    print(f'Epoch {epoch+1} finished.')\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total % 100 == 0:  # Print every 100 images\n",
    "            print(f'Progress: {total}/{len(test_loader.dataset)} images evaluated.')\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "079bf36d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Step 7: Display the Classification Report\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(all_labels, all_predictions, target_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m filtered_train_data))))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_labels' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Step 7: Display the Classification Report\n",
    "print(classification_report(all_labels, all_predictions, target_names=list(set(item['label'] for item in filtered_train_data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df0ed8a",
   "metadata": {},
   "source": [
    "# VIT Amelioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e85205c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filtered train data entries: 11856\n",
      "Number of filtered test data entries: 2975\n",
      "Number of preprocessed files: 14640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\huggingface_hub-0.23.2-py3.8.egg\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([13, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [10/741], Loss: 2.6641\n",
      "Epoch [1/10], Batch [20/741], Loss: 2.4907\n",
      "Epoch [1/10], Batch [30/741], Loss: 2.4401\n",
      "Epoch [1/10], Batch [40/741], Loss: 2.3358\n",
      "Epoch [1/10], Batch [50/741], Loss: 2.3505\n",
      "Epoch [1/10], Batch [60/741], Loss: 2.2187\n",
      "Epoch [1/10], Batch [70/741], Loss: 2.2616\n",
      "Epoch [1/10], Batch [80/741], Loss: 2.2518\n",
      "Epoch [1/10], Batch [90/741], Loss: 2.1958\n",
      "Epoch [1/10], Batch [100/741], Loss: 2.0777\n",
      "Epoch [1/10], Batch [110/741], Loss: 2.1421\n",
      "Epoch [1/10], Batch [120/741], Loss: 2.1205\n",
      "Epoch [1/10], Batch [130/741], Loss: 2.1175\n",
      "Epoch [1/10], Batch [140/741], Loss: 1.8167\n",
      "Epoch [1/10], Batch [150/741], Loss: 2.1877\n",
      "Epoch [1/10], Batch [160/741], Loss: 2.1421\n",
      "Epoch [1/10], Batch [170/741], Loss: 1.9674\n",
      "Epoch [1/10], Batch [180/741], Loss: 2.0756\n",
      "Epoch [1/10], Batch [190/741], Loss: 2.0118\n",
      "Epoch [1/10], Batch [200/741], Loss: 1.9082\n",
      "Epoch [1/10], Batch [210/741], Loss: 1.9278\n",
      "Epoch [1/10], Batch [220/741], Loss: 1.8974\n",
      "Epoch [1/10], Batch [230/741], Loss: 1.9414\n",
      "Epoch [1/10], Batch [240/741], Loss: 1.9270\n",
      "Epoch [1/10], Batch [250/741], Loss: 1.8959\n",
      "Epoch [1/10], Batch [260/741], Loss: 1.8029\n",
      "Epoch [1/10], Batch [270/741], Loss: 1.9539\n",
      "Epoch [1/10], Batch [280/741], Loss: 2.0129\n",
      "Epoch [1/10], Batch [290/741], Loss: 1.8292\n",
      "Epoch [1/10], Batch [300/741], Loss: 1.8501\n",
      "Epoch [1/10], Batch [310/741], Loss: 1.8491\n",
      "Epoch [1/10], Batch [320/741], Loss: 1.9454\n",
      "Epoch [1/10], Batch [330/741], Loss: 1.8894\n",
      "Epoch [1/10], Batch [340/741], Loss: 1.8371\n",
      "Epoch [1/10], Batch [350/741], Loss: 1.7438\n",
      "Epoch [1/10], Batch [360/741], Loss: 1.7605\n",
      "Epoch [1/10], Batch [370/741], Loss: 1.7614\n",
      "Epoch [1/10], Batch [380/741], Loss: 1.7161\n",
      "Epoch [1/10], Batch [390/741], Loss: 1.8191\n",
      "Epoch [1/10], Batch [400/741], Loss: 1.8656\n",
      "Epoch [1/10], Batch [410/741], Loss: 1.8148\n",
      "Epoch [1/10], Batch [420/741], Loss: 1.8250\n",
      "Epoch [1/10], Batch [430/741], Loss: 1.7521\n",
      "Epoch [1/10], Batch [440/741], Loss: 1.6694\n",
      "Epoch [1/10], Batch [450/741], Loss: 1.8026\n",
      "Epoch [1/10], Batch [460/741], Loss: 1.7815\n",
      "Epoch [1/10], Batch [470/741], Loss: 1.8109\n",
      "Epoch [1/10], Batch [480/741], Loss: 1.7988\n",
      "Epoch [1/10], Batch [490/741], Loss: 1.6960\n",
      "Epoch [1/10], Batch [500/741], Loss: 1.8105\n",
      "Epoch [1/10], Batch [510/741], Loss: 1.8454\n",
      "Epoch [1/10], Batch [520/741], Loss: 1.9032\n",
      "Epoch [1/10], Batch [530/741], Loss: 1.6949\n",
      "Epoch [1/10], Batch [540/741], Loss: 1.6373\n",
      "Epoch [1/10], Batch [550/741], Loss: 1.6130\n",
      "Epoch [1/10], Batch [560/741], Loss: 1.7378\n",
      "Epoch [1/10], Batch [570/741], Loss: 1.8021\n",
      "Epoch [1/10], Batch [580/741], Loss: 1.7293\n",
      "Epoch [1/10], Batch [590/741], Loss: 1.6826\n",
      "Epoch [1/10], Batch [600/741], Loss: 1.6991\n",
      "Epoch [1/10], Batch [610/741], Loss: 1.7667\n",
      "Epoch [1/10], Batch [620/741], Loss: 1.5642\n",
      "Epoch [1/10], Batch [630/741], Loss: 1.5166\n",
      "Epoch [1/10], Batch [640/741], Loss: 1.5705\n",
      "Epoch [1/10], Batch [650/741], Loss: 1.9232\n",
      "Epoch [1/10], Batch [660/741], Loss: 1.5979\n",
      "Epoch [1/10], Batch [670/741], Loss: 1.6264\n",
      "Epoch [1/10], Batch [680/741], Loss: 1.6932\n",
      "Epoch [1/10], Batch [690/741], Loss: 1.6193\n",
      "Epoch [1/10], Batch [700/741], Loss: 1.4688\n",
      "Epoch [1/10], Batch [710/741], Loss: 1.7554\n",
      "Epoch [1/10], Batch [720/741], Loss: 1.7307\n",
      "Epoch [1/10], Batch [730/741], Loss: 1.5772\n",
      "Epoch [1/10], Batch [740/741], Loss: 1.7136\n",
      "Epoch 1 finished.\n",
      "Epoch [2/10], Batch [10/741], Loss: 1.6169\n",
      "Epoch [2/10], Batch [20/741], Loss: 1.4831\n",
      "Epoch [2/10], Batch [30/741], Loss: 1.5787\n",
      "Epoch [2/10], Batch [40/741], Loss: 1.5372\n",
      "Epoch [2/10], Batch [50/741], Loss: 1.5448\n",
      "Epoch [2/10], Batch [60/741], Loss: 1.6340\n",
      "Epoch [2/10], Batch [70/741], Loss: 1.6015\n",
      "Epoch [2/10], Batch [80/741], Loss: 1.4582\n",
      "Epoch [2/10], Batch [90/741], Loss: 1.6326\n",
      "Epoch [2/10], Batch [100/741], Loss: 1.5399\n",
      "Epoch [2/10], Batch [110/741], Loss: 1.5253\n",
      "Epoch [2/10], Batch [120/741], Loss: 1.4938\n",
      "Epoch [2/10], Batch [130/741], Loss: 1.4917\n",
      "Epoch [2/10], Batch [140/741], Loss: 1.5294\n",
      "Epoch [2/10], Batch [150/741], Loss: 1.6231\n",
      "Epoch [2/10], Batch [160/741], Loss: 1.5645\n",
      "Epoch [2/10], Batch [170/741], Loss: 1.5251\n",
      "Epoch [2/10], Batch [180/741], Loss: 1.5126\n",
      "Epoch [2/10], Batch [190/741], Loss: 1.2599\n",
      "Epoch [2/10], Batch [200/741], Loss: 1.5893\n",
      "Epoch [2/10], Batch [210/741], Loss: 1.5366\n",
      "Epoch [2/10], Batch [220/741], Loss: 1.5147\n",
      "Epoch [2/10], Batch [230/741], Loss: 1.5965\n",
      "Epoch [2/10], Batch [240/741], Loss: 1.4110\n",
      "Epoch [2/10], Batch [250/741], Loss: 1.5474\n",
      "Epoch [2/10], Batch [260/741], Loss: 1.2841\n",
      "Epoch [2/10], Batch [270/741], Loss: 1.7126\n",
      "Epoch [2/10], Batch [280/741], Loss: 1.3979\n",
      "Epoch [2/10], Batch [290/741], Loss: 1.4161\n",
      "Epoch [2/10], Batch [300/741], Loss: 1.5249\n",
      "Epoch [2/10], Batch [310/741], Loss: 1.5474\n",
      "Epoch [2/10], Batch [320/741], Loss: 1.3279\n",
      "Epoch [2/10], Batch [330/741], Loss: 1.6551\n",
      "Epoch [2/10], Batch [340/741], Loss: 1.4948\n",
      "Epoch [2/10], Batch [350/741], Loss: 1.5687\n",
      "Epoch [2/10], Batch [360/741], Loss: 1.4713\n",
      "Epoch [2/10], Batch [370/741], Loss: 1.3756\n",
      "Epoch [2/10], Batch [380/741], Loss: 1.4535\n",
      "Epoch [2/10], Batch [390/741], Loss: 1.4495\n",
      "Epoch [2/10], Batch [400/741], Loss: 1.3890\n",
      "Epoch [2/10], Batch [410/741], Loss: 1.5044\n",
      "Epoch [2/10], Batch [420/741], Loss: 1.4379\n",
      "Epoch [2/10], Batch [430/741], Loss: 1.4795\n",
      "Epoch [2/10], Batch [440/741], Loss: 1.3198\n",
      "Epoch [2/10], Batch [450/741], Loss: 1.4311\n",
      "Epoch [2/10], Batch [460/741], Loss: 1.3998\n",
      "Epoch [2/10], Batch [470/741], Loss: 1.4400\n",
      "Epoch [2/10], Batch [480/741], Loss: 1.4115\n",
      "Epoch [2/10], Batch [490/741], Loss: 1.4936\n",
      "Epoch [2/10], Batch [500/741], Loss: 1.4448\n",
      "Epoch [2/10], Batch [510/741], Loss: 1.5035\n",
      "Epoch [2/10], Batch [520/741], Loss: 1.6329\n",
      "Epoch [2/10], Batch [530/741], Loss: 1.4801\n",
      "Epoch [2/10], Batch [540/741], Loss: 1.4347\n",
      "Epoch [2/10], Batch [550/741], Loss: 1.4932\n",
      "Epoch [2/10], Batch [560/741], Loss: 1.4064\n",
      "Epoch [2/10], Batch [570/741], Loss: 1.5242\n",
      "Epoch [2/10], Batch [580/741], Loss: 1.4254\n",
      "Epoch [2/10], Batch [590/741], Loss: 1.3772\n",
      "Epoch [2/10], Batch [600/741], Loss: 1.5601\n",
      "Epoch [2/10], Batch [610/741], Loss: 1.2898\n",
      "Epoch [2/10], Batch [620/741], Loss: 1.4488\n",
      "Epoch [2/10], Batch [630/741], Loss: 1.5054\n",
      "Epoch [2/10], Batch [640/741], Loss: 1.3940\n",
      "Epoch [2/10], Batch [650/741], Loss: 1.5846\n",
      "Epoch [2/10], Batch [660/741], Loss: 1.5781\n",
      "Epoch [2/10], Batch [670/741], Loss: 1.4901\n",
      "Epoch [2/10], Batch [680/741], Loss: 1.3748\n",
      "Epoch [2/10], Batch [690/741], Loss: 1.3250\n",
      "Epoch [2/10], Batch [700/741], Loss: 1.4599\n",
      "Epoch [2/10], Batch [710/741], Loss: 1.4703\n",
      "Epoch [2/10], Batch [720/741], Loss: 1.3110\n",
      "Epoch [2/10], Batch [730/741], Loss: 1.3752\n",
      "Epoch [2/10], Batch [740/741], Loss: 1.5709\n",
      "Epoch 2 finished.\n",
      "Epoch [3/10], Batch [10/741], Loss: 1.4272\n",
      "Epoch [3/10], Batch [20/741], Loss: 1.1512\n",
      "Epoch [3/10], Batch [30/741], Loss: 1.4949\n",
      "Epoch [3/10], Batch [40/741], Loss: 1.2085\n",
      "Epoch [3/10], Batch [50/741], Loss: 1.2727\n",
      "Epoch [3/10], Batch [60/741], Loss: 1.3379\n",
      "Epoch [3/10], Batch [70/741], Loss: 1.3899\n",
      "Epoch [3/10], Batch [80/741], Loss: 1.3353\n",
      "Epoch [3/10], Batch [90/741], Loss: 1.3475\n",
      "Epoch [3/10], Batch [100/741], Loss: 1.0845\n",
      "Epoch [3/10], Batch [110/741], Loss: 1.3935\n",
      "Epoch [3/10], Batch [120/741], Loss: 1.3027\n",
      "Epoch [3/10], Batch [130/741], Loss: 1.3520\n",
      "Epoch [3/10], Batch [140/741], Loss: 1.3311\n",
      "Epoch [3/10], Batch [150/741], Loss: 1.2379\n",
      "Epoch [3/10], Batch [160/741], Loss: 1.2070\n",
      "Epoch [3/10], Batch [170/741], Loss: 1.2811\n",
      "Epoch [3/10], Batch [180/741], Loss: 1.3025\n",
      "Epoch [3/10], Batch [190/741], Loss: 1.3100\n",
      "Epoch [3/10], Batch [200/741], Loss: 1.2560\n",
      "Epoch [3/10], Batch [210/741], Loss: 1.2365\n",
      "Epoch [3/10], Batch [220/741], Loss: 1.2196\n",
      "Epoch [3/10], Batch [230/741], Loss: 1.3349\n",
      "Epoch [3/10], Batch [240/741], Loss: 1.1934\n",
      "Epoch [3/10], Batch [250/741], Loss: 1.1780\n",
      "Epoch [3/10], Batch [260/741], Loss: 1.4188\n",
      "Epoch [3/10], Batch [270/741], Loss: 1.2094\n",
      "Epoch [3/10], Batch [280/741], Loss: 1.3203\n",
      "Epoch [3/10], Batch [290/741], Loss: 1.3074\n",
      "Epoch [3/10], Batch [300/741], Loss: 1.1952\n",
      "Epoch [3/10], Batch [310/741], Loss: 1.1754\n",
      "Epoch [3/10], Batch [320/741], Loss: 1.3062\n",
      "Epoch [3/10], Batch [330/741], Loss: 1.1818\n",
      "Epoch [3/10], Batch [340/741], Loss: 1.2143\n",
      "Epoch [3/10], Batch [350/741], Loss: 1.4739\n",
      "Epoch [3/10], Batch [360/741], Loss: 1.2188\n",
      "Epoch [3/10], Batch [370/741], Loss: 1.3498\n",
      "Epoch [3/10], Batch [380/741], Loss: 1.2758\n",
      "Epoch [3/10], Batch [390/741], Loss: 1.1112\n",
      "Epoch [3/10], Batch [400/741], Loss: 1.1551\n",
      "Epoch [3/10], Batch [410/741], Loss: 1.1532\n",
      "Epoch [3/10], Batch [420/741], Loss: 1.2237\n",
      "Epoch [3/10], Batch [430/741], Loss: 1.0677\n",
      "Epoch [3/10], Batch [440/741], Loss: 1.3078\n",
      "Epoch [3/10], Batch [450/741], Loss: 1.2138\n",
      "Epoch [3/10], Batch [460/741], Loss: 1.3140\n",
      "Epoch [3/10], Batch [470/741], Loss: 1.2754\n",
      "Epoch [3/10], Batch [480/741], Loss: 1.3606\n",
      "Epoch [3/10], Batch [490/741], Loss: 1.3937\n",
      "Epoch [3/10], Batch [500/741], Loss: 1.2482\n",
      "Epoch [3/10], Batch [510/741], Loss: 1.1735\n",
      "Epoch [3/10], Batch [520/741], Loss: 1.2341\n",
      "Epoch [3/10], Batch [530/741], Loss: 1.2722\n",
      "Epoch [3/10], Batch [540/741], Loss: 1.2749\n",
      "Epoch [3/10], Batch [550/741], Loss: 1.1831\n",
      "Epoch [3/10], Batch [560/741], Loss: 1.3697\n",
      "Epoch [3/10], Batch [570/741], Loss: 1.2544\n",
      "Epoch [3/10], Batch [580/741], Loss: 1.3780\n",
      "Epoch [3/10], Batch [590/741], Loss: 1.2325\n",
      "Epoch [3/10], Batch [600/741], Loss: 1.3275\n",
      "Epoch [3/10], Batch [610/741], Loss: 1.1192\n",
      "Epoch [3/10], Batch [620/741], Loss: 1.2475\n",
      "Epoch [3/10], Batch [630/741], Loss: 1.2332\n",
      "Epoch [3/10], Batch [640/741], Loss: 1.2107\n",
      "Epoch [3/10], Batch [650/741], Loss: 1.1767\n",
      "Epoch [3/10], Batch [660/741], Loss: 1.2686\n",
      "Epoch [3/10], Batch [670/741], Loss: 1.2023\n",
      "Epoch [3/10], Batch [680/741], Loss: 1.4213\n",
      "Epoch [3/10], Batch [690/741], Loss: 1.2389\n",
      "Epoch [3/10], Batch [700/741], Loss: 1.1813\n",
      "Epoch [3/10], Batch [710/741], Loss: 1.3341\n",
      "Epoch [3/10], Batch [720/741], Loss: 1.3483\n",
      "Epoch [3/10], Batch [730/741], Loss: 1.1931\n",
      "Epoch [3/10], Batch [740/741], Loss: 1.3233\n",
      "Epoch 3 finished.\n",
      "Epoch [4/10], Batch [10/741], Loss: 1.0146\n",
      "Epoch [4/10], Batch [20/741], Loss: 1.2963\n",
      "Epoch [4/10], Batch [30/741], Loss: 1.0002\n",
      "Epoch [4/10], Batch [40/741], Loss: 1.0666\n",
      "Epoch [4/10], Batch [50/741], Loss: 1.1740\n",
      "Epoch [4/10], Batch [60/741], Loss: 0.9458\n",
      "Epoch [4/10], Batch [70/741], Loss: 1.1381\n",
      "Epoch [4/10], Batch [80/741], Loss: 1.0034\n",
      "Epoch [4/10], Batch [90/741], Loss: 1.0532\n",
      "Epoch [4/10], Batch [100/741], Loss: 0.9056\n",
      "Epoch [4/10], Batch [110/741], Loss: 1.1061\n",
      "Epoch [4/10], Batch [120/741], Loss: 1.0742\n",
      "Epoch [4/10], Batch [130/741], Loss: 1.1019\n",
      "Epoch [4/10], Batch [140/741], Loss: 1.0611\n",
      "Epoch [4/10], Batch [150/741], Loss: 1.1464\n",
      "Epoch [4/10], Batch [160/741], Loss: 1.1039\n",
      "Epoch [4/10], Batch [170/741], Loss: 1.0427\n",
      "Epoch [4/10], Batch [180/741], Loss: 1.1583\n",
      "Epoch [4/10], Batch [190/741], Loss: 1.1429\n",
      "Epoch [4/10], Batch [200/741], Loss: 1.1629\n",
      "Epoch [4/10], Batch [210/741], Loss: 0.9669\n",
      "Epoch [4/10], Batch [220/741], Loss: 1.1739\n",
      "Epoch [4/10], Batch [230/741], Loss: 1.2518\n",
      "Epoch [4/10], Batch [240/741], Loss: 1.2538\n",
      "Epoch [4/10], Batch [250/741], Loss: 1.1954\n",
      "Epoch [4/10], Batch [260/741], Loss: 1.1092\n",
      "Epoch [4/10], Batch [270/741], Loss: 1.1070\n",
      "Epoch [4/10], Batch [280/741], Loss: 1.1078\n",
      "Epoch [4/10], Batch [290/741], Loss: 1.0769\n",
      "Epoch [4/10], Batch [300/741], Loss: 1.0774\n",
      "Epoch [4/10], Batch [310/741], Loss: 1.0701\n",
      "Epoch [4/10], Batch [320/741], Loss: 1.0254\n",
      "Epoch [4/10], Batch [330/741], Loss: 1.1840\n",
      "Epoch [4/10], Batch [340/741], Loss: 1.1321\n",
      "Epoch [4/10], Batch [350/741], Loss: 1.2253\n",
      "Epoch [4/10], Batch [360/741], Loss: 1.2347\n",
      "Epoch [4/10], Batch [370/741], Loss: 1.0276\n",
      "Epoch [4/10], Batch [380/741], Loss: 1.0816\n",
      "Epoch [4/10], Batch [390/741], Loss: 1.0445\n",
      "Epoch [4/10], Batch [400/741], Loss: 0.9915\n",
      "Epoch [4/10], Batch [410/741], Loss: 0.9554\n",
      "Epoch [4/10], Batch [420/741], Loss: 1.1315\n",
      "Epoch [4/10], Batch [430/741], Loss: 1.0524\n",
      "Epoch [4/10], Batch [440/741], Loss: 1.1565\n",
      "Epoch [4/10], Batch [450/741], Loss: 1.0702\n",
      "Epoch [4/10], Batch [460/741], Loss: 1.2025\n",
      "Epoch [4/10], Batch [470/741], Loss: 0.9722\n",
      "Epoch [4/10], Batch [480/741], Loss: 1.1007\n",
      "Epoch [4/10], Batch [490/741], Loss: 1.0076\n",
      "Epoch [4/10], Batch [500/741], Loss: 1.2738\n",
      "Epoch [4/10], Batch [510/741], Loss: 1.1542\n",
      "Epoch [4/10], Batch [520/741], Loss: 1.0874\n",
      "Epoch [4/10], Batch [530/741], Loss: 1.0977\n",
      "Epoch [4/10], Batch [540/741], Loss: 1.1262\n",
      "Epoch [4/10], Batch [550/741], Loss: 1.1421\n",
      "Epoch [4/10], Batch [560/741], Loss: 1.1236\n",
      "Epoch [4/10], Batch [570/741], Loss: 1.0741\n",
      "Epoch [4/10], Batch [580/741], Loss: 0.9986\n",
      "Epoch [4/10], Batch [590/741], Loss: 1.1765\n",
      "Epoch [4/10], Batch [600/741], Loss: 1.0780\n",
      "Epoch [4/10], Batch [610/741], Loss: 1.1472\n",
      "Epoch [4/10], Batch [620/741], Loss: 1.0659\n",
      "Epoch [4/10], Batch [630/741], Loss: 1.0055\n",
      "Epoch [4/10], Batch [640/741], Loss: 1.0533\n",
      "Epoch [4/10], Batch [650/741], Loss: 1.0649\n",
      "Epoch [4/10], Batch [660/741], Loss: 1.1736\n",
      "Epoch [4/10], Batch [670/741], Loss: 1.0125\n",
      "Epoch [4/10], Batch [680/741], Loss: 1.0623\n",
      "Epoch [4/10], Batch [690/741], Loss: 1.0768\n",
      "Epoch [4/10], Batch [700/741], Loss: 1.1652\n",
      "Epoch [4/10], Batch [710/741], Loss: 1.0536\n",
      "Epoch [4/10], Batch [720/741], Loss: 1.0377\n",
      "Epoch [4/10], Batch [730/741], Loss: 1.1879\n",
      "Epoch [4/10], Batch [740/741], Loss: 1.1698\n",
      "Epoch 4 finished.\n",
      "Epoch [5/10], Batch [10/741], Loss: 0.9042\n",
      "Epoch [5/10], Batch [20/741], Loss: 0.9274\n",
      "Epoch [5/10], Batch [30/741], Loss: 1.0819\n",
      "Epoch [5/10], Batch [40/741], Loss: 1.0148\n",
      "Epoch [5/10], Batch [50/741], Loss: 0.9476\n",
      "Epoch [5/10], Batch [60/741], Loss: 0.9269\n",
      "Epoch [5/10], Batch [70/741], Loss: 1.0008\n",
      "Epoch [5/10], Batch [80/741], Loss: 0.9365\n",
      "Epoch [5/10], Batch [90/741], Loss: 1.0101\n",
      "Epoch [5/10], Batch [100/741], Loss: 0.9051\n",
      "Epoch [5/10], Batch [110/741], Loss: 0.9938\n",
      "Epoch [5/10], Batch [120/741], Loss: 0.9363\n",
      "Epoch [5/10], Batch [130/741], Loss: 0.8992\n",
      "Epoch [5/10], Batch [140/741], Loss: 0.8816\n",
      "Epoch [5/10], Batch [150/741], Loss: 0.9742\n",
      "Epoch [5/10], Batch [160/741], Loss: 0.9154\n",
      "Epoch [5/10], Batch [170/741], Loss: 0.8471\n",
      "Epoch [5/10], Batch [180/741], Loss: 0.7983\n",
      "Epoch [5/10], Batch [190/741], Loss: 0.9728\n",
      "Epoch [5/10], Batch [200/741], Loss: 0.8281\n",
      "Epoch [5/10], Batch [210/741], Loss: 0.8734\n",
      "Epoch [5/10], Batch [220/741], Loss: 1.0424\n",
      "Epoch [5/10], Batch [230/741], Loss: 0.9971\n",
      "Epoch [5/10], Batch [240/741], Loss: 0.9102\n",
      "Epoch [5/10], Batch [250/741], Loss: 0.9173\n",
      "Epoch [5/10], Batch [260/741], Loss: 0.9627\n",
      "Epoch [5/10], Batch [270/741], Loss: 0.8820\n",
      "Epoch [5/10], Batch [280/741], Loss: 0.9536\n",
      "Epoch [5/10], Batch [290/741], Loss: 0.9591\n",
      "Epoch [5/10], Batch [300/741], Loss: 0.8993\n",
      "Epoch [5/10], Batch [310/741], Loss: 0.8894\n",
      "Epoch [5/10], Batch [320/741], Loss: 0.9461\n",
      "Epoch [5/10], Batch [330/741], Loss: 0.9228\n",
      "Epoch [5/10], Batch [340/741], Loss: 0.9744\n",
      "Epoch [5/10], Batch [350/741], Loss: 1.0381\n",
      "Epoch [5/10], Batch [360/741], Loss: 0.8570\n",
      "Epoch [5/10], Batch [370/741], Loss: 1.0489\n",
      "Epoch [5/10], Batch [380/741], Loss: 0.9829\n",
      "Epoch [5/10], Batch [390/741], Loss: 0.9929\n",
      "Epoch [5/10], Batch [400/741], Loss: 1.0704\n",
      "Epoch [5/10], Batch [410/741], Loss: 0.8014\n",
      "Epoch [5/10], Batch [420/741], Loss: 0.8987\n",
      "Epoch [5/10], Batch [430/741], Loss: 0.9348\n",
      "Epoch [5/10], Batch [440/741], Loss: 1.0075\n",
      "Epoch [5/10], Batch [450/741], Loss: 1.0650\n",
      "Epoch [5/10], Batch [460/741], Loss: 0.9584\n",
      "Epoch [5/10], Batch [470/741], Loss: 0.8270\n",
      "Epoch [5/10], Batch [480/741], Loss: 0.9741\n",
      "Epoch [5/10], Batch [490/741], Loss: 0.8502\n",
      "Epoch [5/10], Batch [500/741], Loss: 0.9740\n",
      "Epoch [5/10], Batch [510/741], Loss: 0.8821\n",
      "Epoch [5/10], Batch [520/741], Loss: 1.0168\n",
      "Epoch [5/10], Batch [530/741], Loss: 0.8610\n",
      "Epoch [5/10], Batch [540/741], Loss: 0.8685\n",
      "Epoch [5/10], Batch [550/741], Loss: 0.9342\n",
      "Epoch [5/10], Batch [560/741], Loss: 0.9299\n",
      "Epoch [5/10], Batch [570/741], Loss: 1.1607\n",
      "Epoch [5/10], Batch [580/741], Loss: 1.0704\n",
      "Epoch [5/10], Batch [590/741], Loss: 0.8265\n",
      "Epoch [5/10], Batch [600/741], Loss: 0.9640\n",
      "Epoch [5/10], Batch [610/741], Loss: 0.9792\n",
      "Epoch [5/10], Batch [620/741], Loss: 0.9074\n",
      "Epoch [5/10], Batch [630/741], Loss: 1.0445\n",
      "Epoch [5/10], Batch [640/741], Loss: 0.8656\n",
      "Epoch [5/10], Batch [650/741], Loss: 0.8545\n",
      "Epoch [5/10], Batch [660/741], Loss: 1.0123\n",
      "Epoch [5/10], Batch [670/741], Loss: 1.0383\n",
      "Epoch [5/10], Batch [680/741], Loss: 0.8521\n",
      "Epoch [5/10], Batch [690/741], Loss: 0.9071\n",
      "Epoch [5/10], Batch [700/741], Loss: 0.8565\n",
      "Epoch [5/10], Batch [710/741], Loss: 0.9085\n",
      "Epoch [5/10], Batch [720/741], Loss: 0.9301\n",
      "Epoch [5/10], Batch [730/741], Loss: 0.9171\n",
      "Epoch [5/10], Batch [740/741], Loss: 0.7973\n",
      "Epoch 5 finished.\n",
      "Epoch [6/10], Batch [10/741], Loss: 0.7793\n",
      "Epoch [6/10], Batch [20/741], Loss: 0.8053\n",
      "Epoch [6/10], Batch [30/741], Loss: 0.7626\n",
      "Epoch [6/10], Batch [40/741], Loss: 0.8947\n",
      "Epoch [6/10], Batch [50/741], Loss: 0.8850\n",
      "Epoch [6/10], Batch [60/741], Loss: 0.9722\n",
      "Epoch [6/10], Batch [70/741], Loss: 0.7164\n",
      "Epoch [6/10], Batch [80/741], Loss: 0.7262\n",
      "Epoch [6/10], Batch [90/741], Loss: 0.9259\n",
      "Epoch [6/10], Batch [100/741], Loss: 0.6929\n",
      "Epoch [6/10], Batch [110/741], Loss: 0.8475\n",
      "Epoch [6/10], Batch [120/741], Loss: 0.7196\n",
      "Epoch [6/10], Batch [130/741], Loss: 0.8019\n",
      "Epoch [6/10], Batch [140/741], Loss: 0.7230\n",
      "Epoch [6/10], Batch [150/741], Loss: 0.7493\n",
      "Epoch [6/10], Batch [160/741], Loss: 0.7560\n",
      "Epoch [6/10], Batch [170/741], Loss: 0.7867\n",
      "Epoch [6/10], Batch [180/741], Loss: 0.7267\n",
      "Epoch [6/10], Batch [190/741], Loss: 0.6525\n",
      "Epoch [6/10], Batch [200/741], Loss: 0.6956\n",
      "Epoch [6/10], Batch [210/741], Loss: 0.7800\n",
      "Epoch [6/10], Batch [220/741], Loss: 0.7253\n",
      "Epoch [6/10], Batch [230/741], Loss: 0.8833\n",
      "Epoch [6/10], Batch [240/741], Loss: 0.7364\n",
      "Epoch [6/10], Batch [250/741], Loss: 0.8027\n",
      "Epoch [6/10], Batch [260/741], Loss: 0.9029\n",
      "Epoch [6/10], Batch [270/741], Loss: 0.7798\n",
      "Epoch [6/10], Batch [280/741], Loss: 0.7573\n",
      "Epoch [6/10], Batch [290/741], Loss: 0.8887\n",
      "Epoch [6/10], Batch [300/741], Loss: 0.7508\n",
      "Epoch [6/10], Batch [310/741], Loss: 0.7898\n",
      "Epoch [6/10], Batch [320/741], Loss: 0.7359\n",
      "Epoch [6/10], Batch [330/741], Loss: 0.8893\n",
      "Epoch [6/10], Batch [340/741], Loss: 0.7774\n",
      "Epoch [6/10], Batch [350/741], Loss: 0.7952\n",
      "Epoch [6/10], Batch [360/741], Loss: 0.8858\n",
      "Epoch [6/10], Batch [370/741], Loss: 0.9236\n",
      "Epoch [6/10], Batch [380/741], Loss: 0.6229\n",
      "Epoch [6/10], Batch [390/741], Loss: 0.8775\n",
      "Epoch [6/10], Batch [400/741], Loss: 0.8173\n",
      "Epoch [6/10], Batch [410/741], Loss: 0.8222\n",
      "Epoch [6/10], Batch [420/741], Loss: 0.7554\n",
      "Epoch [6/10], Batch [430/741], Loss: 0.9028\n",
      "Epoch [6/10], Batch [440/741], Loss: 0.8002\n",
      "Epoch [6/10], Batch [450/741], Loss: 0.8552\n",
      "Epoch [6/10], Batch [460/741], Loss: 0.7240\n",
      "Epoch [6/10], Batch [470/741], Loss: 0.7772\n",
      "Epoch [6/10], Batch [480/741], Loss: 0.8649\n",
      "Epoch [6/10], Batch [490/741], Loss: 0.9241\n",
      "Epoch [6/10], Batch [500/741], Loss: 0.7682\n",
      "Epoch [6/10], Batch [510/741], Loss: 0.7279\n",
      "Epoch [6/10], Batch [520/741], Loss: 0.7526\n",
      "Epoch [6/10], Batch [530/741], Loss: 0.8250\n",
      "Epoch [6/10], Batch [540/741], Loss: 0.8135\n",
      "Epoch [6/10], Batch [550/741], Loss: 0.8707\n",
      "Epoch [6/10], Batch [560/741], Loss: 0.9859\n",
      "Epoch [6/10], Batch [570/741], Loss: 0.7342\n",
      "Epoch [6/10], Batch [580/741], Loss: 0.9360\n",
      "Epoch [6/10], Batch [590/741], Loss: 0.8269\n",
      "Epoch [6/10], Batch [600/741], Loss: 0.6725\n",
      "Epoch [6/10], Batch [610/741], Loss: 0.9259\n",
      "Epoch [6/10], Batch [620/741], Loss: 0.8407\n",
      "Epoch [6/10], Batch [630/741], Loss: 0.8263\n",
      "Epoch [6/10], Batch [640/741], Loss: 0.7468\n",
      "Epoch [6/10], Batch [650/741], Loss: 0.7998\n",
      "Epoch [6/10], Batch [660/741], Loss: 0.8051\n",
      "Epoch [6/10], Batch [670/741], Loss: 0.7066\n",
      "Epoch [6/10], Batch [680/741], Loss: 0.6951\n",
      "Epoch [6/10], Batch [690/741], Loss: 0.8775\n",
      "Epoch [6/10], Batch [700/741], Loss: 0.7674\n",
      "Epoch [6/10], Batch [710/741], Loss: 0.7851\n",
      "Epoch [6/10], Batch [720/741], Loss: 0.8160\n",
      "Epoch [6/10], Batch [730/741], Loss: 0.7606\n",
      "Epoch [6/10], Batch [740/741], Loss: 0.8276\n",
      "Epoch 6 finished.\n",
      "Epoch [7/10], Batch [10/741], Loss: 0.5988\n",
      "Epoch [7/10], Batch [20/741], Loss: 0.6653\n",
      "Epoch [7/10], Batch [30/741], Loss: 0.6086\n",
      "Epoch [7/10], Batch [40/741], Loss: 0.6532\n",
      "Epoch [7/10], Batch [50/741], Loss: 0.4972\n",
      "Epoch [7/10], Batch [60/741], Loss: 0.5810\n",
      "Epoch [7/10], Batch [70/741], Loss: 0.6099\n",
      "Epoch [7/10], Batch [80/741], Loss: 0.5225\n",
      "Epoch [7/10], Batch [90/741], Loss: 0.6107\n",
      "Epoch [7/10], Batch [100/741], Loss: 0.5997\n",
      "Epoch [7/10], Batch [110/741], Loss: 0.5587\n",
      "Epoch [7/10], Batch [120/741], Loss: 0.6948\n",
      "Epoch [7/10], Batch [130/741], Loss: 0.7555\n",
      "Epoch [7/10], Batch [140/741], Loss: 0.6406\n",
      "Epoch [7/10], Batch [150/741], Loss: 0.7040\n",
      "Epoch [7/10], Batch [160/741], Loss: 0.7833\n",
      "Epoch [7/10], Batch [170/741], Loss: 0.7024\n",
      "Epoch [7/10], Batch [180/741], Loss: 0.6012\n",
      "Epoch [7/10], Batch [190/741], Loss: 0.6641\n",
      "Epoch [7/10], Batch [200/741], Loss: 0.5429\n",
      "Epoch [7/10], Batch [210/741], Loss: 0.5991\n",
      "Epoch [7/10], Batch [220/741], Loss: 0.6347\n",
      "Epoch [7/10], Batch [230/741], Loss: 0.7424\n",
      "Epoch [7/10], Batch [240/741], Loss: 0.6706\n",
      "Epoch [7/10], Batch [250/741], Loss: 0.6869\n",
      "Epoch [7/10], Batch [260/741], Loss: 0.6117\n",
      "Epoch [7/10], Batch [270/741], Loss: 0.7102\n",
      "Epoch [7/10], Batch [280/741], Loss: 0.6233\n",
      "Epoch [7/10], Batch [290/741], Loss: 0.7845\n",
      "Epoch [7/10], Batch [300/741], Loss: 0.7382\n",
      "Epoch [7/10], Batch [310/741], Loss: 0.7048\n",
      "Epoch [7/10], Batch [320/741], Loss: 0.6883\n",
      "Epoch [7/10], Batch [330/741], Loss: 0.7149\n",
      "Epoch [7/10], Batch [340/741], Loss: 0.6941\n",
      "Epoch [7/10], Batch [350/741], Loss: 0.7525\n",
      "Epoch [7/10], Batch [360/741], Loss: 0.6505\n",
      "Epoch [7/10], Batch [370/741], Loss: 0.6698\n",
      "Epoch [7/10], Batch [380/741], Loss: 0.5759\n",
      "Epoch [7/10], Batch [390/741], Loss: 0.8321\n",
      "Epoch [7/10], Batch [400/741], Loss: 0.6333\n",
      "Epoch [7/10], Batch [410/741], Loss: 0.5367\n",
      "Epoch [7/10], Batch [420/741], Loss: 0.6177\n",
      "Epoch [7/10], Batch [430/741], Loss: 0.6571\n",
      "Epoch [7/10], Batch [440/741], Loss: 0.6349\n",
      "Epoch [7/10], Batch [450/741], Loss: 0.6027\n",
      "Epoch [7/10], Batch [460/741], Loss: 0.8289\n",
      "Epoch [7/10], Batch [470/741], Loss: 0.7355\n",
      "Epoch [7/10], Batch [480/741], Loss: 0.6109\n",
      "Epoch [7/10], Batch [490/741], Loss: 0.7378\n",
      "Epoch [7/10], Batch [500/741], Loss: 0.8006\n",
      "Epoch [7/10], Batch [510/741], Loss: 0.7121\n",
      "Epoch [7/10], Batch [520/741], Loss: 0.6742\n",
      "Epoch [7/10], Batch [530/741], Loss: 0.6992\n",
      "Epoch [7/10], Batch [540/741], Loss: 0.6844\n",
      "Epoch [7/10], Batch [550/741], Loss: 0.6792\n",
      "Epoch [7/10], Batch [560/741], Loss: 0.6665\n",
      "Epoch [7/10], Batch [570/741], Loss: 0.5879\n",
      "Epoch [7/10], Batch [580/741], Loss: 0.7055\n",
      "Epoch [7/10], Batch [590/741], Loss: 0.6257\n",
      "Epoch [7/10], Batch [600/741], Loss: 0.6213\n",
      "Epoch [7/10], Batch [610/741], Loss: 0.6736\n",
      "Epoch [7/10], Batch [620/741], Loss: 0.6296\n",
      "Epoch [7/10], Batch [630/741], Loss: 0.6828\n",
      "Epoch [7/10], Batch [640/741], Loss: 0.5694\n",
      "Epoch [7/10], Batch [650/741], Loss: 0.7442\n",
      "Epoch [7/10], Batch [660/741], Loss: 0.6572\n",
      "Epoch [7/10], Batch [670/741], Loss: 0.6853\n",
      "Epoch [7/10], Batch [680/741], Loss: 0.7214\n",
      "Epoch [7/10], Batch [690/741], Loss: 0.7589\n",
      "Epoch [7/10], Batch [700/741], Loss: 0.5632\n",
      "Epoch [7/10], Batch [710/741], Loss: 0.5656\n",
      "Epoch [7/10], Batch [720/741], Loss: 0.6591\n",
      "Epoch [7/10], Batch [730/741], Loss: 0.7095\n",
      "Epoch [7/10], Batch [740/741], Loss: 0.6709\n",
      "Epoch 7 finished.\n",
      "Epoch [8/10], Batch [10/741], Loss: 0.5058\n",
      "Epoch [8/10], Batch [20/741], Loss: 0.5383\n",
      "Epoch [8/10], Batch [30/741], Loss: 0.4699\n",
      "Epoch [8/10], Batch [40/741], Loss: 0.5794\n",
      "Epoch [8/10], Batch [50/741], Loss: 0.5794\n",
      "Epoch [8/10], Batch [60/741], Loss: 0.5478\n",
      "Epoch [8/10], Batch [70/741], Loss: 0.5073\n",
      "Epoch [8/10], Batch [80/741], Loss: 0.5778\n",
      "Epoch [8/10], Batch [90/741], Loss: 0.5662\n",
      "Epoch [8/10], Batch [100/741], Loss: 0.5367\n",
      "Epoch [8/10], Batch [110/741], Loss: 0.5787\n",
      "Epoch [8/10], Batch [120/741], Loss: 0.5570\n",
      "Epoch [8/10], Batch [130/741], Loss: 0.5266\n",
      "Epoch [8/10], Batch [140/741], Loss: 0.4156\n",
      "Epoch [8/10], Batch [150/741], Loss: 0.5872\n",
      "Epoch [8/10], Batch [160/741], Loss: 0.4892\n",
      "Epoch [8/10], Batch [170/741], Loss: 0.5029\n",
      "Epoch [8/10], Batch [180/741], Loss: 0.6374\n",
      "Epoch [8/10], Batch [190/741], Loss: 0.4823\n",
      "Epoch [8/10], Batch [200/741], Loss: 0.6029\n",
      "Epoch [8/10], Batch [210/741], Loss: 0.5743\n",
      "Epoch [8/10], Batch [220/741], Loss: 0.5898\n",
      "Epoch [8/10], Batch [230/741], Loss: 0.5218\n",
      "Epoch [8/10], Batch [240/741], Loss: 0.5057\n",
      "Epoch [8/10], Batch [250/741], Loss: 0.4909\n",
      "Epoch [8/10], Batch [260/741], Loss: 0.5682\n",
      "Epoch [8/10], Batch [270/741], Loss: 0.5412\n",
      "Epoch [8/10], Batch [280/741], Loss: 0.5998\n",
      "Epoch [8/10], Batch [290/741], Loss: 0.4522\n",
      "Epoch [8/10], Batch [300/741], Loss: 0.5442\n",
      "Epoch [8/10], Batch [310/741], Loss: 0.4556\n",
      "Epoch [8/10], Batch [320/741], Loss: 0.4283\n",
      "Epoch [8/10], Batch [330/741], Loss: 0.5615\n",
      "Epoch [8/10], Batch [340/741], Loss: 0.5367\n",
      "Epoch [8/10], Batch [350/741], Loss: 0.5520\n",
      "Epoch [8/10], Batch [360/741], Loss: 0.5745\n",
      "Epoch [8/10], Batch [370/741], Loss: 0.5573\n",
      "Epoch [8/10], Batch [380/741], Loss: 0.4778\n",
      "Epoch [8/10], Batch [390/741], Loss: 0.5116\n",
      "Epoch [8/10], Batch [400/741], Loss: 0.4856\n",
      "Epoch [8/10], Batch [410/741], Loss: 0.4803\n",
      "Epoch [8/10], Batch [420/741], Loss: 0.5946\n",
      "Epoch [8/10], Batch [430/741], Loss: 0.5218\n",
      "Epoch [8/10], Batch [440/741], Loss: 0.4664\n",
      "Epoch [8/10], Batch [450/741], Loss: 0.4705\n",
      "Epoch [8/10], Batch [460/741], Loss: 0.5892\n",
      "Epoch [8/10], Batch [470/741], Loss: 0.5035\n",
      "Epoch [8/10], Batch [480/741], Loss: 0.5190\n",
      "Epoch [8/10], Batch [490/741], Loss: 0.5785\n",
      "Epoch [8/10], Batch [500/741], Loss: 0.5044\n",
      "Epoch [8/10], Batch [510/741], Loss: 0.4634\n",
      "Epoch [8/10], Batch [520/741], Loss: 0.4554\n",
      "Epoch [8/10], Batch [530/741], Loss: 0.4740\n",
      "Epoch [8/10], Batch [540/741], Loss: 0.4869\n",
      "Epoch [8/10], Batch [550/741], Loss: 0.7813\n",
      "Epoch [8/10], Batch [560/741], Loss: 0.5361\n",
      "Epoch [8/10], Batch [570/741], Loss: 0.6420\n",
      "Epoch [8/10], Batch [580/741], Loss: 0.6266\n",
      "Epoch [8/10], Batch [590/741], Loss: 0.4643\n",
      "Epoch [8/10], Batch [600/741], Loss: 0.4176\n",
      "Epoch [8/10], Batch [610/741], Loss: 0.5646\n",
      "Epoch [8/10], Batch [620/741], Loss: 0.4782\n",
      "Epoch [8/10], Batch [630/741], Loss: 0.6612\n",
      "Epoch [8/10], Batch [640/741], Loss: 0.5402\n",
      "Epoch [8/10], Batch [650/741], Loss: 0.4575\n",
      "Epoch [8/10], Batch [660/741], Loss: 0.5359\n",
      "Epoch [8/10], Batch [670/741], Loss: 0.5622\n",
      "Epoch [8/10], Batch [680/741], Loss: 0.5123\n",
      "Epoch [8/10], Batch [690/741], Loss: 0.5503\n",
      "Epoch [8/10], Batch [700/741], Loss: 0.4767\n",
      "Epoch [8/10], Batch [710/741], Loss: 0.5562\n",
      "Epoch [8/10], Batch [720/741], Loss: 0.6282\n",
      "Epoch [8/10], Batch [730/741], Loss: 0.5192\n",
      "Epoch [8/10], Batch [740/741], Loss: 0.5984\n",
      "Epoch 8 finished.\n",
      "Epoch [9/10], Batch [10/741], Loss: 0.3592\n",
      "Epoch [9/10], Batch [20/741], Loss: 0.4804\n",
      "Epoch [9/10], Batch [30/741], Loss: 0.4945\n",
      "Epoch [9/10], Batch [40/741], Loss: 0.4739\n",
      "Epoch [9/10], Batch [50/741], Loss: 0.4285\n",
      "Epoch [9/10], Batch [60/741], Loss: 0.3380\n",
      "Epoch [9/10], Batch [70/741], Loss: 0.3927\n",
      "Epoch [9/10], Batch [80/741], Loss: 0.4223\n",
      "Epoch [9/10], Batch [90/741], Loss: 0.4693\n",
      "Epoch [9/10], Batch [100/741], Loss: 0.3593\n",
      "Epoch [9/10], Batch [110/741], Loss: 0.3868\n",
      "Epoch [9/10], Batch [120/741], Loss: 0.4351\n",
      "Epoch [9/10], Batch [130/741], Loss: 0.3498\n",
      "Epoch [9/10], Batch [140/741], Loss: 0.4240\n",
      "Epoch [9/10], Batch [150/741], Loss: 0.3735\n",
      "Epoch [9/10], Batch [160/741], Loss: 0.3712\n",
      "Epoch [9/10], Batch [170/741], Loss: 0.3274\n",
      "Epoch [9/10], Batch [180/741], Loss: 0.5097\n",
      "Epoch [9/10], Batch [190/741], Loss: 0.3896\n",
      "Epoch [9/10], Batch [200/741], Loss: 0.3700\n",
      "Epoch [9/10], Batch [210/741], Loss: 0.4927\n",
      "Epoch [9/10], Batch [220/741], Loss: 0.3890\n",
      "Epoch [9/10], Batch [230/741], Loss: 0.4723\n",
      "Epoch [9/10], Batch [240/741], Loss: 0.4072\n",
      "Epoch [9/10], Batch [250/741], Loss: 0.5045\n",
      "Epoch [9/10], Batch [260/741], Loss: 0.4397\n",
      "Epoch [9/10], Batch [270/741], Loss: 0.4028\n",
      "Epoch [9/10], Batch [280/741], Loss: 0.5131\n",
      "Epoch [9/10], Batch [290/741], Loss: 0.3119\n",
      "Epoch [9/10], Batch [300/741], Loss: 0.3396\n",
      "Epoch [9/10], Batch [310/741], Loss: 0.4858\n",
      "Epoch [9/10], Batch [320/741], Loss: 0.4961\n",
      "Epoch [9/10], Batch [330/741], Loss: 0.5212\n",
      "Epoch [9/10], Batch [340/741], Loss: 0.4877\n",
      "Epoch [9/10], Batch [350/741], Loss: 0.4609\n",
      "Epoch [9/10], Batch [360/741], Loss: 0.4672\n",
      "Epoch [9/10], Batch [370/741], Loss: 0.3045\n",
      "Epoch [9/10], Batch [380/741], Loss: 0.4138\n",
      "Epoch [9/10], Batch [390/741], Loss: 0.3893\n",
      "Epoch [9/10], Batch [400/741], Loss: 0.4941\n",
      "Epoch [9/10], Batch [410/741], Loss: 0.3640\n",
      "Epoch [9/10], Batch [420/741], Loss: 0.4235\n",
      "Epoch [9/10], Batch [430/741], Loss: 0.3804\n",
      "Epoch [9/10], Batch [440/741], Loss: 0.4371\n",
      "Epoch [9/10], Batch [450/741], Loss: 0.4060\n",
      "Epoch [9/10], Batch [460/741], Loss: 0.3584\n",
      "Epoch [9/10], Batch [470/741], Loss: 0.3722\n",
      "Epoch [9/10], Batch [480/741], Loss: 0.3614\n",
      "Epoch [9/10], Batch [490/741], Loss: 0.4227\n",
      "Epoch [9/10], Batch [500/741], Loss: 0.3622\n",
      "Epoch [9/10], Batch [510/741], Loss: 0.4974\n",
      "Epoch [9/10], Batch [520/741], Loss: 0.4905\n",
      "Epoch [9/10], Batch [530/741], Loss: 0.3975\n",
      "Epoch [9/10], Batch [540/741], Loss: 0.4370\n",
      "Epoch [9/10], Batch [550/741], Loss: 0.4255\n",
      "Epoch [9/10], Batch [560/741], Loss: 0.4550\n",
      "Epoch [9/10], Batch [570/741], Loss: 0.3739\n",
      "Epoch [9/10], Batch [580/741], Loss: 0.4429\n",
      "Epoch [9/10], Batch [590/741], Loss: 0.4216\n",
      "Epoch [9/10], Batch [600/741], Loss: 0.3197\n",
      "Epoch [9/10], Batch [610/741], Loss: 0.4964\n",
      "Epoch [9/10], Batch [620/741], Loss: 0.4712\n",
      "Epoch [9/10], Batch [630/741], Loss: 0.4897\n",
      "Epoch [9/10], Batch [640/741], Loss: 0.4674\n",
      "Epoch [9/10], Batch [650/741], Loss: 0.3395\n",
      "Epoch [9/10], Batch [660/741], Loss: 0.4321\n",
      "Epoch [9/10], Batch [670/741], Loss: 0.4347\n",
      "Epoch [9/10], Batch [680/741], Loss: 0.2806\n",
      "Epoch [9/10], Batch [690/741], Loss: 0.3953\n",
      "Epoch [9/10], Batch [700/741], Loss: 0.4592\n",
      "Epoch [9/10], Batch [710/741], Loss: 0.4150\n",
      "Epoch [9/10], Batch [720/741], Loss: 0.2993\n",
      "Epoch [9/10], Batch [730/741], Loss: 0.4831\n",
      "Epoch [9/10], Batch [740/741], Loss: 0.3771\n",
      "Epoch 9 finished.\n",
      "Epoch [10/10], Batch [10/741], Loss: 0.2951\n",
      "Epoch [10/10], Batch [20/741], Loss: 0.2794\n",
      "Epoch [10/10], Batch [30/741], Loss: 0.3602\n",
      "Epoch [10/10], Batch [40/741], Loss: 0.2620\n",
      "Epoch [10/10], Batch [50/741], Loss: 0.3600\n",
      "Epoch [10/10], Batch [60/741], Loss: 0.3062\n",
      "Epoch [10/10], Batch [70/741], Loss: 0.3027\n",
      "Epoch [10/10], Batch [80/741], Loss: 0.2852\n",
      "Epoch [10/10], Batch [90/741], Loss: 0.3200\n",
      "Epoch [10/10], Batch [100/741], Loss: 0.3557\n",
      "Epoch [10/10], Batch [110/741], Loss: 0.4064\n",
      "Epoch [10/10], Batch [120/741], Loss: 0.3334\n",
      "Epoch [10/10], Batch [130/741], Loss: 0.3045\n",
      "Epoch [10/10], Batch [140/741], Loss: 0.3158\n",
      "Epoch [10/10], Batch [150/741], Loss: 0.2975\n",
      "Epoch [10/10], Batch [160/741], Loss: 0.3176\n",
      "Epoch [10/10], Batch [170/741], Loss: 0.3793\n",
      "Epoch [10/10], Batch [180/741], Loss: 0.3050\n",
      "Epoch [10/10], Batch [190/741], Loss: 0.3213\n",
      "Epoch [10/10], Batch [200/741], Loss: 0.3495\n",
      "Epoch [10/10], Batch [210/741], Loss: 0.3433\n",
      "Epoch [10/10], Batch [220/741], Loss: 0.2868\n",
      "Epoch [10/10], Batch [230/741], Loss: 0.3413\n",
      "Epoch [10/10], Batch [240/741], Loss: 0.3392\n",
      "Epoch [10/10], Batch [250/741], Loss: 0.3205\n",
      "Epoch [10/10], Batch [260/741], Loss: 0.3160\n",
      "Epoch [10/10], Batch [270/741], Loss: 0.3095\n",
      "Epoch [10/10], Batch [280/741], Loss: 0.3412\n",
      "Epoch [10/10], Batch [290/741], Loss: 0.3829\n",
      "Epoch [10/10], Batch [300/741], Loss: 0.2683\n",
      "Epoch [10/10], Batch [310/741], Loss: 0.3466\n",
      "Epoch [10/10], Batch [320/741], Loss: 0.2687\n",
      "Epoch [10/10], Batch [330/741], Loss: 0.2896\n",
      "Epoch [10/10], Batch [340/741], Loss: 0.3107\n",
      "Epoch [10/10], Batch [350/741], Loss: 0.2928\n",
      "Epoch [10/10], Batch [360/741], Loss: 0.3056\n",
      "Epoch [10/10], Batch [370/741], Loss: 0.3303\n",
      "Epoch [10/10], Batch [380/741], Loss: 0.3100\n",
      "Epoch [10/10], Batch [390/741], Loss: 0.3520\n",
      "Epoch [10/10], Batch [400/741], Loss: 0.2987\n",
      "Epoch [10/10], Batch [410/741], Loss: 0.3633\n",
      "Epoch [10/10], Batch [420/741], Loss: 0.3179\n",
      "Epoch [10/10], Batch [430/741], Loss: 0.2874\n",
      "Epoch [10/10], Batch [440/741], Loss: 0.3015\n",
      "Epoch [10/10], Batch [450/741], Loss: 0.3222\n",
      "Epoch [10/10], Batch [460/741], Loss: 0.2657\n",
      "Epoch [10/10], Batch [470/741], Loss: 0.3902\n",
      "Epoch [10/10], Batch [480/741], Loss: 0.3274\n",
      "Epoch [10/10], Batch [490/741], Loss: 0.3348\n",
      "Epoch [10/10], Batch [500/741], Loss: 0.3111\n",
      "Epoch [10/10], Batch [510/741], Loss: 0.3804\n",
      "Epoch [10/10], Batch [520/741], Loss: 0.3369\n",
      "Epoch [10/10], Batch [530/741], Loss: 0.4290\n",
      "Epoch [10/10], Batch [540/741], Loss: 0.3640\n",
      "Epoch [10/10], Batch [550/741], Loss: 0.3552\n",
      "Epoch [10/10], Batch [560/741], Loss: 0.3254\n",
      "Epoch [10/10], Batch [570/741], Loss: 0.2946\n",
      "Epoch [10/10], Batch [580/741], Loss: 0.3124\n",
      "Epoch [10/10], Batch [590/741], Loss: 0.3208\n",
      "Epoch [10/10], Batch [600/741], Loss: 0.3798\n",
      "Epoch [10/10], Batch [610/741], Loss: 0.3370\n",
      "Epoch [10/10], Batch [620/741], Loss: 0.3610\n",
      "Epoch [10/10], Batch [630/741], Loss: 0.2388\n",
      "Epoch [10/10], Batch [640/741], Loss: 0.2608\n",
      "Epoch [10/10], Batch [650/741], Loss: 0.4102\n",
      "Epoch [10/10], Batch [660/741], Loss: 0.3653\n",
      "Epoch [10/10], Batch [670/741], Loss: 0.2991\n",
      "Epoch [10/10], Batch [680/741], Loss: 0.2853\n",
      "Epoch [10/10], Batch [690/741], Loss: 0.3273\n",
      "Epoch [10/10], Batch [700/741], Loss: 0.3151\n",
      "Epoch [10/10], Batch [710/741], Loss: 0.3032\n",
      "Epoch [10/10], Batch [720/741], Loss: 0.3016\n",
      "Epoch [10/10], Batch [730/741], Loss: 0.2392\n",
      "Epoch [10/10], Batch [740/741], Loss: 0.3471\n",
      "Epoch 10 finished.\n",
      "Progress: 400/2975 images evaluated.\n",
      "Progress: 800/2975 images evaluated.\n",
      "Progress: 1200/2975 images evaluated.\n",
      "Progress: 1600/2975 images evaluated.\n",
      "Progress: 2000/2975 images evaluated.\n",
      "Progress: 2400/2975 images evaluated.\n",
      "Progress: 2800/2975 images evaluated.\n",
      "Accuracy: 63.90%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load Data from JSON\n",
    "with open('C:\\\\Users\\\\hp\\\\Downloads\\\\DataVIT.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Flatten the image data\n",
    "image_data = []\n",
    "for item in data:\n",
    "    for img in item['images']:\n",
    "        image_data.append({'url': img['url'], 'label': item['interests'][0]})\n",
    "\n",
    "# Collect paths to all .pt files\n",
    "preprocessed_files = sorted([f'C:\\\\Users\\\\hp\\\\Downloads\\\\preprocessed_images_combined\\\\{filename}' \n",
    "                             for filename in os.listdir('C:\\\\Users\\\\hp\\\\Downloads\\\\preprocessed_images_combined') \n",
    "                             if filename.endswith('.pt')])\n",
    "\n",
    "# Create a mapping from filename to preprocessed file path\n",
    "preprocessed_file_map = {os.path.basename(file): file for file in preprocessed_files}\n",
    "\n",
    "# Step 2: Split Data into Training and Test Sets\n",
    "train_data, test_data = train_test_split(image_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Filter the train data to include only entries that have a corresponding preprocessed file\n",
    "filtered_train_data = [entry for entry in train_data if os.path.basename(entry['url']).replace('.jpg', '.pt') in preprocessed_file_map]\n",
    "\n",
    "# Filter the test data to include only entries that have a corresponding preprocessed file\n",
    "filtered_test_data = [entry for entry in test_data if os.path.basename(entry['url']).replace('.jpg', '.pt') in preprocessed_file_map]\n",
    "\n",
    "# Verify the length of filtered_train_data, filtered_test_data and preprocessed_files\n",
    "print(f\"Number of filtered train data entries: {len(filtered_train_data)}\")\n",
    "print(f\"Number of filtered test data entries: {len(filtered_test_data)}\")\n",
    "print(f\"Number of preprocessed files: {len(preprocessed_files)}\")\n",
    "\n",
    "# Map the preprocessed files to train and test data\n",
    "preprocessed_train_files = [preprocessed_file_map[os.path.basename(entry['url']).replace('.jpg', '.pt')] for entry in filtered_train_data]\n",
    "preprocessed_test_files = [preprocessed_file_map[os.path.basename(entry['url']).replace('.jpg', '.pt')] for entry in filtered_test_data]\n",
    "\n",
    "# Step 3: Create the Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, preprocessed_files, transform=None):\n",
    "        self.data = data\n",
    "        self.preprocessed_files = preprocessed_files\n",
    "        self.transform = transform\n",
    "\n",
    "        # Ensure the lengths match\n",
    "        assert len(self.data) == len(self.preprocessed_files), (\n",
    "            f\"Data length {len(self.data)} does not match number of preprocessed files {len(self.preprocessed_files)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        preprocessed_image = torch.load(self.preprocessed_files[idx])  # Load the preprocessed tensor from file\n",
    "        label = self.data[idx]['label']\n",
    "        if self.transform:\n",
    "            preprocessed_image = self.transform(preprocessed_image)\n",
    "        return preprocessed_image, label\n",
    "\n",
    "# Define transformations (including data augmentation)\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(filtered_train_data, preprocessed_train_files, transform=transform)\n",
    "test_dataset = CustomDataset(filtered_test_data, preprocessed_test_files, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Step 4: Create the ViT Model\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=len(set([item['label'] for item in filtered_train_data])),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Step 5: Train the Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # Print every 10 batches\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {running_loss / 10:.4f}')\n",
    "            running_loss = 0.0\n",
    "    print(f'Epoch {epoch+1} finished.')\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total % 100 == 0:  # Print every 100 images\n",
    "            print(f'Progress: {total}/{len(test_loader.dataset)} images evaluated.')\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bd01e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([13, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [10/741], Loss: 2.5717\n",
      "Epoch [1/10], Batch [20/741], Loss: 2.4114\n",
      "Epoch [1/10], Batch [30/741], Loss: 2.4064\n",
      "Epoch [1/10], Batch [40/741], Loss: 2.3562\n",
      "Epoch [1/10], Batch [50/741], Loss: 2.3055\n",
      "Epoch [1/10], Batch [60/741], Loss: 2.3425\n",
      "Epoch [1/10], Batch [70/741], Loss: 2.2487\n",
      "Epoch [1/10], Batch [80/741], Loss: 2.2641\n",
      "Epoch [1/10], Batch [90/741], Loss: 2.1757\n",
      "Epoch [1/10], Batch [100/741], Loss: 2.1765\n",
      "Epoch [1/10], Batch [110/741], Loss: 2.1063\n",
      "Epoch [1/10], Batch [120/741], Loss: 2.1868\n",
      "Epoch [1/10], Batch [130/741], Loss: 2.0708\n",
      "Epoch [1/10], Batch [140/741], Loss: 2.0361\n",
      "Epoch [1/10], Batch [150/741], Loss: 2.0284\n",
      "Epoch [1/10], Batch [160/741], Loss: 1.9470\n",
      "Epoch [1/10], Batch [170/741], Loss: 2.0512\n",
      "Epoch [1/10], Batch [180/741], Loss: 2.1432\n",
      "Epoch [1/10], Batch [190/741], Loss: 2.0070\n",
      "Epoch [1/10], Batch [200/741], Loss: 1.9716\n",
      "Epoch [1/10], Batch [210/741], Loss: 1.8476\n",
      "Epoch [1/10], Batch [220/741], Loss: 1.8850\n",
      "Epoch [1/10], Batch [230/741], Loss: 1.9932\n",
      "Epoch [1/10], Batch [240/741], Loss: 1.8006\n",
      "Epoch [1/10], Batch [250/741], Loss: 1.9431\n",
      "Epoch [1/10], Batch [260/741], Loss: 1.8133\n",
      "Epoch [1/10], Batch [270/741], Loss: 1.9604\n",
      "Epoch [1/10], Batch [280/741], Loss: 1.8040\n",
      "Epoch [1/10], Batch [290/741], Loss: 1.8912\n",
      "Epoch [1/10], Batch [300/741], Loss: 1.7389\n",
      "Epoch [1/10], Batch [310/741], Loss: 1.7416\n",
      "Epoch [1/10], Batch [320/741], Loss: 1.7448\n",
      "Epoch [1/10], Batch [330/741], Loss: 1.8327\n",
      "Epoch [1/10], Batch [340/741], Loss: 1.7015\n",
      "Epoch [1/10], Batch [350/741], Loss: 1.8408\n",
      "Epoch [1/10], Batch [360/741], Loss: 1.7926\n",
      "Epoch [1/10], Batch [370/741], Loss: 1.6732\n",
      "Epoch [1/10], Batch [380/741], Loss: 1.7081\n",
      "Epoch [1/10], Batch [390/741], Loss: 1.8854\n",
      "Epoch [1/10], Batch [400/741], Loss: 1.8367\n",
      "Epoch [1/10], Batch [410/741], Loss: 1.6218\n",
      "Epoch [1/10], Batch [420/741], Loss: 1.6384\n",
      "Epoch [1/10], Batch [430/741], Loss: 1.8365\n",
      "Epoch [1/10], Batch [440/741], Loss: 1.7702\n",
      "Epoch [1/10], Batch [450/741], Loss: 1.7009\n",
      "Epoch [1/10], Batch [460/741], Loss: 1.8219\n",
      "Epoch [1/10], Batch [470/741], Loss: 1.5698\n",
      "Epoch [1/10], Batch [480/741], Loss: 1.7168\n",
      "Epoch [1/10], Batch [490/741], Loss: 1.8314\n",
      "Epoch [1/10], Batch [500/741], Loss: 1.8561\n",
      "Epoch [1/10], Batch [510/741], Loss: 1.7948\n",
      "Epoch [1/10], Batch [520/741], Loss: 1.6922\n",
      "Epoch [1/10], Batch [530/741], Loss: 1.6254\n",
      "Epoch [1/10], Batch [540/741], Loss: 1.8882\n",
      "Epoch [1/10], Batch [550/741], Loss: 1.6182\n",
      "Epoch [1/10], Batch [560/741], Loss: 1.6554\n",
      "Epoch [1/10], Batch [570/741], Loss: 1.6842\n",
      "Epoch [1/10], Batch [580/741], Loss: 1.5702\n",
      "Epoch [1/10], Batch [590/741], Loss: 1.4001\n",
      "Epoch [1/10], Batch [600/741], Loss: 1.6736\n",
      "Epoch [1/10], Batch [610/741], Loss: 1.8010\n",
      "Epoch [1/10], Batch [620/741], Loss: 1.7216\n",
      "Epoch [1/10], Batch [630/741], Loss: 1.6852\n",
      "Epoch [1/10], Batch [640/741], Loss: 1.6206\n",
      "Epoch [1/10], Batch [650/741], Loss: 1.5234\n",
      "Epoch [1/10], Batch [660/741], Loss: 1.6043\n",
      "Epoch [1/10], Batch [670/741], Loss: 1.4879\n",
      "Epoch [1/10], Batch [680/741], Loss: 1.6683\n",
      "Epoch [1/10], Batch [690/741], Loss: 1.5825\n",
      "Epoch [1/10], Batch [700/741], Loss: 1.5979\n",
      "Epoch [1/10], Batch [710/741], Loss: 1.6368\n",
      "Epoch [1/10], Batch [720/741], Loss: 1.5241\n",
      "Epoch [1/10], Batch [730/741], Loss: 1.6630\n",
      "Epoch [1/10], Batch [740/741], Loss: 1.6608\n",
      "Epoch 1 finished.\n",
      "Epoch [2/10], Batch [10/741], Loss: 1.5860\n",
      "Epoch [2/10], Batch [20/741], Loss: 1.5109\n",
      "Epoch [2/10], Batch [30/741], Loss: 1.4117\n",
      "Epoch [2/10], Batch [40/741], Loss: 1.3493\n",
      "Epoch [2/10], Batch [50/741], Loss: 1.3384\n",
      "Epoch [2/10], Batch [60/741], Loss: 1.4620\n",
      "Epoch [2/10], Batch [70/741], Loss: 1.3457\n",
      "Epoch [2/10], Batch [80/741], Loss: 1.4639\n",
      "Epoch [2/10], Batch [90/741], Loss: 1.4532\n",
      "Epoch [2/10], Batch [100/741], Loss: 1.4913\n",
      "Epoch [2/10], Batch [110/741], Loss: 1.2291\n",
      "Epoch [2/10], Batch [120/741], Loss: 1.4716\n",
      "Epoch [2/10], Batch [130/741], Loss: 1.4775\n",
      "Epoch [2/10], Batch [140/741], Loss: 1.4418\n",
      "Epoch [2/10], Batch [150/741], Loss: 1.3401\n",
      "Epoch [2/10], Batch [160/741], Loss: 1.3630\n",
      "Epoch [2/10], Batch [170/741], Loss: 1.4311\n",
      "Epoch [2/10], Batch [180/741], Loss: 1.4099\n",
      "Epoch [2/10], Batch [190/741], Loss: 1.5267\n",
      "Epoch [2/10], Batch [200/741], Loss: 1.5175\n",
      "Epoch [2/10], Batch [210/741], Loss: 1.3114\n",
      "Epoch [2/10], Batch [220/741], Loss: 1.5533\n",
      "Epoch [2/10], Batch [230/741], Loss: 1.5624\n",
      "Epoch [2/10], Batch [240/741], Loss: 1.3783\n",
      "Epoch [2/10], Batch [250/741], Loss: 1.5530\n",
      "Epoch [2/10], Batch [260/741], Loss: 1.3975\n",
      "Epoch [2/10], Batch [270/741], Loss: 1.4771\n",
      "Epoch [2/10], Batch [280/741], Loss: 1.4749\n",
      "Epoch [2/10], Batch [290/741], Loss: 1.3448\n",
      "Epoch [2/10], Batch [300/741], Loss: 1.4721\n",
      "Epoch [2/10], Batch [310/741], Loss: 1.2479\n",
      "Epoch [2/10], Batch [320/741], Loss: 1.4036\n",
      "Epoch [2/10], Batch [330/741], Loss: 1.4930\n",
      "Epoch [2/10], Batch [340/741], Loss: 1.3886\n",
      "Epoch [2/10], Batch [350/741], Loss: 1.3833\n",
      "Epoch [2/10], Batch [360/741], Loss: 1.4684\n",
      "Epoch [2/10], Batch [370/741], Loss: 1.5390\n",
      "Epoch [2/10], Batch [380/741], Loss: 1.4806\n",
      "Epoch [2/10], Batch [390/741], Loss: 1.2271\n",
      "Epoch [2/10], Batch [400/741], Loss: 1.3696\n",
      "Epoch [2/10], Batch [410/741], Loss: 1.4113\n",
      "Epoch [2/10], Batch [420/741], Loss: 1.2915\n",
      "Epoch [2/10], Batch [430/741], Loss: 1.3455\n",
      "Epoch [2/10], Batch [440/741], Loss: 1.3702\n",
      "Epoch [2/10], Batch [450/741], Loss: 1.2866\n",
      "Epoch [2/10], Batch [460/741], Loss: 1.2823\n",
      "Epoch [2/10], Batch [470/741], Loss: 1.5440\n",
      "Epoch [2/10], Batch [480/741], Loss: 1.2673\n",
      "Epoch [2/10], Batch [490/741], Loss: 1.2417\n",
      "Epoch [2/10], Batch [500/741], Loss: 1.5113\n",
      "Epoch [2/10], Batch [510/741], Loss: 1.3311\n",
      "Epoch [2/10], Batch [520/741], Loss: 1.2330\n",
      "Epoch [2/10], Batch [530/741], Loss: 1.2866\n",
      "Epoch [2/10], Batch [540/741], Loss: 1.2951\n",
      "Epoch [2/10], Batch [550/741], Loss: 1.2169\n",
      "Epoch [2/10], Batch [560/741], Loss: 1.2990\n",
      "Epoch [2/10], Batch [570/741], Loss: 1.5837\n",
      "Epoch [2/10], Batch [580/741], Loss: 1.4933\n",
      "Epoch [2/10], Batch [590/741], Loss: 1.2472\n",
      "Epoch [2/10], Batch [600/741], Loss: 1.4328\n",
      "Epoch [2/10], Batch [610/741], Loss: 1.2420\n",
      "Epoch [2/10], Batch [620/741], Loss: 1.3914\n",
      "Epoch [2/10], Batch [630/741], Loss: 1.4599\n",
      "Epoch [2/10], Batch [640/741], Loss: 1.3599\n",
      "Epoch [2/10], Batch [650/741], Loss: 1.3469\n",
      "Epoch [2/10], Batch [660/741], Loss: 1.3076\n",
      "Epoch [2/10], Batch [670/741], Loss: 1.5211\n",
      "Epoch [2/10], Batch [680/741], Loss: 1.2224\n",
      "Epoch [2/10], Batch [690/741], Loss: 1.2456\n",
      "Epoch [2/10], Batch [700/741], Loss: 1.3081\n",
      "Epoch [2/10], Batch [710/741], Loss: 1.4188\n",
      "Epoch [2/10], Batch [720/741], Loss: 1.2820\n",
      "Epoch [2/10], Batch [730/741], Loss: 1.4876\n",
      "Epoch [2/10], Batch [740/741], Loss: 1.2671\n",
      "Epoch 2 finished.\n",
      "Epoch [3/10], Batch [10/741], Loss: 1.0853\n",
      "Epoch [3/10], Batch [20/741], Loss: 1.0984\n",
      "Epoch [3/10], Batch [30/741], Loss: 1.2216\n",
      "Epoch [3/10], Batch [40/741], Loss: 1.0502\n",
      "Epoch [3/10], Batch [50/741], Loss: 1.2649\n",
      "Epoch [3/10], Batch [60/741], Loss: 0.8756\n",
      "Epoch [3/10], Batch [70/741], Loss: 1.0436\n",
      "Epoch [3/10], Batch [80/741], Loss: 1.1241\n",
      "Epoch [3/10], Batch [90/741], Loss: 1.2098\n",
      "Epoch [3/10], Batch [100/741], Loss: 1.2162\n",
      "Epoch [3/10], Batch [110/741], Loss: 1.2661\n",
      "Epoch [3/10], Batch [120/741], Loss: 1.0292\n",
      "Epoch [3/10], Batch [130/741], Loss: 1.1127\n",
      "Epoch [3/10], Batch [140/741], Loss: 1.1134\n",
      "Epoch [3/10], Batch [150/741], Loss: 0.9969\n",
      "Epoch [3/10], Batch [160/741], Loss: 1.0045\n",
      "Epoch [3/10], Batch [170/741], Loss: 1.0760\n",
      "Epoch [3/10], Batch [180/741], Loss: 1.0929\n",
      "Epoch [3/10], Batch [190/741], Loss: 1.1520\n",
      "Epoch [3/10], Batch [200/741], Loss: 1.0243\n",
      "Epoch [3/10], Batch [210/741], Loss: 1.0905\n",
      "Epoch [3/10], Batch [220/741], Loss: 0.9831\n",
      "Epoch [3/10], Batch [230/741], Loss: 1.1028\n",
      "Epoch [3/10], Batch [240/741], Loss: 1.1326\n",
      "Epoch [3/10], Batch [250/741], Loss: 1.0928\n",
      "Epoch [3/10], Batch [260/741], Loss: 1.1243\n",
      "Epoch [3/10], Batch [270/741], Loss: 1.2620\n",
      "Epoch [3/10], Batch [280/741], Loss: 1.1516\n",
      "Epoch [3/10], Batch [290/741], Loss: 0.9722\n",
      "Epoch [3/10], Batch [300/741], Loss: 1.1565\n",
      "Epoch [3/10], Batch [310/741], Loss: 1.0269\n",
      "Epoch [3/10], Batch [320/741], Loss: 1.0484\n",
      "Epoch [3/10], Batch [330/741], Loss: 1.1697\n",
      "Epoch [3/10], Batch [340/741], Loss: 1.1184\n",
      "Epoch [3/10], Batch [350/741], Loss: 1.1070\n",
      "Epoch [3/10], Batch [360/741], Loss: 1.1710\n",
      "Epoch [3/10], Batch [370/741], Loss: 1.1157\n",
      "Epoch [3/10], Batch [380/741], Loss: 1.1388\n",
      "Epoch [3/10], Batch [390/741], Loss: 0.9495\n",
      "Epoch [3/10], Batch [400/741], Loss: 1.0956\n",
      "Epoch [3/10], Batch [410/741], Loss: 1.0845\n",
      "Epoch [3/10], Batch [420/741], Loss: 1.0267\n",
      "Epoch [3/10], Batch [430/741], Loss: 1.2005\n",
      "Epoch [3/10], Batch [440/741], Loss: 1.0397\n",
      "Epoch [3/10], Batch [450/741], Loss: 1.0612\n",
      "Epoch [3/10], Batch [460/741], Loss: 0.9670\n",
      "Epoch [3/10], Batch [470/741], Loss: 1.3271\n",
      "Epoch [3/10], Batch [480/741], Loss: 1.0591\n",
      "Epoch [3/10], Batch [490/741], Loss: 1.1228\n",
      "Epoch [3/10], Batch [500/741], Loss: 1.2483\n",
      "Epoch [3/10], Batch [510/741], Loss: 1.1983\n",
      "Epoch [3/10], Batch [520/741], Loss: 1.1120\n",
      "Epoch [3/10], Batch [530/741], Loss: 1.1088\n",
      "Epoch [3/10], Batch [540/741], Loss: 1.1601\n",
      "Epoch [3/10], Batch [550/741], Loss: 1.2437\n",
      "Epoch [3/10], Batch [560/741], Loss: 1.0625\n",
      "Epoch [3/10], Batch [570/741], Loss: 1.0146\n",
      "Epoch [3/10], Batch [580/741], Loss: 1.3485\n",
      "Epoch [3/10], Batch [590/741], Loss: 1.0785\n",
      "Epoch [3/10], Batch [600/741], Loss: 1.0313\n",
      "Epoch [3/10], Batch [610/741], Loss: 1.0365\n",
      "Epoch [3/10], Batch [620/741], Loss: 1.1832\n",
      "Epoch [3/10], Batch [630/741], Loss: 1.2166\n",
      "Epoch [3/10], Batch [640/741], Loss: 1.1615\n",
      "Epoch [3/10], Batch [650/741], Loss: 1.2859\n",
      "Epoch [3/10], Batch [660/741], Loss: 0.9847\n",
      "Epoch [3/10], Batch [670/741], Loss: 1.1473\n",
      "Epoch [3/10], Batch [680/741], Loss: 1.1756\n",
      "Epoch [3/10], Batch [690/741], Loss: 1.1487\n",
      "Epoch [3/10], Batch [700/741], Loss: 1.1000\n",
      "Epoch [3/10], Batch [710/741], Loss: 1.1457\n",
      "Epoch [3/10], Batch [720/741], Loss: 1.0426\n",
      "Epoch [3/10], Batch [730/741], Loss: 1.1843\n",
      "Epoch [3/10], Batch [740/741], Loss: 0.9985\n",
      "Epoch 3 finished.\n",
      "Epoch [4/10], Batch [10/741], Loss: 0.7502\n",
      "Epoch [4/10], Batch [20/741], Loss: 0.8138\n",
      "Epoch [4/10], Batch [30/741], Loss: 0.8188\n",
      "Epoch [4/10], Batch [40/741], Loss: 0.7379\n",
      "Epoch [4/10], Batch [50/741], Loss: 0.9162\n",
      "Epoch [4/10], Batch [60/741], Loss: 0.8045\n",
      "Epoch [4/10], Batch [70/741], Loss: 0.7944\n",
      "Epoch [4/10], Batch [80/741], Loss: 0.8126\n",
      "Epoch [4/10], Batch [90/741], Loss: 0.8269\n",
      "Epoch [4/10], Batch [100/741], Loss: 0.8287\n",
      "Epoch [4/10], Batch [110/741], Loss: 0.7829\n",
      "Epoch [4/10], Batch [120/741], Loss: 0.8866\n",
      "Epoch [4/10], Batch [130/741], Loss: 0.7333\n",
      "Epoch [4/10], Batch [140/741], Loss: 0.9680\n",
      "Epoch [4/10], Batch [150/741], Loss: 0.9046\n",
      "Epoch [4/10], Batch [160/741], Loss: 0.8521\n",
      "Epoch [4/10], Batch [170/741], Loss: 0.8881\n",
      "Epoch [4/10], Batch [180/741], Loss: 0.8302\n",
      "Epoch [4/10], Batch [190/741], Loss: 1.0241\n",
      "Epoch [4/10], Batch [200/741], Loss: 0.9206\n",
      "Epoch [4/10], Batch [210/741], Loss: 0.9771\n",
      "Epoch [4/10], Batch [220/741], Loss: 0.7933\n",
      "Epoch [4/10], Batch [230/741], Loss: 0.8629\n",
      "Epoch [4/10], Batch [240/741], Loss: 0.7857\n",
      "Epoch [4/10], Batch [250/741], Loss: 0.8245\n",
      "Epoch [4/10], Batch [260/741], Loss: 0.8889\n",
      "Epoch [4/10], Batch [270/741], Loss: 0.8359\n",
      "Epoch [4/10], Batch [280/741], Loss: 0.9673\n",
      "Epoch [4/10], Batch [290/741], Loss: 0.8167\n",
      "Epoch [4/10], Batch [300/741], Loss: 0.9066\n",
      "Epoch [4/10], Batch [310/741], Loss: 0.9066\n",
      "Epoch [4/10], Batch [320/741], Loss: 0.8533\n",
      "Epoch [4/10], Batch [330/741], Loss: 0.9634\n",
      "Epoch [4/10], Batch [340/741], Loss: 0.8478\n",
      "Epoch [4/10], Batch [350/741], Loss: 0.8718\n",
      "Epoch [4/10], Batch [360/741], Loss: 0.7834\n",
      "Epoch [4/10], Batch [370/741], Loss: 0.8193\n",
      "Epoch [4/10], Batch [380/741], Loss: 0.9207\n",
      "Epoch [4/10], Batch [390/741], Loss: 0.8981\n",
      "Epoch [4/10], Batch [400/741], Loss: 0.8810\n",
      "Epoch [4/10], Batch [410/741], Loss: 0.8580\n",
      "Epoch [4/10], Batch [420/741], Loss: 0.9120\n",
      "Epoch [4/10], Batch [430/741], Loss: 0.9187\n",
      "Epoch [4/10], Batch [440/741], Loss: 0.7580\n",
      "Epoch [4/10], Batch [450/741], Loss: 0.8903\n",
      "Epoch [4/10], Batch [460/741], Loss: 0.8262\n",
      "Epoch [4/10], Batch [470/741], Loss: 0.9731\n",
      "Epoch [4/10], Batch [480/741], Loss: 0.7544\n",
      "Epoch [4/10], Batch [490/741], Loss: 0.8340\n",
      "Epoch [4/10], Batch [500/741], Loss: 0.8722\n",
      "Epoch [4/10], Batch [510/741], Loss: 0.9010\n",
      "Epoch [4/10], Batch [520/741], Loss: 0.7986\n",
      "Epoch [4/10], Batch [530/741], Loss: 0.8341\n",
      "Epoch [4/10], Batch [540/741], Loss: 0.9973\n",
      "Epoch [4/10], Batch [550/741], Loss: 0.8894\n",
      "Epoch [4/10], Batch [560/741], Loss: 0.8249\n",
      "Epoch [4/10], Batch [570/741], Loss: 0.8735\n",
      "Epoch [4/10], Batch [580/741], Loss: 0.8475\n",
      "Epoch [4/10], Batch [590/741], Loss: 0.9768\n",
      "Epoch [4/10], Batch [600/741], Loss: 0.7455\n",
      "Epoch [4/10], Batch [610/741], Loss: 0.7823\n",
      "Epoch [4/10], Batch [620/741], Loss: 0.8430\n",
      "Epoch [4/10], Batch [630/741], Loss: 0.8590\n",
      "Epoch [4/10], Batch [640/741], Loss: 0.9966\n",
      "Epoch [4/10], Batch [650/741], Loss: 0.8437\n",
      "Epoch [4/10], Batch [660/741], Loss: 0.8983\n",
      "Epoch [4/10], Batch [670/741], Loss: 0.9248\n",
      "Epoch [4/10], Batch [680/741], Loss: 0.7708\n",
      "Epoch [4/10], Batch [690/741], Loss: 0.8844\n",
      "Epoch [4/10], Batch [700/741], Loss: 0.8544\n",
      "Epoch [4/10], Batch [710/741], Loss: 0.8622\n",
      "Epoch [4/10], Batch [720/741], Loss: 0.8124\n",
      "Epoch [4/10], Batch [730/741], Loss: 0.8958\n",
      "Epoch [4/10], Batch [740/741], Loss: 0.9045\n",
      "Epoch 4 finished.\n",
      "Epoch [5/10], Batch [10/741], Loss: 0.6682\n",
      "Epoch [5/10], Batch [20/741], Loss: 0.7050\n",
      "Epoch [5/10], Batch [30/741], Loss: 0.6157\n",
      "Epoch [5/10], Batch [40/741], Loss: 0.6729\n",
      "Epoch [5/10], Batch [50/741], Loss: 0.6587\n",
      "Epoch [5/10], Batch [60/741], Loss: 0.6115\n",
      "Epoch [5/10], Batch [70/741], Loss: 0.7943\n",
      "Epoch [5/10], Batch [80/741], Loss: 0.6147\n",
      "Epoch [5/10], Batch [90/741], Loss: 0.6304\n",
      "Epoch [5/10], Batch [100/741], Loss: 0.6727\n",
      "Epoch [5/10], Batch [110/741], Loss: 0.6600\n",
      "Epoch [5/10], Batch [120/741], Loss: 0.6979\n",
      "Epoch [5/10], Batch [130/741], Loss: 0.6622\n",
      "Epoch [5/10], Batch [140/741], Loss: 0.5281\n",
      "Epoch [5/10], Batch [150/741], Loss: 0.6999\n",
      "Epoch [5/10], Batch [160/741], Loss: 0.6109\n",
      "Epoch [5/10], Batch [170/741], Loss: 0.5837\n",
      "Epoch [5/10], Batch [180/741], Loss: 0.7299\n",
      "Epoch [5/10], Batch [190/741], Loss: 0.5637\n",
      "Epoch [5/10], Batch [200/741], Loss: 0.6599\n",
      "Epoch [5/10], Batch [210/741], Loss: 0.6247\n",
      "Epoch [5/10], Batch [220/741], Loss: 0.5444\n",
      "Epoch [5/10], Batch [230/741], Loss: 0.5807\n",
      "Epoch [5/10], Batch [240/741], Loss: 0.6994\n",
      "Epoch [5/10], Batch [250/741], Loss: 0.6509\n",
      "Epoch [5/10], Batch [260/741], Loss: 0.7068\n",
      "Epoch [5/10], Batch [270/741], Loss: 0.6060\n",
      "Epoch [5/10], Batch [280/741], Loss: 0.6683\n",
      "Epoch [5/10], Batch [290/741], Loss: 0.5052\n",
      "Epoch [5/10], Batch [300/741], Loss: 0.5642\n",
      "Epoch [5/10], Batch [310/741], Loss: 0.6328\n",
      "Epoch [5/10], Batch [320/741], Loss: 0.5131\n",
      "Epoch [5/10], Batch [330/741], Loss: 0.6490\n",
      "Epoch [5/10], Batch [340/741], Loss: 0.7501\n",
      "Epoch [5/10], Batch [350/741], Loss: 0.6282\n",
      "Epoch [5/10], Batch [360/741], Loss: 0.6491\n",
      "Epoch [5/10], Batch [370/741], Loss: 0.5583\n",
      "Epoch [5/10], Batch [380/741], Loss: 0.6999\n",
      "Epoch [5/10], Batch [390/741], Loss: 0.6811\n",
      "Epoch [5/10], Batch [400/741], Loss: 0.5760\n",
      "Epoch [5/10], Batch [410/741], Loss: 0.7502\n",
      "Epoch [5/10], Batch [420/741], Loss: 0.5881\n",
      "Epoch [5/10], Batch [430/741], Loss: 0.6088\n",
      "Epoch [5/10], Batch [440/741], Loss: 0.6989\n",
      "Epoch [5/10], Batch [450/741], Loss: 0.5784\n",
      "Epoch [5/10], Batch [460/741], Loss: 0.6775\n",
      "Epoch [5/10], Batch [470/741], Loss: 0.6555\n",
      "Epoch [5/10], Batch [480/741], Loss: 0.5185\n",
      "Epoch [5/10], Batch [490/741], Loss: 0.6345\n",
      "Epoch [5/10], Batch [500/741], Loss: 0.6488\n",
      "Epoch [5/10], Batch [510/741], Loss: 0.5771\n",
      "Epoch [5/10], Batch [520/741], Loss: 0.6091\n",
      "Epoch [5/10], Batch [530/741], Loss: 0.5339\n",
      "Epoch [5/10], Batch [540/741], Loss: 0.6532\n",
      "Epoch [5/10], Batch [550/741], Loss: 0.5757\n",
      "Epoch [5/10], Batch [560/741], Loss: 0.6453\n",
      "Epoch [5/10], Batch [570/741], Loss: 0.6355\n",
      "Epoch [5/10], Batch [580/741], Loss: 0.6242\n",
      "Epoch [5/10], Batch [590/741], Loss: 0.7290\n",
      "Epoch [5/10], Batch [600/741], Loss: 0.5085\n",
      "Epoch [5/10], Batch [610/741], Loss: 0.5461\n",
      "Epoch [5/10], Batch [620/741], Loss: 0.6240\n",
      "Epoch [5/10], Batch [630/741], Loss: 0.6237\n",
      "Epoch [5/10], Batch [640/741], Loss: 0.5561\n",
      "Epoch [5/10], Batch [650/741], Loss: 0.5815\n",
      "Epoch [5/10], Batch [660/741], Loss: 0.6463\n",
      "Epoch [5/10], Batch [670/741], Loss: 0.5539\n",
      "Epoch [5/10], Batch [680/741], Loss: 0.5512\n",
      "Epoch [5/10], Batch [690/741], Loss: 0.5811\n",
      "Epoch [5/10], Batch [700/741], Loss: 0.5849\n",
      "Epoch [5/10], Batch [710/741], Loss: 0.5993\n",
      "Epoch [5/10], Batch [720/741], Loss: 0.7781\n",
      "Epoch [5/10], Batch [730/741], Loss: 0.5239\n",
      "Epoch [5/10], Batch [740/741], Loss: 0.6835\n",
      "Epoch 5 finished.\n",
      "Epoch [6/10], Batch [10/741], Loss: 0.4149\n",
      "Epoch [6/10], Batch [20/741], Loss: 0.4991\n",
      "Epoch [6/10], Batch [30/741], Loss: 0.4778\n",
      "Epoch [6/10], Batch [40/741], Loss: 0.5099\n",
      "Epoch [6/10], Batch [50/741], Loss: 0.4278\n",
      "Epoch [6/10], Batch [60/741], Loss: 0.4229\n",
      "Epoch [6/10], Batch [70/741], Loss: 0.4465\n",
      "Epoch [6/10], Batch [80/741], Loss: 0.3966\n",
      "Epoch [6/10], Batch [90/741], Loss: 0.4282\n",
      "Epoch [6/10], Batch [100/741], Loss: 0.4523\n",
      "Epoch [6/10], Batch [110/741], Loss: 0.4163\n",
      "Epoch [6/10], Batch [120/741], Loss: 0.4817\n",
      "Epoch [6/10], Batch [130/741], Loss: 0.4594\n",
      "Epoch [6/10], Batch [140/741], Loss: 0.5161\n",
      "Epoch [6/10], Batch [150/741], Loss: 0.4364\n",
      "Epoch [6/10], Batch [160/741], Loss: 0.4334\n",
      "Epoch [6/10], Batch [170/741], Loss: 0.3791\n",
      "Epoch [6/10], Batch [180/741], Loss: 0.4172\n",
      "Epoch [6/10], Batch [190/741], Loss: 0.4519\n",
      "Epoch [6/10], Batch [200/741], Loss: 0.4907\n",
      "Epoch [6/10], Batch [210/741], Loss: 0.4405\n",
      "Epoch [6/10], Batch [220/741], Loss: 0.3772\n",
      "Epoch [6/10], Batch [230/741], Loss: 0.3082\n",
      "Epoch [6/10], Batch [240/741], Loss: 0.3756\n",
      "Epoch [6/10], Batch [250/741], Loss: 0.3945\n",
      "Epoch [6/10], Batch [260/741], Loss: 0.3357\n",
      "Epoch [6/10], Batch [270/741], Loss: 0.4101\n",
      "Epoch [6/10], Batch [280/741], Loss: 0.4514\n",
      "Epoch [6/10], Batch [290/741], Loss: 0.3380\n",
      "Epoch [6/10], Batch [300/741], Loss: 0.4308\n",
      "Epoch [6/10], Batch [310/741], Loss: 0.4300\n",
      "Epoch [6/10], Batch [320/741], Loss: 0.4508\n",
      "Epoch [6/10], Batch [330/741], Loss: 0.3340\n",
      "Epoch [6/10], Batch [340/741], Loss: 0.4348\n",
      "Epoch [6/10], Batch [350/741], Loss: 0.4171\n",
      "Epoch [6/10], Batch [360/741], Loss: 0.4294\n",
      "Epoch [6/10], Batch [370/741], Loss: 0.5108\n",
      "Epoch [6/10], Batch [380/741], Loss: 0.4324\n",
      "Epoch [6/10], Batch [390/741], Loss: 0.4238\n",
      "Epoch [6/10], Batch [400/741], Loss: 0.4408\n",
      "Epoch [6/10], Batch [410/741], Loss: 0.4501\n",
      "Epoch [6/10], Batch [420/741], Loss: 0.4520\n",
      "Epoch [6/10], Batch [430/741], Loss: 0.3308\n",
      "Epoch [6/10], Batch [440/741], Loss: 0.3913\n",
      "Epoch [6/10], Batch [450/741], Loss: 0.4409\n",
      "Epoch [6/10], Batch [460/741], Loss: 0.4198\n",
      "Epoch [6/10], Batch [470/741], Loss: 0.4016\n",
      "Epoch [6/10], Batch [480/741], Loss: 0.4627\n",
      "Epoch [6/10], Batch [490/741], Loss: 0.3711\n",
      "Epoch [6/10], Batch [500/741], Loss: 0.4722\n",
      "Epoch [6/10], Batch [510/741], Loss: 0.3727\n",
      "Epoch [6/10], Batch [520/741], Loss: 0.3375\n",
      "Epoch [6/10], Batch [530/741], Loss: 0.4628\n",
      "Epoch [6/10], Batch [540/741], Loss: 0.5219\n",
      "Epoch [6/10], Batch [550/741], Loss: 0.4083\n",
      "Epoch [6/10], Batch [560/741], Loss: 0.4469\n",
      "Epoch [6/10], Batch [570/741], Loss: 0.4529\n",
      "Epoch [6/10], Batch [580/741], Loss: 0.4446\n",
      "Epoch [6/10], Batch [590/741], Loss: 0.4549\n",
      "Epoch [6/10], Batch [600/741], Loss: 0.4525\n",
      "Epoch [6/10], Batch [610/741], Loss: 0.3827\n",
      "Epoch [6/10], Batch [620/741], Loss: 0.3945\n",
      "Epoch [6/10], Batch [630/741], Loss: 0.2729\n",
      "Epoch [6/10], Batch [640/741], Loss: 0.3415\n",
      "Epoch [6/10], Batch [650/741], Loss: 0.4575\n",
      "Epoch [6/10], Batch [660/741], Loss: 0.5086\n",
      "Epoch [6/10], Batch [670/741], Loss: 0.3829\n",
      "Epoch [6/10], Batch [680/741], Loss: 0.4045\n",
      "Epoch [6/10], Batch [690/741], Loss: 0.4997\n",
      "Epoch [6/10], Batch [700/741], Loss: 0.4043\n",
      "Epoch [6/10], Batch [710/741], Loss: 0.4275\n",
      "Epoch [6/10], Batch [720/741], Loss: 0.3976\n",
      "Epoch [6/10], Batch [730/741], Loss: 0.4267\n",
      "Epoch [6/10], Batch [740/741], Loss: 0.4227\n",
      "Epoch 6 finished.\n",
      "Epoch [7/10], Batch [10/741], Loss: 0.2496\n",
      "Epoch [7/10], Batch [20/741], Loss: 0.2203\n",
      "Epoch [7/10], Batch [30/741], Loss: 0.2834\n",
      "Epoch [7/10], Batch [40/741], Loss: 0.2752\n",
      "Epoch [7/10], Batch [50/741], Loss: 0.2228\n",
      "Epoch [7/10], Batch [60/741], Loss: 0.2868\n",
      "Epoch [7/10], Batch [70/741], Loss: 0.2312\n",
      "Epoch [7/10], Batch [80/741], Loss: 0.2619\n",
      "Epoch [7/10], Batch [90/741], Loss: 0.3122\n",
      "Epoch [7/10], Batch [100/741], Loss: 0.2954\n",
      "Epoch [7/10], Batch [110/741], Loss: 0.2528\n",
      "Epoch [7/10], Batch [120/741], Loss: 0.2517\n",
      "Epoch [7/10], Batch [130/741], Loss: 0.2570\n",
      "Epoch [7/10], Batch [140/741], Loss: 0.2399\n",
      "Epoch [7/10], Batch [150/741], Loss: 0.2899\n",
      "Epoch [7/10], Batch [160/741], Loss: 0.2336\n",
      "Epoch [7/10], Batch [170/741], Loss: 0.2960\n",
      "Epoch [7/10], Batch [180/741], Loss: 0.1777\n",
      "Epoch [7/10], Batch [190/741], Loss: 0.2077\n",
      "Epoch [7/10], Batch [200/741], Loss: 0.2311\n",
      "Epoch [7/10], Batch [210/741], Loss: 0.2398\n",
      "Epoch [7/10], Batch [220/741], Loss: 0.2637\n",
      "Epoch [7/10], Batch [230/741], Loss: 0.2624\n",
      "Epoch [7/10], Batch [240/741], Loss: 0.2424\n",
      "Epoch [7/10], Batch [250/741], Loss: 0.2675\n",
      "Epoch [7/10], Batch [260/741], Loss: 0.3385\n",
      "Epoch [7/10], Batch [270/741], Loss: 0.2543\n",
      "Epoch [7/10], Batch [280/741], Loss: 0.2468\n",
      "Epoch [7/10], Batch [290/741], Loss: 0.2475\n",
      "Epoch [7/10], Batch [300/741], Loss: 0.2763\n",
      "Epoch [7/10], Batch [310/741], Loss: 0.2383\n",
      "Epoch [7/10], Batch [320/741], Loss: 0.2413\n",
      "Epoch [7/10], Batch [330/741], Loss: 0.2257\n",
      "Epoch [7/10], Batch [340/741], Loss: 0.3027\n",
      "Epoch [7/10], Batch [350/741], Loss: 0.2386\n",
      "Epoch [7/10], Batch [360/741], Loss: 0.2802\n",
      "Epoch [7/10], Batch [370/741], Loss: 0.2573\n",
      "Epoch [7/10], Batch [380/741], Loss: 0.2745\n",
      "Epoch [7/10], Batch [390/741], Loss: 0.2879\n",
      "Epoch [7/10], Batch [400/741], Loss: 0.2726\n",
      "Epoch [7/10], Batch [410/741], Loss: 0.2676\n",
      "Epoch [7/10], Batch [420/741], Loss: 0.2185\n",
      "Epoch [7/10], Batch [430/741], Loss: 0.2356\n",
      "Epoch [7/10], Batch [440/741], Loss: 0.2591\n",
      "Epoch [7/10], Batch [450/741], Loss: 0.2600\n",
      "Epoch [7/10], Batch [460/741], Loss: 0.1982\n",
      "Epoch [7/10], Batch [470/741], Loss: 0.2627\n",
      "Epoch [7/10], Batch [480/741], Loss: 0.2517\n",
      "Epoch [7/10], Batch [490/741], Loss: 0.2735\n",
      "Epoch [7/10], Batch [500/741], Loss: 0.2546\n",
      "Epoch [7/10], Batch [510/741], Loss: 0.2565\n",
      "Epoch [7/10], Batch [520/741], Loss: 0.2572\n",
      "Epoch [7/10], Batch [530/741], Loss: 0.2465\n",
      "Epoch [7/10], Batch [540/741], Loss: 0.2495\n",
      "Epoch [7/10], Batch [550/741], Loss: 0.2586\n",
      "Epoch [7/10], Batch [560/741], Loss: 0.2353\n",
      "Epoch [7/10], Batch [570/741], Loss: 0.2859\n",
      "Epoch [7/10], Batch [580/741], Loss: 0.2075\n",
      "Epoch [7/10], Batch [590/741], Loss: 0.2420\n",
      "Epoch [7/10], Batch [600/741], Loss: 0.2379\n",
      "Epoch [7/10], Batch [610/741], Loss: 0.2609\n",
      "Epoch [7/10], Batch [620/741], Loss: 0.2535\n",
      "Epoch [7/10], Batch [630/741], Loss: 0.2839\n",
      "Epoch [7/10], Batch [640/741], Loss: 0.2831\n",
      "Epoch [7/10], Batch [650/741], Loss: 0.2276\n",
      "Epoch [7/10], Batch [660/741], Loss: 0.2261\n",
      "Epoch [7/10], Batch [670/741], Loss: 0.1914\n",
      "Epoch [7/10], Batch [680/741], Loss: 0.2645\n",
      "Epoch [7/10], Batch [690/741], Loss: 0.2602\n",
      "Epoch [7/10], Batch [700/741], Loss: 0.2645\n",
      "Epoch [7/10], Batch [710/741], Loss: 0.2509\n",
      "Epoch [7/10], Batch [720/741], Loss: 0.2780\n",
      "Epoch [7/10], Batch [730/741], Loss: 0.2674\n",
      "Epoch [7/10], Batch [740/741], Loss: 0.1996\n",
      "Epoch 7 finished.\n",
      "Epoch [8/10], Batch [10/741], Loss: 0.1876\n",
      "Epoch [8/10], Batch [20/741], Loss: 0.1555\n",
      "Epoch [8/10], Batch [30/741], Loss: 0.2128\n",
      "Epoch [8/10], Batch [40/741], Loss: 0.1643\n",
      "Epoch [8/10], Batch [50/741], Loss: 0.1603\n",
      "Epoch [8/10], Batch [60/741], Loss: 0.1448\n",
      "Epoch [8/10], Batch [70/741], Loss: 0.1635\n",
      "Epoch [8/10], Batch [80/741], Loss: 0.1004\n",
      "Epoch [8/10], Batch [90/741], Loss: 0.1465\n",
      "Epoch [8/10], Batch [100/741], Loss: 0.1335\n",
      "Epoch [8/10], Batch [110/741], Loss: 0.1281\n",
      "Epoch [8/10], Batch [120/741], Loss: 0.1740\n",
      "Epoch [8/10], Batch [130/741], Loss: 0.1524\n",
      "Epoch [8/10], Batch [140/741], Loss: 0.1335\n",
      "Epoch [8/10], Batch [150/741], Loss: 0.1427\n",
      "Epoch [8/10], Batch [160/741], Loss: 0.1652\n",
      "Epoch [8/10], Batch [170/741], Loss: 0.1784\n",
      "Epoch [8/10], Batch [180/741], Loss: 0.1570\n",
      "Epoch [8/10], Batch [190/741], Loss: 0.1344\n",
      "Epoch [8/10], Batch [200/741], Loss: 0.1445\n",
      "Epoch [8/10], Batch [210/741], Loss: 0.1210\n",
      "Epoch [8/10], Batch [220/741], Loss: 0.1267\n",
      "Epoch [8/10], Batch [230/741], Loss: 0.1272\n",
      "Epoch [8/10], Batch [240/741], Loss: 0.1171\n",
      "Epoch [8/10], Batch [250/741], Loss: 0.1177\n",
      "Epoch [8/10], Batch [260/741], Loss: 0.1185\n",
      "Epoch [8/10], Batch [270/741], Loss: 0.1153\n",
      "Epoch [8/10], Batch [280/741], Loss: 0.1070\n",
      "Epoch [8/10], Batch [290/741], Loss: 0.1513\n",
      "Epoch [8/10], Batch [300/741], Loss: 0.1954\n",
      "Epoch [8/10], Batch [310/741], Loss: 0.1277\n",
      "Epoch [8/10], Batch [320/741], Loss: 0.1353\n",
      "Epoch [8/10], Batch [330/741], Loss: 0.1344\n",
      "Epoch [8/10], Batch [340/741], Loss: 0.1537\n",
      "Epoch [8/10], Batch [350/741], Loss: 0.1163\n",
      "Epoch [8/10], Batch [360/741], Loss: 0.1779\n",
      "Epoch [8/10], Batch [370/741], Loss: 0.1438\n",
      "Epoch [8/10], Batch [380/741], Loss: 0.1220\n",
      "Epoch [8/10], Batch [390/741], Loss: 0.1521\n",
      "Epoch [8/10], Batch [400/741], Loss: 0.1226\n",
      "Epoch [8/10], Batch [410/741], Loss: 0.1042\n",
      "Epoch [8/10], Batch [420/741], Loss: 0.1474\n",
      "Epoch [8/10], Batch [430/741], Loss: 0.1464\n",
      "Epoch [8/10], Batch [440/741], Loss: 0.1267\n",
      "Epoch [8/10], Batch [450/741], Loss: 0.1457\n",
      "Epoch [8/10], Batch [460/741], Loss: 0.1358\n",
      "Epoch [8/10], Batch [470/741], Loss: 0.1069\n",
      "Epoch [8/10], Batch [480/741], Loss: 0.1447\n",
      "Epoch [8/10], Batch [490/741], Loss: 0.1807\n",
      "Epoch [8/10], Batch [500/741], Loss: 0.1351\n",
      "Epoch [8/10], Batch [510/741], Loss: 0.1222\n",
      "Epoch [8/10], Batch [520/741], Loss: 0.1106\n",
      "Epoch [8/10], Batch [530/741], Loss: 0.1379\n",
      "Epoch [8/10], Batch [540/741], Loss: 0.0976\n",
      "Epoch [8/10], Batch [550/741], Loss: 0.1097\n",
      "Epoch [8/10], Batch [560/741], Loss: 0.1718\n",
      "Epoch [8/10], Batch [570/741], Loss: 0.1411\n",
      "Epoch [8/10], Batch [580/741], Loss: 0.1211\n",
      "Epoch [8/10], Batch [590/741], Loss: 0.1763\n",
      "Epoch [8/10], Batch [600/741], Loss: 0.1094\n",
      "Epoch [8/10], Batch [610/741], Loss: 0.1148\n",
      "Epoch [8/10], Batch [620/741], Loss: 0.1377\n",
      "Epoch [8/10], Batch [630/741], Loss: 0.1122\n",
      "Epoch [8/10], Batch [640/741], Loss: 0.1545\n",
      "Epoch [8/10], Batch [650/741], Loss: 0.1219\n",
      "Epoch [8/10], Batch [660/741], Loss: 0.1187\n",
      "Epoch [8/10], Batch [670/741], Loss: 0.1357\n",
      "Epoch [8/10], Batch [680/741], Loss: 0.1203\n",
      "Epoch [8/10], Batch [690/741], Loss: 0.1712\n",
      "Epoch [8/10], Batch [700/741], Loss: 0.1027\n",
      "Epoch [8/10], Batch [710/741], Loss: 0.1562\n",
      "Epoch [8/10], Batch [720/741], Loss: 0.0962\n",
      "Epoch [8/10], Batch [730/741], Loss: 0.1298\n",
      "Epoch [8/10], Batch [740/741], Loss: 0.1401\n",
      "Epoch 8 finished.\n",
      "Epoch [9/10], Batch [10/741], Loss: 0.0846\n",
      "Epoch [9/10], Batch [20/741], Loss: 0.0759\n",
      "Epoch [9/10], Batch [30/741], Loss: 0.0787\n",
      "Epoch [9/10], Batch [40/741], Loss: 0.0952\n",
      "Epoch [9/10], Batch [50/741], Loss: 0.0810\n",
      "Epoch [9/10], Batch [60/741], Loss: 0.0627\n",
      "Epoch [9/10], Batch [70/741], Loss: 0.0711\n",
      "Epoch [9/10], Batch [80/741], Loss: 0.0544\n",
      "Epoch [9/10], Batch [90/741], Loss: 0.0839\n",
      "Epoch [9/10], Batch [100/741], Loss: 0.0701\n",
      "Epoch [9/10], Batch [110/741], Loss: 0.0636\n",
      "Epoch [9/10], Batch [120/741], Loss: 0.0738\n",
      "Epoch [9/10], Batch [130/741], Loss: 0.0833\n",
      "Epoch [9/10], Batch [140/741], Loss: 0.0618\n",
      "Epoch [9/10], Batch [150/741], Loss: 0.0815\n",
      "Epoch [9/10], Batch [160/741], Loss: 0.0844\n",
      "Epoch [9/10], Batch [170/741], Loss: 0.0661\n",
      "Epoch [9/10], Batch [180/741], Loss: 0.0673\n",
      "Epoch [9/10], Batch [190/741], Loss: 0.0634\n",
      "Epoch [9/10], Batch [200/741], Loss: 0.0875\n",
      "Epoch [9/10], Batch [210/741], Loss: 0.0664\n",
      "Epoch [9/10], Batch [220/741], Loss: 0.0661\n",
      "Epoch [9/10], Batch [230/741], Loss: 0.0683\n",
      "Epoch [9/10], Batch [240/741], Loss: 0.0535\n",
      "Epoch [9/10], Batch [250/741], Loss: 0.0718\n",
      "Epoch [9/10], Batch [260/741], Loss: 0.0666\n",
      "Epoch [9/10], Batch [270/741], Loss: 0.0797\n",
      "Epoch [9/10], Batch [280/741], Loss: 0.0829\n",
      "Epoch [9/10], Batch [290/741], Loss: 0.0718\n",
      "Epoch [9/10], Batch [300/741], Loss: 0.0676\n",
      "Epoch [9/10], Batch [310/741], Loss: 0.0695\n",
      "Epoch [9/10], Batch [320/741], Loss: 0.0747\n",
      "Epoch [9/10], Batch [330/741], Loss: 0.0789\n",
      "Epoch [9/10], Batch [340/741], Loss: 0.0843\n",
      "Epoch [9/10], Batch [350/741], Loss: 0.0682\n",
      "Epoch [9/10], Batch [360/741], Loss: 0.0568\n",
      "Epoch [9/10], Batch [370/741], Loss: 0.0852\n",
      "Epoch [9/10], Batch [380/741], Loss: 0.0695\n",
      "Epoch [9/10], Batch [390/741], Loss: 0.0818\n",
      "Epoch [9/10], Batch [400/741], Loss: 0.0690\n",
      "Epoch [9/10], Batch [410/741], Loss: 0.0703\n",
      "Epoch [9/10], Batch [420/741], Loss: 0.0863\n",
      "Epoch [9/10], Batch [430/741], Loss: 0.0747\n",
      "Epoch [9/10], Batch [440/741], Loss: 0.0630\n",
      "Epoch [9/10], Batch [450/741], Loss: 0.0663\n",
      "Epoch [9/10], Batch [460/741], Loss: 0.0581\n",
      "Epoch [9/10], Batch [470/741], Loss: 0.0669\n",
      "Epoch [9/10], Batch [480/741], Loss: 0.0620\n",
      "Epoch [9/10], Batch [490/741], Loss: 0.0505\n",
      "Epoch [9/10], Batch [500/741], Loss: 0.0728\n",
      "Epoch [9/10], Batch [510/741], Loss: 0.0649\n",
      "Epoch [9/10], Batch [520/741], Loss: 0.0599\n",
      "Epoch [9/10], Batch [530/741], Loss: 0.0626\n",
      "Epoch [9/10], Batch [540/741], Loss: 0.0609\n",
      "Epoch [9/10], Batch [550/741], Loss: 0.0725\n",
      "Epoch [9/10], Batch [560/741], Loss: 0.0793\n",
      "Epoch [9/10], Batch [570/741], Loss: 0.0684\n",
      "Epoch [9/10], Batch [580/741], Loss: 0.0654\n",
      "Epoch [9/10], Batch [590/741], Loss: 0.0785\n",
      "Epoch [9/10], Batch [600/741], Loss: 0.0495\n",
      "Epoch [9/10], Batch [610/741], Loss: 0.0555\n",
      "Epoch [9/10], Batch [620/741], Loss: 0.0596\n",
      "Epoch [9/10], Batch [630/741], Loss: 0.0697\n",
      "Epoch [9/10], Batch [640/741], Loss: 0.0455\n",
      "Epoch [9/10], Batch [650/741], Loss: 0.0569\n",
      "Epoch [9/10], Batch [660/741], Loss: 0.0493\n",
      "Epoch [9/10], Batch [670/741], Loss: 0.1007\n",
      "Epoch [9/10], Batch [680/741], Loss: 0.0562\n",
      "Epoch [9/10], Batch [690/741], Loss: 0.0621\n",
      "Epoch [9/10], Batch [700/741], Loss: 0.0617\n",
      "Epoch [9/10], Batch [710/741], Loss: 0.0767\n",
      "Epoch [9/10], Batch [720/741], Loss: 0.0704\n",
      "Epoch [9/10], Batch [730/741], Loss: 0.0460\n",
      "Epoch [9/10], Batch [740/741], Loss: 0.0612\n",
      "Epoch 9 finished.\n",
      "Epoch [10/10], Batch [10/741], Loss: 0.0398\n",
      "Epoch [10/10], Batch [20/741], Loss: 0.0298\n",
      "Epoch [10/10], Batch [30/741], Loss: 0.0320\n",
      "Epoch [10/10], Batch [40/741], Loss: 0.0468\n",
      "Epoch [10/10], Batch [50/741], Loss: 0.0429\n",
      "Epoch [10/10], Batch [60/741], Loss: 0.0401\n",
      "Epoch [10/10], Batch [70/741], Loss: 0.0566\n",
      "Epoch [10/10], Batch [80/741], Loss: 0.0347\n",
      "Epoch [10/10], Batch [90/741], Loss: 0.0327\n",
      "Epoch [10/10], Batch [100/741], Loss: 0.0431\n",
      "Epoch [10/10], Batch [110/741], Loss: 0.0307\n",
      "Epoch [10/10], Batch [120/741], Loss: 0.0418\n",
      "Epoch [10/10], Batch [130/741], Loss: 0.0421\n",
      "Epoch [10/10], Batch [140/741], Loss: 0.0348\n",
      "Epoch [10/10], Batch [150/741], Loss: 0.0307\n",
      "Epoch [10/10], Batch [160/741], Loss: 0.0363\n",
      "Epoch [10/10], Batch [170/741], Loss: 0.0320\n",
      "Epoch [10/10], Batch [180/741], Loss: 0.0348\n",
      "Epoch [10/10], Batch [190/741], Loss: 0.0334\n",
      "Epoch [10/10], Batch [200/741], Loss: 0.0403\n",
      "Epoch [10/10], Batch [210/741], Loss: 0.0333\n",
      "Epoch [10/10], Batch [220/741], Loss: 0.0276\n",
      "Epoch [10/10], Batch [230/741], Loss: 0.0252\n",
      "Epoch [10/10], Batch [240/741], Loss: 0.0373\n",
      "Epoch [10/10], Batch [250/741], Loss: 0.0440\n",
      "Epoch [10/10], Batch [260/741], Loss: 0.0374\n",
      "Epoch [10/10], Batch [270/741], Loss: 0.0282\n",
      "Epoch [10/10], Batch [280/741], Loss: 0.0311\n",
      "Epoch [10/10], Batch [290/741], Loss: 0.0301\n",
      "Epoch [10/10], Batch [300/741], Loss: 0.0465\n",
      "Epoch [10/10], Batch [310/741], Loss: 0.0467\n",
      "Epoch [10/10], Batch [320/741], Loss: 0.0292\n",
      "Epoch [10/10], Batch [330/741], Loss: 0.0294\n",
      "Epoch [10/10], Batch [340/741], Loss: 0.0478\n",
      "Epoch [10/10], Batch [350/741], Loss: 0.0529\n",
      "Epoch [10/10], Batch [360/741], Loss: 0.0412\n",
      "Epoch [10/10], Batch [370/741], Loss: 0.0367\n",
      "Epoch [10/10], Batch [380/741], Loss: 0.0453\n",
      "Epoch [10/10], Batch [390/741], Loss: 0.0581\n",
      "Epoch [10/10], Batch [400/741], Loss: 0.0513\n",
      "Epoch [10/10], Batch [410/741], Loss: 0.0325\n",
      "Epoch [10/10], Batch [420/741], Loss: 0.0438\n",
      "Epoch [10/10], Batch [430/741], Loss: 0.0334\n",
      "Epoch [10/10], Batch [440/741], Loss: 0.0488\n",
      "Epoch [10/10], Batch [450/741], Loss: 0.0357\n",
      "Epoch [10/10], Batch [460/741], Loss: 0.0324\n",
      "Epoch [10/10], Batch [470/741], Loss: 0.0253\n",
      "Epoch [10/10], Batch [480/741], Loss: 0.0328\n",
      "Epoch [10/10], Batch [490/741], Loss: 0.0317\n",
      "Epoch [10/10], Batch [500/741], Loss: 0.0326\n",
      "Epoch [10/10], Batch [510/741], Loss: 0.0340\n",
      "Epoch [10/10], Batch [520/741], Loss: 0.0430\n",
      "Epoch [10/10], Batch [530/741], Loss: 0.0413\n",
      "Epoch [10/10], Batch [540/741], Loss: 0.0381\n",
      "Epoch [10/10], Batch [550/741], Loss: 0.0340\n",
      "Epoch [10/10], Batch [560/741], Loss: 0.0239\n",
      "Epoch [10/10], Batch [570/741], Loss: 0.0322\n",
      "Epoch [10/10], Batch [580/741], Loss: 0.0311\n",
      "Epoch [10/10], Batch [590/741], Loss: 0.0364\n",
      "Epoch [10/10], Batch [600/741], Loss: 0.0388\n",
      "Epoch [10/10], Batch [610/741], Loss: 0.0281\n",
      "Epoch [10/10], Batch [620/741], Loss: 0.0290\n",
      "Epoch [10/10], Batch [630/741], Loss: 0.0339\n",
      "Epoch [10/10], Batch [640/741], Loss: 0.0320\n",
      "Epoch [10/10], Batch [650/741], Loss: 0.0354\n",
      "Epoch [10/10], Batch [660/741], Loss: 0.0467\n",
      "Epoch [10/10], Batch [670/741], Loss: 0.0283\n",
      "Epoch [10/10], Batch [680/741], Loss: 0.0220\n",
      "Epoch [10/10], Batch [690/741], Loss: 0.0277\n",
      "Epoch [10/10], Batch [700/741], Loss: 0.0322\n",
      "Epoch [10/10], Batch [710/741], Loss: 0.0289\n",
      "Epoch [10/10], Batch [720/741], Loss: 0.0290\n",
      "Epoch [10/10], Batch [730/741], Loss: 0.0453\n",
      "Epoch [10/10], Batch [740/741], Loss: 0.0277\n",
      "Epoch 10 finished.\n",
      "Progress: 400/2975 images evaluated.\n",
      "Progress: 800/2975 images evaluated.\n",
      "Progress: 1200/2975 images evaluated.\n",
      "Progress: 1600/2975 images evaluated.\n",
      "Progress: 2000/2975 images evaluated.\n",
      "Progress: 2400/2975 images evaluated.\n",
      "Progress: 2800/2975 images evaluated.\n",
      "Accuracy: 62.29%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load Data from JSON\n",
    "with open('C:\\\\Users\\\\hp\\\\Downloads\\\\DataVIT.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Flatten the image data\n",
    "image_data = []\n",
    "for item in data:\n",
    "    for img in item['images']:\n",
    "        image_data.append({'url': img['url'], 'label': item['interests'][0]})\n",
    "\n",
    "# Collect paths to all .pt files\n",
    "preprocessed_files = sorted([f'C:\\\\Users\\\\hp\\\\Downloads\\\\preprocessed_images_combined\\\\{filename}' \n",
    "                             for filename in os.listdir('C:\\\\Users\\\\hp\\\\Downloads\\\\preprocessed_images_combined') \n",
    "                             if filename.endswith('.pt')])\n",
    "\n",
    "# Create a mapping from filename to preprocessed file path\n",
    "preprocessed_file_map = {os.path.basename(file): file for file in preprocessed_files}\n",
    "\n",
    "# Step 2: Split Data into Training and Test Sets\n",
    "train_data, test_data = train_test_split(image_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Filter the train data to include only entries that have a corresponding preprocessed file\n",
    "filtered_train_data = [entry for entry in train_data if os.path.basename(entry['url']).replace('.jpg', '.pt') in preprocessed_file_map]\n",
    "\n",
    "# Filter the test data to include only entries that have a corresponding preprocessed file\n",
    "filtered_test_data = [entry for entry in test_data if os.path.basename(entry['url']).replace('.jpg', '.pt') in preprocessed_file_map]\n",
    "\n",
    "# Define transformations (without data augmentation)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(filtered_train_data, preprocessed_train_files, transform=transform)\n",
    "test_dataset = CustomDataset(filtered_test_data, preprocessed_test_files, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Step 4: Create the ViT Model\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=len(set([item['label'] for item in filtered_train_data])),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Step 5: Train the Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # Print every 10 batches\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {running_loss / 10:.4f}')\n",
    "            running_loss = 0.0\n",
    "    print(f'Epoch {epoch+1} finished.')\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total % 100 == 0:  # Print every 100 images\n",
    "            print(f'Progress: {total}/{len(test_loader.dataset)} images evaluated.')\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2339d07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filtered train data entries: 11856\n",
      "Number of filtered test data entries: 2975\n",
      "Number of preprocessed files: 14640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([13, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [10/741], Loss: 2.5790\n",
      "Epoch [1/10], Batch [20/741], Loss: 2.2073\n",
      "Epoch [1/10], Batch [30/741], Loss: 2.1986\n",
      "Epoch [1/10], Batch [40/741], Loss: 2.1167\n",
      "Epoch [1/10], Batch [50/741], Loss: 2.0829\n",
      "Epoch [1/10], Batch [60/741], Loss: 2.1577\n",
      "Epoch [1/10], Batch [70/741], Loss: 2.0347\n",
      "Epoch [1/10], Batch [80/741], Loss: 1.9986\n",
      "Epoch [1/10], Batch [90/741], Loss: 2.0597\n",
      "Epoch [1/10], Batch [100/741], Loss: 1.8853\n",
      "Epoch [1/10], Batch [110/741], Loss: 1.8176\n",
      "Epoch [1/10], Batch [120/741], Loss: 2.0313\n",
      "Epoch [1/10], Batch [130/741], Loss: 1.9176\n",
      "Epoch [1/10], Batch [140/741], Loss: 2.0088\n",
      "Epoch [1/10], Batch [150/741], Loss: 1.8199\n",
      "Epoch [1/10], Batch [160/741], Loss: 1.7769\n",
      "Epoch [1/10], Batch [170/741], Loss: 1.7643\n",
      "Epoch [1/10], Batch [180/741], Loss: 2.0189\n",
      "Epoch [1/10], Batch [190/741], Loss: 1.7140\n",
      "Epoch [1/10], Batch [200/741], Loss: 1.7912\n",
      "Epoch [1/10], Batch [210/741], Loss: 1.8924\n",
      "Epoch [1/10], Batch [220/741], Loss: 1.9192\n",
      "Epoch [1/10], Batch [230/741], Loss: 1.7919\n",
      "Epoch [1/10], Batch [240/741], Loss: 1.8605\n",
      "Epoch [1/10], Batch [250/741], Loss: 1.7842\n",
      "Epoch [1/10], Batch [260/741], Loss: 1.8487\n",
      "Epoch [1/10], Batch [270/741], Loss: 1.8270\n",
      "Epoch [1/10], Batch [280/741], Loss: 1.6069\n",
      "Epoch [1/10], Batch [290/741], Loss: 1.8185\n",
      "Epoch [1/10], Batch [300/741], Loss: 1.6269\n",
      "Epoch [1/10], Batch [310/741], Loss: 1.6496\n",
      "Epoch [1/10], Batch [320/741], Loss: 1.5803\n",
      "Epoch [1/10], Batch [330/741], Loss: 1.7073\n",
      "Epoch [1/10], Batch [340/741], Loss: 1.6903\n",
      "Epoch [1/10], Batch [350/741], Loss: 1.6455\n",
      "Epoch [1/10], Batch [360/741], Loss: 1.5906\n",
      "Epoch [1/10], Batch [370/741], Loss: 1.5997\n",
      "Epoch [1/10], Batch [380/741], Loss: 1.4455\n",
      "Epoch [1/10], Batch [390/741], Loss: 1.5451\n",
      "Epoch [1/10], Batch [400/741], Loss: 1.6222\n",
      "Epoch [1/10], Batch [410/741], Loss: 1.8206\n",
      "Epoch [1/10], Batch [420/741], Loss: 1.7853\n",
      "Epoch [1/10], Batch [430/741], Loss: 1.5462\n",
      "Epoch [1/10], Batch [440/741], Loss: 1.5501\n",
      "Epoch [1/10], Batch [450/741], Loss: 1.5496\n",
      "Epoch [1/10], Batch [460/741], Loss: 1.4722\n",
      "Epoch [1/10], Batch [470/741], Loss: 1.7583\n",
      "Epoch [1/10], Batch [480/741], Loss: 1.5992\n",
      "Epoch [1/10], Batch [490/741], Loss: 1.4894\n",
      "Epoch [1/10], Batch [500/741], Loss: 1.5990\n",
      "Epoch [1/10], Batch [510/741], Loss: 1.3902\n",
      "Epoch [1/10], Batch [520/741], Loss: 1.6161\n",
      "Epoch [1/10], Batch [530/741], Loss: 1.5565\n",
      "Epoch [1/10], Batch [540/741], Loss: 1.4621\n",
      "Epoch [1/10], Batch [550/741], Loss: 1.4428\n",
      "Epoch [1/10], Batch [560/741], Loss: 1.5940\n",
      "Epoch [1/10], Batch [570/741], Loss: 1.4422\n",
      "Epoch [1/10], Batch [580/741], Loss: 1.7450\n",
      "Epoch [1/10], Batch [590/741], Loss: 1.4444\n",
      "Epoch [1/10], Batch [600/741], Loss: 1.6766\n",
      "Epoch [1/10], Batch [610/741], Loss: 1.3845\n",
      "Epoch [1/10], Batch [620/741], Loss: 1.4120\n",
      "Epoch [1/10], Batch [630/741], Loss: 1.4543\n",
      "Epoch [1/10], Batch [640/741], Loss: 1.4673\n",
      "Epoch [1/10], Batch [650/741], Loss: 1.5347\n",
      "Epoch [1/10], Batch [660/741], Loss: 1.3440\n",
      "Epoch [1/10], Batch [670/741], Loss: 1.5096\n",
      "Epoch [1/10], Batch [680/741], Loss: 1.4147\n",
      "Epoch [1/10], Batch [690/741], Loss: 1.4868\n",
      "Epoch [1/10], Batch [700/741], Loss: 1.3764\n",
      "Epoch [1/10], Batch [710/741], Loss: 1.5661\n",
      "Epoch [1/10], Batch [720/741], Loss: 1.3831\n",
      "Epoch [1/10], Batch [730/741], Loss: 1.3723\n",
      "Epoch [1/10], Batch [740/741], Loss: 1.4098\n",
      "Epoch 1 finished.\n",
      "Epoch [2/10], Batch [10/741], Loss: 1.1223\n",
      "Epoch [2/10], Batch [20/741], Loss: 1.2926\n",
      "Epoch [2/10], Batch [30/741], Loss: 1.1155\n",
      "Epoch [2/10], Batch [40/741], Loss: 1.0329\n",
      "Epoch [2/10], Batch [50/741], Loss: 1.1443\n",
      "Epoch [2/10], Batch [60/741], Loss: 1.2561\n",
      "Epoch [2/10], Batch [70/741], Loss: 1.1268\n",
      "Epoch [2/10], Batch [80/741], Loss: 1.1347\n",
      "Epoch [2/10], Batch [90/741], Loss: 1.1972\n",
      "Epoch [2/10], Batch [100/741], Loss: 1.3052\n",
      "Epoch [2/10], Batch [110/741], Loss: 1.0317\n",
      "Epoch [2/10], Batch [120/741], Loss: 1.2739\n",
      "Epoch [2/10], Batch [130/741], Loss: 1.1917\n",
      "Epoch [2/10], Batch [140/741], Loss: 1.2519\n",
      "Epoch [2/10], Batch [150/741], Loss: 1.1671\n",
      "Epoch [2/10], Batch [160/741], Loss: 1.3049\n",
      "Epoch [2/10], Batch [170/741], Loss: 0.9947\n",
      "Epoch [2/10], Batch [180/741], Loss: 1.0705\n",
      "Epoch [2/10], Batch [190/741], Loss: 1.1983\n",
      "Epoch [2/10], Batch [200/741], Loss: 1.2117\n",
      "Epoch [2/10], Batch [210/741], Loss: 1.1034\n",
      "Epoch [2/10], Batch [220/741], Loss: 1.1165\n",
      "Epoch [2/10], Batch [230/741], Loss: 1.1985\n",
      "Epoch [2/10], Batch [240/741], Loss: 1.1586\n",
      "Epoch [2/10], Batch [250/741], Loss: 1.3539\n",
      "Epoch [2/10], Batch [260/741], Loss: 1.0401\n",
      "Epoch [2/10], Batch [270/741], Loss: 1.1016\n",
      "Epoch [2/10], Batch [280/741], Loss: 1.2365\n",
      "Epoch [2/10], Batch [290/741], Loss: 1.2106\n",
      "Epoch [2/10], Batch [300/741], Loss: 1.1830\n",
      "Epoch [2/10], Batch [310/741], Loss: 1.1798\n",
      "Epoch [2/10], Batch [320/741], Loss: 1.2335\n",
      "Epoch [2/10], Batch [330/741], Loss: 1.2187\n",
      "Epoch [2/10], Batch [340/741], Loss: 1.2248\n",
      "Epoch [2/10], Batch [350/741], Loss: 1.0797\n",
      "Epoch [2/10], Batch [360/741], Loss: 1.0665\n",
      "Epoch [2/10], Batch [370/741], Loss: 1.0286\n",
      "Epoch [2/10], Batch [380/741], Loss: 0.9969\n",
      "Epoch [2/10], Batch [390/741], Loss: 1.1161\n",
      "Epoch [2/10], Batch [400/741], Loss: 1.0158\n",
      "Epoch [2/10], Batch [410/741], Loss: 1.2269\n",
      "Epoch [2/10], Batch [420/741], Loss: 1.0750\n",
      "Epoch [2/10], Batch [430/741], Loss: 1.0959\n",
      "Epoch [2/10], Batch [440/741], Loss: 1.2156\n",
      "Epoch [2/10], Batch [450/741], Loss: 1.1084\n",
      "Epoch [2/10], Batch [460/741], Loss: 1.0144\n",
      "Epoch [2/10], Batch [470/741], Loss: 1.1624\n",
      "Epoch [2/10], Batch [480/741], Loss: 1.0881\n",
      "Epoch [2/10], Batch [490/741], Loss: 1.0779\n",
      "Epoch [2/10], Batch [500/741], Loss: 1.1344\n",
      "Epoch [2/10], Batch [510/741], Loss: 1.0756\n",
      "Epoch [2/10], Batch [520/741], Loss: 1.2025\n",
      "Epoch [2/10], Batch [530/741], Loss: 1.1819\n",
      "Epoch [2/10], Batch [540/741], Loss: 1.0012\n",
      "Epoch [2/10], Batch [550/741], Loss: 0.9802\n",
      "Epoch [2/10], Batch [560/741], Loss: 1.1685\n",
      "Epoch [2/10], Batch [570/741], Loss: 1.1365\n",
      "Epoch [2/10], Batch [580/741], Loss: 1.0537\n",
      "Epoch [2/10], Batch [590/741], Loss: 1.2006\n",
      "Epoch [2/10], Batch [600/741], Loss: 1.1962\n",
      "Epoch [2/10], Batch [610/741], Loss: 1.2090\n",
      "Epoch [2/10], Batch [620/741], Loss: 1.2062\n",
      "Epoch [2/10], Batch [630/741], Loss: 1.1058\n",
      "Epoch [2/10], Batch [640/741], Loss: 1.1465\n",
      "Epoch [2/10], Batch [650/741], Loss: 1.0277\n",
      "Epoch [2/10], Batch [660/741], Loss: 0.8603\n",
      "Epoch [2/10], Batch [670/741], Loss: 1.1335\n",
      "Epoch [2/10], Batch [680/741], Loss: 1.0684\n",
      "Epoch [2/10], Batch [690/741], Loss: 1.0249\n",
      "Epoch [2/10], Batch [700/741], Loss: 1.0820\n",
      "Epoch [2/10], Batch [710/741], Loss: 1.1414\n",
      "Epoch [2/10], Batch [720/741], Loss: 0.9589\n",
      "Epoch [2/10], Batch [730/741], Loss: 1.0720\n",
      "Epoch [2/10], Batch [740/741], Loss: 1.1708\n",
      "Epoch 2 finished.\n",
      "Epoch [3/10], Batch [10/741], Loss: 0.7027\n",
      "Epoch [3/10], Batch [20/741], Loss: 0.7313\n",
      "Epoch [3/10], Batch [30/741], Loss: 0.6171\n",
      "Epoch [3/10], Batch [40/741], Loss: 0.6464\n",
      "Epoch [3/10], Batch [50/741], Loss: 0.6509\n",
      "Epoch [3/10], Batch [60/741], Loss: 0.7166\n",
      "Epoch [3/10], Batch [70/741], Loss: 0.6829\n",
      "Epoch [3/10], Batch [80/741], Loss: 0.6532\n",
      "Epoch [3/10], Batch [90/741], Loss: 0.6072\n",
      "Epoch [3/10], Batch [100/741], Loss: 0.6665\n",
      "Epoch [3/10], Batch [110/741], Loss: 0.8180\n",
      "Epoch [3/10], Batch [120/741], Loss: 0.6992\n",
      "Epoch [3/10], Batch [130/741], Loss: 0.7578\n",
      "Epoch [3/10], Batch [140/741], Loss: 0.7487\n",
      "Epoch [3/10], Batch [150/741], Loss: 0.6060\n",
      "Epoch [3/10], Batch [160/741], Loss: 0.7442\n",
      "Epoch [3/10], Batch [170/741], Loss: 0.7570\n",
      "Epoch [3/10], Batch [180/741], Loss: 0.6798\n",
      "Epoch [3/10], Batch [190/741], Loss: 0.5685\n",
      "Epoch [3/10], Batch [200/741], Loss: 0.7516\n",
      "Epoch [3/10], Batch [210/741], Loss: 0.6624\n",
      "Epoch [3/10], Batch [220/741], Loss: 0.7021\n",
      "Epoch [3/10], Batch [230/741], Loss: 0.6953\n",
      "Epoch [3/10], Batch [240/741], Loss: 0.7132\n",
      "Epoch [3/10], Batch [250/741], Loss: 0.6712\n",
      "Epoch [3/10], Batch [260/741], Loss: 0.6142\n",
      "Epoch [3/10], Batch [270/741], Loss: 0.6765\n",
      "Epoch [3/10], Batch [280/741], Loss: 0.7340\n",
      "Epoch [3/10], Batch [290/741], Loss: 0.6583\n",
      "Epoch [3/10], Batch [300/741], Loss: 0.6948\n",
      "Epoch [3/10], Batch [310/741], Loss: 0.6626\n",
      "Epoch [3/10], Batch [320/741], Loss: 0.6834\n",
      "Epoch [3/10], Batch [330/741], Loss: 0.6704\n",
      "Epoch [3/10], Batch [340/741], Loss: 0.5994\n",
      "Epoch [3/10], Batch [350/741], Loss: 0.7267\n",
      "Epoch [3/10], Batch [360/741], Loss: 0.8066\n",
      "Epoch [3/10], Batch [370/741], Loss: 0.6250\n",
      "Epoch [3/10], Batch [380/741], Loss: 0.7459\n",
      "Epoch [3/10], Batch [390/741], Loss: 0.7108\n",
      "Epoch [3/10], Batch [400/741], Loss: 0.7437\n",
      "Epoch [3/10], Batch [410/741], Loss: 0.8163\n",
      "Epoch [3/10], Batch [420/741], Loss: 0.6776\n",
      "Epoch [3/10], Batch [430/741], Loss: 0.6840\n",
      "Epoch [3/10], Batch [440/741], Loss: 0.5941\n",
      "Epoch [3/10], Batch [450/741], Loss: 0.6608\n",
      "Epoch [3/10], Batch [460/741], Loss: 0.6680\n",
      "Epoch [3/10], Batch [470/741], Loss: 0.6106\n",
      "Epoch [3/10], Batch [480/741], Loss: 0.6590\n",
      "Epoch [3/10], Batch [490/741], Loss: 0.6997\n",
      "Epoch [3/10], Batch [500/741], Loss: 0.7068\n",
      "Epoch [3/10], Batch [510/741], Loss: 0.7925\n",
      "Epoch [3/10], Batch [520/741], Loss: 0.6129\n",
      "Epoch [3/10], Batch [530/741], Loss: 0.5534\n",
      "Epoch [3/10], Batch [540/741], Loss: 0.6694\n",
      "Epoch [3/10], Batch [550/741], Loss: 0.5871\n",
      "Epoch [3/10], Batch [560/741], Loss: 0.4886\n",
      "Epoch [3/10], Batch [570/741], Loss: 0.7134\n",
      "Epoch [3/10], Batch [580/741], Loss: 0.5201\n",
      "Epoch [3/10], Batch [590/741], Loss: 0.5505\n",
      "Epoch [3/10], Batch [600/741], Loss: 0.6051\n",
      "Epoch [3/10], Batch [610/741], Loss: 0.7708\n",
      "Epoch [3/10], Batch [620/741], Loss: 0.6873\n",
      "Epoch [3/10], Batch [630/741], Loss: 0.6610\n",
      "Epoch [3/10], Batch [640/741], Loss: 0.6675\n",
      "Epoch [3/10], Batch [650/741], Loss: 0.5484\n",
      "Epoch [3/10], Batch [660/741], Loss: 0.6539\n",
      "Epoch [3/10], Batch [670/741], Loss: 0.6108\n",
      "Epoch [3/10], Batch [680/741], Loss: 0.7679\n",
      "Epoch [3/10], Batch [690/741], Loss: 0.7912\n",
      "Epoch [3/10], Batch [700/741], Loss: 0.6679\n",
      "Epoch [3/10], Batch [710/741], Loss: 0.5531\n",
      "Epoch [3/10], Batch [720/741], Loss: 0.7061\n",
      "Epoch [3/10], Batch [730/741], Loss: 0.5107\n",
      "Epoch [3/10], Batch [740/741], Loss: 0.6246\n",
      "Epoch 3 finished.\n",
      "Epoch [4/10], Batch [10/741], Loss: 0.3583\n",
      "Epoch [4/10], Batch [20/741], Loss: 0.3413\n",
      "Epoch [4/10], Batch [30/741], Loss: 0.2887\n",
      "Epoch [4/10], Batch [40/741], Loss: 0.2872\n",
      "Epoch [4/10], Batch [50/741], Loss: 0.3026\n",
      "Epoch [4/10], Batch [60/741], Loss: 0.2665\n",
      "Epoch [4/10], Batch [70/741], Loss: 0.2597\n",
      "Epoch [4/10], Batch [80/741], Loss: 0.2747\n",
      "Epoch [4/10], Batch [90/741], Loss: 0.2785\n",
      "Epoch [4/10], Batch [100/741], Loss: 0.2392\n",
      "Epoch [4/10], Batch [110/741], Loss: 0.2539\n",
      "Epoch [4/10], Batch [120/741], Loss: 0.3498\n",
      "Epoch [4/10], Batch [130/741], Loss: 0.3105\n",
      "Epoch [4/10], Batch [140/741], Loss: 0.1760\n",
      "Epoch [4/10], Batch [150/741], Loss: 0.2649\n",
      "Epoch [4/10], Batch [160/741], Loss: 0.3273\n",
      "Epoch [4/10], Batch [170/741], Loss: 0.2994\n",
      "Epoch [4/10], Batch [180/741], Loss: 0.2828\n",
      "Epoch [4/10], Batch [190/741], Loss: 0.3290\n",
      "Epoch [4/10], Batch [200/741], Loss: 0.2668\n",
      "Epoch [4/10], Batch [210/741], Loss: 0.2353\n",
      "Epoch [4/10], Batch [220/741], Loss: 0.2604\n",
      "Epoch [4/10], Batch [230/741], Loss: 0.2939\n",
      "Epoch [4/10], Batch [240/741], Loss: 0.2698\n",
      "Epoch [4/10], Batch [250/741], Loss: 0.2682\n",
      "Epoch [4/10], Batch [260/741], Loss: 0.2430\n",
      "Epoch [4/10], Batch [270/741], Loss: 0.2195\n",
      "Epoch [4/10], Batch [280/741], Loss: 0.2516\n",
      "Epoch [4/10], Batch [290/741], Loss: 0.3181\n",
      "Epoch [4/10], Batch [300/741], Loss: 0.2613\n",
      "Epoch [4/10], Batch [310/741], Loss: 0.2726\n",
      "Epoch [4/10], Batch [320/741], Loss: 0.2996\n",
      "Epoch [4/10], Batch [330/741], Loss: 0.2616\n",
      "Epoch [4/10], Batch [340/741], Loss: 0.2246\n",
      "Epoch [4/10], Batch [350/741], Loss: 0.2312\n",
      "Epoch [4/10], Batch [360/741], Loss: 0.3091\n",
      "Epoch [4/10], Batch [370/741], Loss: 0.2980\n",
      "Epoch [4/10], Batch [380/741], Loss: 0.2885\n",
      "Epoch [4/10], Batch [390/741], Loss: 0.3345\n",
      "Epoch [4/10], Batch [400/741], Loss: 0.2732\n",
      "Epoch [4/10], Batch [410/741], Loss: 0.2713\n",
      "Epoch [4/10], Batch [420/741], Loss: 0.2940\n",
      "Epoch [4/10], Batch [430/741], Loss: 0.2608\n",
      "Epoch [4/10], Batch [440/741], Loss: 0.2819\n",
      "Epoch [4/10], Batch [450/741], Loss: 0.2850\n",
      "Epoch [4/10], Batch [460/741], Loss: 0.3407\n",
      "Epoch [4/10], Batch [470/741], Loss: 0.3062\n",
      "Epoch [4/10], Batch [480/741], Loss: 0.3404\n",
      "Epoch [4/10], Batch [490/741], Loss: 0.2787\n",
      "Epoch [4/10], Batch [500/741], Loss: 0.3939\n",
      "Epoch [4/10], Batch [510/741], Loss: 0.2582\n",
      "Epoch [4/10], Batch [520/741], Loss: 0.3390\n",
      "Epoch [4/10], Batch [530/741], Loss: 0.2910\n",
      "Epoch [4/10], Batch [540/741], Loss: 0.3291\n",
      "Epoch [4/10], Batch [550/741], Loss: 0.3093\n",
      "Epoch [4/10], Batch [560/741], Loss: 0.2393\n",
      "Epoch [4/10], Batch [570/741], Loss: 0.2605\n",
      "Epoch [4/10], Batch [580/741], Loss: 0.2995\n",
      "Epoch [4/10], Batch [590/741], Loss: 0.2367\n",
      "Epoch [4/10], Batch [600/741], Loss: 0.1926\n",
      "Epoch [4/10], Batch [610/741], Loss: 0.2218\n",
      "Epoch [4/10], Batch [620/741], Loss: 0.2697\n",
      "Epoch [4/10], Batch [630/741], Loss: 0.1982\n",
      "Epoch [4/10], Batch [640/741], Loss: 0.1812\n",
      "Epoch [4/10], Batch [650/741], Loss: 0.2237\n",
      "Epoch [4/10], Batch [660/741], Loss: 0.3149\n",
      "Epoch [4/10], Batch [670/741], Loss: 0.2185\n",
      "Epoch [4/10], Batch [680/741], Loss: 0.2193\n",
      "Epoch [4/10], Batch [690/741], Loss: 0.2754\n",
      "Epoch [4/10], Batch [700/741], Loss: 0.3343\n",
      "Epoch [4/10], Batch [710/741], Loss: 0.2498\n",
      "Epoch [4/10], Batch [720/741], Loss: 0.3119\n",
      "Epoch [4/10], Batch [730/741], Loss: 0.2375\n",
      "Epoch [4/10], Batch [740/741], Loss: 0.2889\n",
      "Epoch 4 finished.\n",
      "Epoch [5/10], Batch [10/741], Loss: 0.1035\n",
      "Epoch [5/10], Batch [20/741], Loss: 0.0888\n",
      "Epoch [5/10], Batch [30/741], Loss: 0.0852\n",
      "Epoch [5/10], Batch [40/741], Loss: 0.0946\n",
      "Epoch [5/10], Batch [50/741], Loss: 0.0912\n",
      "Epoch [5/10], Batch [60/741], Loss: 0.0664\n",
      "Epoch [5/10], Batch [70/741], Loss: 0.0788\n",
      "Epoch [5/10], Batch [80/741], Loss: 0.0789\n",
      "Epoch [5/10], Batch [90/741], Loss: 0.0705\n",
      "Epoch [5/10], Batch [100/741], Loss: 0.0664\n",
      "Epoch [5/10], Batch [110/741], Loss: 0.0695\n",
      "Epoch [5/10], Batch [120/741], Loss: 0.0694\n",
      "Epoch [5/10], Batch [130/741], Loss: 0.0585\n",
      "Epoch [5/10], Batch [140/741], Loss: 0.0433\n",
      "Epoch [5/10], Batch [150/741], Loss: 0.0870\n",
      "Epoch [5/10], Batch [160/741], Loss: 0.0566\n",
      "Epoch [5/10], Batch [170/741], Loss: 0.0681\n",
      "Epoch [5/10], Batch [180/741], Loss: 0.0914\n",
      "Epoch [5/10], Batch [190/741], Loss: 0.0891\n",
      "Epoch [5/10], Batch [200/741], Loss: 0.0619\n",
      "Epoch [5/10], Batch [210/741], Loss: 0.0820\n",
      "Epoch [5/10], Batch [220/741], Loss: 0.0676\n",
      "Epoch [5/10], Batch [230/741], Loss: 0.0663\n",
      "Epoch [5/10], Batch [240/741], Loss: 0.0907\n",
      "Epoch [5/10], Batch [250/741], Loss: 0.0583\n",
      "Epoch [5/10], Batch [260/741], Loss: 0.0581\n",
      "Epoch [5/10], Batch [270/741], Loss: 0.0812\n",
      "Epoch [5/10], Batch [280/741], Loss: 0.0754\n",
      "Epoch [5/10], Batch [290/741], Loss: 0.0445\n",
      "Epoch [5/10], Batch [300/741], Loss: 0.0666\n",
      "Epoch [5/10], Batch [310/741], Loss: 0.0618\n",
      "Epoch [5/10], Batch [320/741], Loss: 0.0477\n",
      "Epoch [5/10], Batch [330/741], Loss: 0.0513\n",
      "Epoch [5/10], Batch [340/741], Loss: 0.0643\n",
      "Epoch [5/10], Batch [350/741], Loss: 0.0725\n",
      "Epoch [5/10], Batch [360/741], Loss: 0.0564\n",
      "Epoch [5/10], Batch [370/741], Loss: 0.0750\n",
      "Epoch [5/10], Batch [380/741], Loss: 0.0549\n",
      "Epoch [5/10], Batch [390/741], Loss: 0.0733\n",
      "Epoch [5/10], Batch [400/741], Loss: 0.0617\n",
      "Epoch [5/10], Batch [410/741], Loss: 0.0541\n",
      "Epoch [5/10], Batch [420/741], Loss: 0.0525\n",
      "Epoch [5/10], Batch [430/741], Loss: 0.0433\n",
      "Epoch [5/10], Batch [440/741], Loss: 0.0524\n",
      "Epoch [5/10], Batch [450/741], Loss: 0.0512\n",
      "Epoch [5/10], Batch [460/741], Loss: 0.0501\n",
      "Epoch [5/10], Batch [470/741], Loss: 0.0459\n",
      "Epoch [5/10], Batch [480/741], Loss: 0.0604\n",
      "Epoch [5/10], Batch [490/741], Loss: 0.0671\n",
      "Epoch [5/10], Batch [500/741], Loss: 0.0655\n",
      "Epoch [5/10], Batch [510/741], Loss: 0.0591\n",
      "Epoch [5/10], Batch [520/741], Loss: 0.0693\n",
      "Epoch [5/10], Batch [530/741], Loss: 0.0728\n",
      "Epoch [5/10], Batch [540/741], Loss: 0.0405\n",
      "Epoch [5/10], Batch [550/741], Loss: 0.0626\n",
      "Epoch [5/10], Batch [560/741], Loss: 0.0844\n",
      "Epoch [5/10], Batch [570/741], Loss: 0.0833\n",
      "Epoch [5/10], Batch [580/741], Loss: 0.0576\n",
      "Epoch [5/10], Batch [590/741], Loss: 0.0799\n",
      "Epoch [5/10], Batch [600/741], Loss: 0.0585\n",
      "Epoch [5/10], Batch [610/741], Loss: 0.0557\n",
      "Epoch [5/10], Batch [620/741], Loss: 0.0731\n",
      "Epoch [5/10], Batch [630/741], Loss: 0.0470\n",
      "Epoch [5/10], Batch [640/741], Loss: 0.0578\n",
      "Epoch [5/10], Batch [650/741], Loss: 0.0569\n",
      "Epoch [5/10], Batch [660/741], Loss: 0.0728\n",
      "Epoch [5/10], Batch [670/741], Loss: 0.0534\n",
      "Epoch [5/10], Batch [680/741], Loss: 0.0517\n",
      "Epoch [5/10], Batch [690/741], Loss: 0.0623\n",
      "Epoch [5/10], Batch [700/741], Loss: 0.0697\n",
      "Epoch [5/10], Batch [710/741], Loss: 0.0776\n",
      "Epoch [5/10], Batch [720/741], Loss: 0.0499\n",
      "Epoch [5/10], Batch [730/741], Loss: 0.0788\n",
      "Epoch [5/10], Batch [740/741], Loss: 0.0623\n",
      "Epoch 5 finished.\n",
      "Epoch [6/10], Batch [10/741], Loss: 0.0201\n",
      "Epoch [6/10], Batch [20/741], Loss: 0.0229\n",
      "Epoch [6/10], Batch [30/741], Loss: 0.0219\n",
      "Epoch [6/10], Batch [40/741], Loss: 0.0169\n",
      "Epoch [6/10], Batch [50/741], Loss: 0.0209\n",
      "Epoch [6/10], Batch [60/741], Loss: 0.0153\n",
      "Epoch [6/10], Batch [70/741], Loss: 0.0139\n",
      "Epoch [6/10], Batch [80/741], Loss: 0.0170\n",
      "Epoch [6/10], Batch [90/741], Loss: 0.0184\n",
      "Epoch [6/10], Batch [100/741], Loss: 0.0176\n",
      "Epoch [6/10], Batch [110/741], Loss: 0.0188\n",
      "Epoch [6/10], Batch [120/741], Loss: 0.0140\n",
      "Epoch [6/10], Batch [130/741], Loss: 0.0145\n",
      "Epoch [6/10], Batch [140/741], Loss: 0.0144\n",
      "Epoch [6/10], Batch [150/741], Loss: 0.0179\n",
      "Epoch [6/10], Batch [160/741], Loss: 0.0143\n",
      "Epoch [6/10], Batch [170/741], Loss: 0.0128\n",
      "Epoch [6/10], Batch [180/741], Loss: 0.0120\n",
      "Epoch [6/10], Batch [190/741], Loss: 0.0147\n",
      "Epoch [6/10], Batch [200/741], Loss: 0.0488\n",
      "Epoch [6/10], Batch [210/741], Loss: 0.0114\n",
      "Epoch [6/10], Batch [220/741], Loss: 0.0144\n",
      "Epoch [6/10], Batch [230/741], Loss: 0.0159\n",
      "Epoch [6/10], Batch [240/741], Loss: 0.0134\n",
      "Epoch [6/10], Batch [250/741], Loss: 0.0116\n",
      "Epoch [6/10], Batch [260/741], Loss: 0.0129\n",
      "Epoch [6/10], Batch [270/741], Loss: 0.0108\n",
      "Epoch [6/10], Batch [280/741], Loss: 0.0144\n",
      "Epoch [6/10], Batch [290/741], Loss: 0.0221\n",
      "Epoch [6/10], Batch [300/741], Loss: 0.0130\n",
      "Epoch [6/10], Batch [310/741], Loss: 0.0093\n",
      "Epoch [6/10], Batch [320/741], Loss: 0.0150\n",
      "Epoch [6/10], Batch [330/741], Loss: 0.0123\n",
      "Epoch [6/10], Batch [340/741], Loss: 0.0137\n",
      "Epoch [6/10], Batch [350/741], Loss: 0.0212\n",
      "Epoch [6/10], Batch [360/741], Loss: 0.0106\n",
      "Epoch [6/10], Batch [370/741], Loss: 0.0133\n",
      "Epoch [6/10], Batch [380/741], Loss: 0.0109\n",
      "Epoch [6/10], Batch [390/741], Loss: 0.0113\n",
      "Epoch [6/10], Batch [400/741], Loss: 0.0113\n",
      "Epoch [6/10], Batch [410/741], Loss: 0.0150\n",
      "Epoch [6/10], Batch [420/741], Loss: 0.0134\n",
      "Epoch [6/10], Batch [430/741], Loss: 0.0190\n",
      "Epoch [6/10], Batch [440/741], Loss: 0.0283\n",
      "Epoch [6/10], Batch [450/741], Loss: 0.0149\n",
      "Epoch [6/10], Batch [460/741], Loss: 0.0166\n",
      "Epoch [6/10], Batch [470/741], Loss: 0.0149\n",
      "Epoch [6/10], Batch [480/741], Loss: 0.0218\n",
      "Epoch [6/10], Batch [490/741], Loss: 0.0125\n",
      "Epoch [6/10], Batch [500/741], Loss: 0.0115\n",
      "Epoch [6/10], Batch [510/741], Loss: 0.0159\n",
      "Epoch [6/10], Batch [520/741], Loss: 0.0431\n",
      "Epoch [6/10], Batch [530/741], Loss: 0.0187\n",
      "Epoch [6/10], Batch [540/741], Loss: 0.0288\n",
      "Epoch [6/10], Batch [550/741], Loss: 0.0231\n",
      "Epoch [6/10], Batch [560/741], Loss: 0.0115\n",
      "Epoch [6/10], Batch [570/741], Loss: 0.0126\n",
      "Epoch [6/10], Batch [580/741], Loss: 0.0219\n",
      "Epoch [6/10], Batch [590/741], Loss: 0.0132\n",
      "Epoch [6/10], Batch [600/741], Loss: 0.0107\n",
      "Epoch [6/10], Batch [610/741], Loss: 0.0182\n",
      "Epoch [6/10], Batch [620/741], Loss: 0.0395\n",
      "Epoch [6/10], Batch [630/741], Loss: 0.0154\n",
      "Epoch [6/10], Batch [640/741], Loss: 0.0220\n",
      "Epoch [6/10], Batch [650/741], Loss: 0.0116\n",
      "Epoch [6/10], Batch [660/741], Loss: 0.0276\n",
      "Epoch [6/10], Batch [670/741], Loss: 0.0122\n",
      "Epoch [6/10], Batch [680/741], Loss: 0.0379\n",
      "Epoch [6/10], Batch [690/741], Loss: 0.0091\n",
      "Epoch [6/10], Batch [700/741], Loss: 0.0083\n",
      "Epoch [6/10], Batch [710/741], Loss: 0.0100\n",
      "Epoch [6/10], Batch [720/741], Loss: 0.0089\n",
      "Epoch [6/10], Batch [730/741], Loss: 0.0125\n",
      "Epoch [6/10], Batch [740/741], Loss: 0.0143\n",
      "Epoch 6 finished.\n",
      "Epoch [7/10], Batch [10/741], Loss: 0.0350\n",
      "Epoch [7/10], Batch [20/741], Loss: 0.0073\n",
      "Epoch [7/10], Batch [30/741], Loss: 0.0092\n",
      "Epoch [7/10], Batch [40/741], Loss: 0.0096\n",
      "Epoch [7/10], Batch [50/741], Loss: 0.0079\n",
      "Epoch [7/10], Batch [60/741], Loss: 0.0073\n",
      "Epoch [7/10], Batch [70/741], Loss: 0.0063\n",
      "Epoch [7/10], Batch [80/741], Loss: 0.0055\n",
      "Epoch [7/10], Batch [90/741], Loss: 0.0076\n",
      "Epoch [7/10], Batch [100/741], Loss: 0.0054\n",
      "Epoch [7/10], Batch [110/741], Loss: 0.0349\n",
      "Epoch [7/10], Batch [120/741], Loss: 0.0060\n",
      "Epoch [7/10], Batch [130/741], Loss: 0.0073\n",
      "Epoch [7/10], Batch [140/741], Loss: 0.0068\n",
      "Epoch [7/10], Batch [150/741], Loss: 0.0062\n",
      "Epoch [7/10], Batch [160/741], Loss: 0.0057\n",
      "Epoch [7/10], Batch [170/741], Loss: 0.0060\n",
      "Epoch [7/10], Batch [180/741], Loss: 0.0060\n",
      "Epoch [7/10], Batch [190/741], Loss: 0.0233\n",
      "Epoch [7/10], Batch [200/741], Loss: 0.0156\n",
      "Epoch [7/10], Batch [210/741], Loss: 0.0066\n",
      "Epoch [7/10], Batch [220/741], Loss: 0.0072\n",
      "Epoch [7/10], Batch [230/741], Loss: 0.0066\n",
      "Epoch [7/10], Batch [240/741], Loss: 0.0051\n",
      "Epoch [7/10], Batch [250/741], Loss: 0.0062\n",
      "Epoch [7/10], Batch [260/741], Loss: 0.0056\n",
      "Epoch [7/10], Batch [270/741], Loss: 0.0070\n",
      "Epoch [7/10], Batch [280/741], Loss: 0.0052\n",
      "Epoch [7/10], Batch [290/741], Loss: 0.0060\n",
      "Epoch [7/10], Batch [300/741], Loss: 0.0083\n",
      "Epoch [7/10], Batch [310/741], Loss: 0.0236\n",
      "Epoch [7/10], Batch [320/741], Loss: 0.0068\n",
      "Epoch [7/10], Batch [330/741], Loss: 0.0060\n",
      "Epoch [7/10], Batch [340/741], Loss: 0.0065\n",
      "Epoch [7/10], Batch [350/741], Loss: 0.0062\n",
      "Epoch [7/10], Batch [360/741], Loss: 0.0062\n",
      "Epoch [7/10], Batch [370/741], Loss: 0.0057\n",
      "Epoch [7/10], Batch [380/741], Loss: 0.0056\n",
      "Epoch [7/10], Batch [390/741], Loss: 0.0051\n",
      "Epoch [7/10], Batch [400/741], Loss: 0.0059\n",
      "Epoch [7/10], Batch [410/741], Loss: 0.0236\n",
      "Epoch [7/10], Batch [420/741], Loss: 0.0051\n",
      "Epoch [7/10], Batch [430/741], Loss: 0.0047\n",
      "Epoch [7/10], Batch [440/741], Loss: 0.0049\n",
      "Epoch [7/10], Batch [450/741], Loss: 0.0037\n",
      "Epoch [7/10], Batch [460/741], Loss: 0.0102\n",
      "Epoch [7/10], Batch [470/741], Loss: 0.0072\n",
      "Epoch [7/10], Batch [480/741], Loss: 0.0041\n",
      "Epoch [7/10], Batch [490/741], Loss: 0.0155\n",
      "Epoch [7/10], Batch [500/741], Loss: 0.0077\n",
      "Epoch [7/10], Batch [510/741], Loss: 0.0075\n",
      "Epoch [7/10], Batch [520/741], Loss: 0.0366\n",
      "Epoch [7/10], Batch [530/741], Loss: 0.0046\n",
      "Epoch [7/10], Batch [540/741], Loss: 0.0057\n",
      "Epoch [7/10], Batch [550/741], Loss: 0.0047\n",
      "Epoch [7/10], Batch [560/741], Loss: 0.0059\n",
      "Epoch [7/10], Batch [570/741], Loss: 0.0056\n",
      "Epoch [7/10], Batch [580/741], Loss: 0.0048\n",
      "Epoch [7/10], Batch [590/741], Loss: 0.0045\n",
      "Epoch [7/10], Batch [600/741], Loss: 0.0072\n",
      "Epoch [7/10], Batch [610/741], Loss: 0.0069\n",
      "Epoch [7/10], Batch [620/741], Loss: 0.0043\n",
      "Epoch [7/10], Batch [630/741], Loss: 0.0092\n",
      "Epoch [7/10], Batch [640/741], Loss: 0.0062\n",
      "Epoch [7/10], Batch [650/741], Loss: 0.0205\n",
      "Epoch [7/10], Batch [660/741], Loss: 0.0048\n",
      "Epoch [7/10], Batch [670/741], Loss: 0.0044\n",
      "Epoch [7/10], Batch [680/741], Loss: 0.0052\n",
      "Epoch [7/10], Batch [690/741], Loss: 0.0043\n",
      "Epoch [7/10], Batch [700/741], Loss: 0.0187\n",
      "Epoch [7/10], Batch [710/741], Loss: 0.0047\n",
      "Epoch [7/10], Batch [720/741], Loss: 0.0049\n",
      "Epoch [7/10], Batch [730/741], Loss: 0.0057\n",
      "Epoch [7/10], Batch [740/741], Loss: 0.0041\n",
      "Epoch 7 finished.\n",
      "Epoch [8/10], Batch [10/741], Loss: 0.0026\n",
      "Epoch [8/10], Batch [20/741], Loss: 0.0028\n",
      "Epoch [8/10], Batch [30/741], Loss: 0.0029\n",
      "Epoch [8/10], Batch [40/741], Loss: 0.0031\n",
      "Epoch [8/10], Batch [50/741], Loss: 0.0034\n",
      "Epoch [8/10], Batch [60/741], Loss: 0.0145\n",
      "Epoch [8/10], Batch [70/741], Loss: 0.0037\n",
      "Epoch [8/10], Batch [80/741], Loss: 0.0036\n",
      "Epoch [8/10], Batch [90/741], Loss: 0.0029\n",
      "Epoch [8/10], Batch [100/741], Loss: 0.0031\n",
      "Epoch [8/10], Batch [110/741], Loss: 0.0075\n",
      "Epoch [8/10], Batch [120/741], Loss: 0.0032\n",
      "Epoch [8/10], Batch [130/741], Loss: 0.0087\n",
      "Epoch [8/10], Batch [140/741], Loss: 0.0217\n",
      "Epoch [8/10], Batch [150/741], Loss: 0.0041\n",
      "Epoch [8/10], Batch [160/741], Loss: 0.0040\n",
      "Epoch [8/10], Batch [170/741], Loss: 0.0032\n",
      "Epoch [8/10], Batch [180/741], Loss: 0.0046\n",
      "Epoch [8/10], Batch [190/741], Loss: 0.0037\n",
      "Epoch [8/10], Batch [200/741], Loss: 0.0034\n",
      "Epoch [8/10], Batch [210/741], Loss: 0.0036\n",
      "Epoch [8/10], Batch [220/741], Loss: 0.0032\n",
      "Epoch [8/10], Batch [230/741], Loss: 0.0035\n",
      "Epoch [8/10], Batch [240/741], Loss: 0.0036\n",
      "Epoch [8/10], Batch [250/741], Loss: 0.0038\n",
      "Epoch [8/10], Batch [260/741], Loss: 0.0031\n",
      "Epoch [8/10], Batch [270/741], Loss: 0.0251\n",
      "Epoch [8/10], Batch [280/741], Loss: 0.0032\n",
      "Epoch [8/10], Batch [290/741], Loss: 0.0045\n",
      "Epoch [8/10], Batch [300/741], Loss: 0.0733\n",
      "Epoch [8/10], Batch [310/741], Loss: 0.0077\n",
      "Epoch [8/10], Batch [320/741], Loss: 0.0087\n",
      "Epoch [8/10], Batch [330/741], Loss: 0.0052\n",
      "Epoch [8/10], Batch [340/741], Loss: 0.0056\n",
      "Epoch [8/10], Batch [350/741], Loss: 0.0053\n",
      "Epoch [8/10], Batch [360/741], Loss: 0.0052\n",
      "Epoch [8/10], Batch [370/741], Loss: 0.0310\n",
      "Epoch [8/10], Batch [380/741], Loss: 0.0046\n",
      "Epoch [8/10], Batch [390/741], Loss: 0.0029\n",
      "Epoch [8/10], Batch [400/741], Loss: 0.0027\n",
      "Epoch [8/10], Batch [410/741], Loss: 0.0034\n",
      "Epoch [8/10], Batch [420/741], Loss: 0.0027\n",
      "Epoch [8/10], Batch [430/741], Loss: 0.0071\n",
      "Epoch [8/10], Batch [440/741], Loss: 0.0041\n",
      "Epoch [8/10], Batch [450/741], Loss: 0.0040\n",
      "Epoch [8/10], Batch [460/741], Loss: 0.0036\n",
      "Epoch [8/10], Batch [470/741], Loss: 0.0085\n",
      "Epoch [8/10], Batch [480/741], Loss: 0.0031\n",
      "Epoch [8/10], Batch [490/741], Loss: 0.0026\n",
      "Epoch [8/10], Batch [500/741], Loss: 0.0029\n",
      "Epoch [8/10], Batch [510/741], Loss: 0.0196\n",
      "Epoch [8/10], Batch [520/741], Loss: 0.0029\n",
      "Epoch [8/10], Batch [530/741], Loss: 0.0038\n",
      "Epoch [8/10], Batch [540/741], Loss: 0.0029\n",
      "Epoch [8/10], Batch [550/741], Loss: 0.0024\n",
      "Epoch [8/10], Batch [560/741], Loss: 0.0028\n",
      "Epoch [8/10], Batch [570/741], Loss: 0.0285\n",
      "Epoch [8/10], Batch [580/741], Loss: 0.0151\n",
      "Epoch [8/10], Batch [590/741], Loss: 0.0167\n",
      "Epoch [8/10], Batch [600/741], Loss: 0.0032\n",
      "Epoch [8/10], Batch [610/741], Loss: 0.0031\n",
      "Epoch [8/10], Batch [620/741], Loss: 0.0023\n",
      "Epoch [8/10], Batch [630/741], Loss: 0.0052\n",
      "Epoch [8/10], Batch [640/741], Loss: 0.0026\n",
      "Epoch [8/10], Batch [650/741], Loss: 0.0031\n",
      "Epoch [8/10], Batch [660/741], Loss: 0.0038\n",
      "Epoch [8/10], Batch [670/741], Loss: 0.0025\n",
      "Epoch [8/10], Batch [680/741], Loss: 0.0030\n",
      "Epoch [8/10], Batch [690/741], Loss: 0.0024\n",
      "Epoch [8/10], Batch [700/741], Loss: 0.0022\n",
      "Epoch [8/10], Batch [710/741], Loss: 0.0039\n",
      "Epoch [8/10], Batch [720/741], Loss: 0.0022\n",
      "Epoch [8/10], Batch [730/741], Loss: 0.0026\n",
      "Epoch [8/10], Batch [740/741], Loss: 0.0029\n",
      "Epoch 8 finished.\n",
      "Epoch [9/10], Batch [10/741], Loss: 0.0019\n",
      "Epoch [9/10], Batch [20/741], Loss: 0.0018\n",
      "Epoch [9/10], Batch [30/741], Loss: 0.0019\n",
      "Epoch [9/10], Batch [40/741], Loss: 0.0019\n",
      "Epoch [9/10], Batch [50/741], Loss: 0.0015\n",
      "Epoch [9/10], Batch [60/741], Loss: 0.0018\n",
      "Epoch [9/10], Batch [70/741], Loss: 0.0018\n",
      "Epoch [9/10], Batch [80/741], Loss: 0.0156\n",
      "Epoch [9/10], Batch [90/741], Loss: 0.0016\n",
      "Epoch [9/10], Batch [100/741], Loss: 0.0020\n",
      "Epoch [9/10], Batch [110/741], Loss: 0.0023\n",
      "Epoch [9/10], Batch [120/741], Loss: 0.0017\n",
      "Epoch [9/10], Batch [130/741], Loss: 0.0016\n",
      "Epoch [9/10], Batch [140/741], Loss: 0.0020\n",
      "Epoch [9/10], Batch [150/741], Loss: 0.0014\n",
      "Epoch [9/10], Batch [160/741], Loss: 0.0018\n",
      "Epoch [9/10], Batch [170/741], Loss: 0.0019\n",
      "Epoch [9/10], Batch [180/741], Loss: 0.0018\n",
      "Epoch [9/10], Batch [190/741], Loss: 0.0014\n",
      "Epoch [9/10], Batch [200/741], Loss: 0.0015\n",
      "Epoch [9/10], Batch [210/741], Loss: 0.0020\n",
      "Epoch [9/10], Batch [220/741], Loss: 0.0019\n",
      "Epoch [9/10], Batch [230/741], Loss: 0.0018\n",
      "Epoch [9/10], Batch [240/741], Loss: 0.0015\n",
      "Epoch [9/10], Batch [250/741], Loss: 0.0015\n",
      "Epoch [9/10], Batch [260/741], Loss: 0.0018\n",
      "Epoch [9/10], Batch [270/741], Loss: 0.0014\n",
      "Epoch [9/10], Batch [280/741], Loss: 0.0016\n",
      "Epoch [9/10], Batch [290/741], Loss: 0.0020\n",
      "Epoch [9/10], Batch [300/741], Loss: 0.0022\n",
      "Epoch [9/10], Batch [310/741], Loss: 0.0018\n",
      "Epoch [9/10], Batch [320/741], Loss: 0.0017\n",
      "Epoch [9/10], Batch [330/741], Loss: 0.0015\n",
      "Epoch [9/10], Batch [340/741], Loss: 0.0017\n",
      "Epoch [9/10], Batch [350/741], Loss: 0.0037\n",
      "Epoch [9/10], Batch [360/741], Loss: 0.0112\n",
      "Epoch [9/10], Batch [370/741], Loss: 0.0026\n",
      "Epoch [9/10], Batch [380/741], Loss: 0.0053\n",
      "Epoch [9/10], Batch [390/741], Loss: 0.0042\n",
      "Epoch [9/10], Batch [400/741], Loss: 0.0129\n",
      "Epoch [9/10], Batch [410/741], Loss: 0.0027\n",
      "Epoch [9/10], Batch [420/741], Loss: 0.0015\n",
      "Epoch [9/10], Batch [430/741], Loss: 0.0018\n",
      "Epoch [9/10], Batch [440/741], Loss: 0.0014\n",
      "Epoch [9/10], Batch [450/741], Loss: 0.0022\n",
      "Epoch [9/10], Batch [460/741], Loss: 0.0021\n",
      "Epoch [9/10], Batch [470/741], Loss: 0.0019\n",
      "Epoch [9/10], Batch [480/741], Loss: 0.0232\n",
      "Epoch [9/10], Batch [490/741], Loss: 0.0014\n",
      "Epoch [9/10], Batch [500/741], Loss: 0.0029\n",
      "Epoch [9/10], Batch [510/741], Loss: 0.0043\n",
      "Epoch [9/10], Batch [520/741], Loss: 0.0178\n",
      "Epoch [9/10], Batch [530/741], Loss: 0.0015\n",
      "Epoch [9/10], Batch [540/741], Loss: 0.0041\n",
      "Epoch [9/10], Batch [550/741], Loss: 0.0019\n",
      "Epoch [9/10], Batch [560/741], Loss: 0.0016\n",
      "Epoch [9/10], Batch [570/741], Loss: 0.0021\n",
      "Epoch [9/10], Batch [580/741], Loss: 0.0013\n",
      "Epoch [9/10], Batch [590/741], Loss: 0.0132\n",
      "Epoch [9/10], Batch [600/741], Loss: 0.0014\n",
      "Epoch [9/10], Batch [610/741], Loss: 0.0013\n",
      "Epoch [9/10], Batch [620/741], Loss: 0.0254\n",
      "Epoch [9/10], Batch [630/741], Loss: 0.0035\n",
      "Epoch [9/10], Batch [640/741], Loss: 0.0021\n",
      "Epoch [9/10], Batch [650/741], Loss: 0.0015\n",
      "Epoch [9/10], Batch [660/741], Loss: 0.0015\n",
      "Epoch [9/10], Batch [670/741], Loss: 0.0017\n",
      "Epoch [9/10], Batch [680/741], Loss: 0.0015\n",
      "Epoch [9/10], Batch [690/741], Loss: 0.0016\n",
      "Epoch [9/10], Batch [700/741], Loss: 0.0222\n",
      "Epoch [9/10], Batch [710/741], Loss: 0.0017\n",
      "Epoch [9/10], Batch [720/741], Loss: 0.0013\n",
      "Epoch [9/10], Batch [730/741], Loss: 0.0016\n",
      "Epoch [9/10], Batch [740/741], Loss: 0.0059\n",
      "Epoch 9 finished.\n",
      "Epoch [10/10], Batch [10/741], Loss: 0.0269\n",
      "Epoch [10/10], Batch [20/741], Loss: 0.0014\n",
      "Epoch [10/10], Batch [30/741], Loss: 0.0012\n",
      "Epoch [10/10], Batch [40/741], Loss: 0.0016\n",
      "Epoch [10/10], Batch [50/741], Loss: 0.0011\n",
      "Epoch [10/10], Batch [60/741], Loss: 0.0016\n",
      "Epoch [10/10], Batch [70/741], Loss: 0.0014\n",
      "Epoch [10/10], Batch [80/741], Loss: 0.0082\n",
      "Epoch [10/10], Batch [90/741], Loss: 0.0019\n",
      "Epoch [10/10], Batch [100/741], Loss: 0.0049\n",
      "Epoch [10/10], Batch [110/741], Loss: 0.0262\n",
      "Epoch [10/10], Batch [120/741], Loss: 0.0240\n",
      "Epoch [10/10], Batch [130/741], Loss: 0.0517\n",
      "Epoch [10/10], Batch [140/741], Loss: 0.1152\n",
      "Epoch [10/10], Batch [150/741], Loss: 0.3833\n",
      "Epoch [10/10], Batch [160/741], Loss: 0.4523\n",
      "Epoch [10/10], Batch [170/741], Loss: 0.6353\n",
      "Epoch [10/10], Batch [180/741], Loss: 0.6673\n",
      "Epoch [10/10], Batch [190/741], Loss: 0.6092\n",
      "Epoch [10/10], Batch [200/741], Loss: 0.7070\n",
      "Epoch [10/10], Batch [210/741], Loss: 0.5619\n",
      "Epoch [10/10], Batch [220/741], Loss: 0.4963\n",
      "Epoch [10/10], Batch [230/741], Loss: 0.3868\n",
      "Epoch [10/10], Batch [240/741], Loss: 0.3342\n",
      "Epoch [10/10], Batch [250/741], Loss: 0.4091\n",
      "Epoch [10/10], Batch [260/741], Loss: 0.2887\n",
      "Epoch [10/10], Batch [270/741], Loss: 0.5006\n",
      "Epoch [10/10], Batch [280/741], Loss: 0.3494\n",
      "Epoch [10/10], Batch [290/741], Loss: 0.2985\n",
      "Epoch [10/10], Batch [300/741], Loss: 0.2338\n",
      "Epoch [10/10], Batch [310/741], Loss: 0.1999\n",
      "Epoch [10/10], Batch [320/741], Loss: 0.1345\n",
      "Epoch [10/10], Batch [330/741], Loss: 0.1169\n",
      "Epoch [10/10], Batch [340/741], Loss: 0.1080\n",
      "Epoch [10/10], Batch [350/741], Loss: 0.1117\n",
      "Epoch [10/10], Batch [360/741], Loss: 0.0900\n",
      "Epoch [10/10], Batch [370/741], Loss: 0.1363\n",
      "Epoch [10/10], Batch [380/741], Loss: 0.0840\n",
      "Epoch [10/10], Batch [390/741], Loss: 0.1033\n",
      "Epoch [10/10], Batch [400/741], Loss: 0.0703\n",
      "Epoch [10/10], Batch [410/741], Loss: 0.0784\n",
      "Epoch [10/10], Batch [420/741], Loss: 0.0600\n",
      "Epoch [10/10], Batch [430/741], Loss: 0.0564\n",
      "Epoch [10/10], Batch [440/741], Loss: 0.0692\n",
      "Epoch [10/10], Batch [450/741], Loss: 0.0965\n",
      "Epoch [10/10], Batch [460/741], Loss: 0.1486\n",
      "Epoch [10/10], Batch [470/741], Loss: 0.0489\n",
      "Epoch [10/10], Batch [480/741], Loss: 0.1015\n",
      "Epoch [10/10], Batch [490/741], Loss: 0.0823\n",
      "Epoch [10/10], Batch [500/741], Loss: 0.0710\n",
      "Epoch [10/10], Batch [510/741], Loss: 0.0865\n",
      "Epoch [10/10], Batch [520/741], Loss: 0.0857\n",
      "Epoch [10/10], Batch [530/741], Loss: 0.0986\n",
      "Epoch [10/10], Batch [540/741], Loss: 0.0446\n",
      "Epoch [10/10], Batch [550/741], Loss: 0.0456\n",
      "Epoch [10/10], Batch [560/741], Loss: 0.0678\n",
      "Epoch [10/10], Batch [570/741], Loss: 0.0821\n",
      "Epoch [10/10], Batch [580/741], Loss: 0.0778\n",
      "Epoch [10/10], Batch [590/741], Loss: 0.0806\n",
      "Epoch [10/10], Batch [600/741], Loss: 0.0888\n",
      "Epoch [10/10], Batch [610/741], Loss: 0.0725\n",
      "Epoch [10/10], Batch [620/741], Loss: 0.0975\n",
      "Epoch [10/10], Batch [630/741], Loss: 0.0846\n",
      "Epoch [10/10], Batch [640/741], Loss: 0.0413\n",
      "Epoch [10/10], Batch [650/741], Loss: 0.0431\n",
      "Epoch [10/10], Batch [660/741], Loss: 0.0579\n",
      "Epoch [10/10], Batch [670/741], Loss: 0.0492\n",
      "Epoch [10/10], Batch [680/741], Loss: 0.0865\n",
      "Epoch [10/10], Batch [690/741], Loss: 0.0401\n",
      "Epoch [10/10], Batch [700/741], Loss: 0.0652\n",
      "Epoch [10/10], Batch [710/741], Loss: 0.0929\n",
      "Epoch [10/10], Batch [720/741], Loss: 0.0400\n",
      "Epoch [10/10], Batch [730/741], Loss: 0.0387\n",
      "Epoch [10/10], Batch [740/741], Loss: 0.0448\n",
      "Epoch 10 finished.\n",
      "Progress: 400/2975 images evaluated.\n",
      "Progress: 800/2975 images evaluated.\n",
      "Progress: 1200/2975 images evaluated.\n",
      "Progress: 1600/2975 images evaluated.\n",
      "Progress: 2000/2975 images evaluated.\n",
      "Progress: 2400/2975 images evaluated.\n",
      "Progress: 2800/2975 images evaluated.\n",
      "Accuracy: 64.50%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load Data from JSON\n",
    "with open('C:\\\\Users\\\\hp\\\\Downloads\\\\DataVIT.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Flatten the image data\n",
    "image_data = []\n",
    "for item in data:\n",
    "    for img in item['images']:\n",
    "        image_data.append({'url': img['url'], 'label': item['interests'][0]})\n",
    "\n",
    "# Collect paths to all .pt files\n",
    "preprocessed_files = sorted([f'C:\\\\Users\\\\hp\\\\Downloads\\\\preprocessed_images_combined\\\\{filename}' \n",
    "                             for filename in os.listdir('C:\\\\Users\\\\hp\\\\Downloads\\\\preprocessed_images_combined/') \n",
    "                             if filename.endswith('.pt')])\n",
    "\n",
    "# Create a mapping from filename to preprocessed file path\n",
    "preprocessed_file_map = {os.path.basename(file): file for file in preprocessed_files}\n",
    "\n",
    "# Step 2: Split Data into Training and Test Sets\n",
    "train_data, test_data = train_test_split(image_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Filter the train data to include only entries that have a corresponding preprocessed file\n",
    "filtered_train_data = [entry for entry in train_data if os.path.basename(entry['url']).replace('.jpg', '.pt') in preprocessed_file_map]\n",
    "\n",
    "# Filter the test data to include only entries that have a corresponding preprocessed file\n",
    "filtered_test_data = [entry for entry in test_data if os.path.basename(entry['url']).replace('.jpg', '.pt') in preprocessed_file_map]\n",
    "\n",
    "# Verify the length of filtered_train_data, filtered_test_data and preprocessed_files\n",
    "print(f\"Number of filtered train data entries: {len(filtered_train_data)}\")\n",
    "print(f\"Number of filtered test data entries: {len(filtered_test_data)}\")\n",
    "print(f\"Number of preprocessed files: {len(preprocessed_files)}\")\n",
    "\n",
    "# Map the preprocessed files to train and test data\n",
    "preprocessed_train_files = [preprocessed_file_map[os.path.basename(entry['url']).replace('.jpg', '.pt')] for entry in filtered_train_data]\n",
    "preprocessed_test_files = [preprocessed_file_map[os.path.basename(entry['url']).replace('.jpg', '.pt')] for entry in filtered_test_data]\n",
    "\n",
    "# Step 3: Create the Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, preprocessed_files, transform=None):\n",
    "        self.data = data\n",
    "        self.preprocessed_files = preprocessed_files\n",
    "        self.transform = transform\n",
    "\n",
    "        # Ensure the lengths match\n",
    "        assert len(self.data) == len(self.preprocessed_files), (\n",
    "            f\"Data length {len(self.data)} does not match number of preprocessed files {len(self.preprocessed_files)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        preprocessed_image = torch.load(self.preprocessed_files[idx])  # Load the preprocessed tensor from file\n",
    "        label = self.data[idx]['label']\n",
    "        if self.transform:\n",
    "            preprocessed_image = self.transform(preprocessed_image)\n",
    "        return preprocessed_image, label\n",
    "\n",
    "# Define transformations (if any additional transformations are needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(filtered_train_data, preprocessed_train_files, transform=transform)\n",
    "test_dataset = CustomDataset(filtered_test_data, preprocessed_test_files, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Step 4: Create the ViT Model\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=len(set([item['label'] for item in filtered_train_data])),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Step 5: Train the Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # Print every 10 batches\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {running_loss / 10:.4f}')\n",
    "            running_loss = 0.0\n",
    "    print(f'Epoch {epoch+1} finished.')\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total % 100 == 0:  # Print every 100 images\n",
    "            print(f'Progress: {total}/{len(test_loader.dataset)} images evaluated.')\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c4158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "num_epochs = 1\n",
    "image_size = 224  # Adjust based on your actual image size\n",
    "patch_size = 7\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [projection_dim * 2, projection_dim]\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [56, 28]\n",
    "n_classes = 10  # Adjust based on your actual number of classes\n",
    "\n",
    "# Define MLP\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = L.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = L.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "# Define Patch Creation Layer\n",
    "class Patches(L.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID',\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "# Define Patch Encoding Layer\n",
    "class PatchEncoder(L.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = L.Dense(units=projection_dim)\n",
    "        self.position_embedding = L.Embedding(input_dim=num_patches, output_dim=projection_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "# Build the ViT model\n",
    "def vision_transformer():\n",
    "    inputs = L.Input(shape=(image_size, image_size, 3))\n",
    "    \n",
    "    # Create patches\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    \n",
    "    # Encode patches\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "    \n",
    "    # Create multiple layers of the Transformer block\n",
    "    for _ in range(transformer_layers):\n",
    "        x1 = L.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = L.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n",
    "        x2 = L.Add()([attention_output, encoded_patches])\n",
    "        x3 = L.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        encoded_patches = L.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor\n",
    "    representation = L.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = L.Flatten()(representation)\n",
    "    representation = L.Dropout(0.5)(representation)\n",
    "    \n",
    "    # Add MLP\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    \n",
    "    # Classify outputs\n",
    "    logits = L.Dense(n_classes)(features)\n",
    "    \n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set learning rate schedule and optimizer\n",
    "decay_steps = len(dataset) // 32  # Adjust based on your batch size\n",
    "initial_learning_rate = learning_rate\n",
    "lr_decayed_fn = tf.keras.experimental.CosineDecay(initial_learning_rate, decay_steps)\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_decayed_fn)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model = vision_transformer()\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5, mode='max', restore_best_weights=True, verbose=1)\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='./model.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "callbacks = [earlystopping, lr_scheduler, checkpointer]\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = dataset.take(train_size)\n",
    "valid_dataset = dataset.skip(train_size)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, validation_data=valid_dataset, epochs=num_epochs, callbacks=callbacks)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Training results')\n",
    "model.evaluate(train_dataset)\n",
    "print('Validation results')\n",
    "model.evaluate(valid_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
